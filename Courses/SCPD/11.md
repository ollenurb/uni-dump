\newpage
# Calcolo parallelo su GPU
Fino ad ora abbiamo visto modelli che si basano sulle CPU. In generale,
l'architettura di una CPU moderna a pipeline e' costituita da diversi componenti
pricipali quali:

* Registri
* ALU
* Cache
* Componenti di OOO, Branch Prediction, Memory

Siccome i registri e l'ALU formano essenzialmente un core minimale per
l'esecuzione effettiva delle istruzioni, una direzione possibile per aumentare
la densita' di calcolo potrebbe essere quella di aumentare effettivamente il
numero di questi core sullo stesso die di silicio.
Possiamo inoltre notare come tutti questi componenti (a parte i registri e
l'ALU), siano stati progettati appositamente per fare pipelining in questa
tipologia di processori.
Inoltre, di tutti questi componenti, l'unita' di calcolo effettiva richiede
veramente poco spazio rispetto ad altri componenti come la cache. Questo
perche' la cache si e' essenzialmente sviluppata per avere uno storage sempre
piu' grande in modo da ridurre al minimo i cache misses.

Il pipelining e' una tecnica molto comoda e nascosta al programmatore, che ci
permette di eseguire codice sequenziale il parallelo, ma per questa ragione ci
impone anche dei vincoli che possono essere sorpassati solo cambiando modello di
esecuzione in primo luogo.

Un'idea possibile, quindi, potrebbe essere quella di rimuovere completamente
tutti i componenti adibiti al pipelining (compresa la cache) in favore di un
maggiore numero di unita' di calcolo. Il problema, pero', e' che se metto un
numero molto alto di queste unita' di calcolo, ogni core dovra' andare a leggere
il proprio flusso di istruzion indipendente dagli altri. Poter usare tutti
questi core contemporaneamente sarebbe impossibile, perche' il collo di
bottiglia dato dal traffico in memoria sarebbe enorme (immagina ogni processore
indipendente che va a leggere dalla memoria ogni volta dati indipendenti).

Per risolvere il problema, quindi, si puo' pensare di condividere con tutti i
cores un solo program counter (*Instuction Stream Sharing*). In questo modo,
tutti i threads eseguono *la stessa istruzione* contemporaneamente, ma su dati
diversi. La conseguenza di questo punto di vista e' che si vanno a ridurre tutti
i trasferimenti in memoria, andando a raggrupparli in "*blocchi*". Se prima ogni
threads accedeva potenzialmente ad elementi di array differenti, con questo
approccio ogni thread opera su una particolare cella dello stesso array,
riducendo di conseguenza il numero di accessi totali. Per questa ragione, il
modello di programmazione di questi sistemi e' detto **SIMD** (*Single
Instruction Multiple Data*).

Questo modo di vedere le cose ha delle implicazioni profonde sulla
programmazione di sistemi SIMD. Una di queste e' che il codice sequenziale non
puo' essere utilizzato in nessun modo senza essere ristrutturato per supportare
questa tipologia di architettura. Un'altra problematica e' quella della *thread
divergence*, che si verifica quando ogni unita' di calcolo deve fare un
branching. Si consideri, ad esempio, il seguente codice:

```c
for(i=0; i < N; i++)
    if(A[i] < 0) A[i] = -A[i];
    else A[i] = A[i] + B[i];
```

Quale delle due istruzioni dovranno eseguire tutti i threads? Ovviamente, non e'
possibile farlo, per cui l'unico modo e' far eseguire il corpo dell'`if` da
tutti i threads per cui e' vera mantenendo quelli per cui e' falsa in *idle*, e
poi fare lo stesso per il branch `else`.

## Da SMD a SMT
Per mitigare queste problematiche, le GPU utilizzano un modello **SIMT** (*Single
Instruction Multiple Threads*). Il focus essenzialmente non e' quello di
eseguire la stessa istruzione per tutti i cores, ma invece per tutti i threads.
In questo modo e' possibile eseguire altri stream di istruzioni mentre si stanno
aspettando i dati dalla memoria, una tecnica chiamata *latency hiding*.
Per eseguire in modo efficiente centinaia di thread leggeri e concorrenti, il
multiprocessore della GPU implementa il multithreading in hardware, ossia
gestisce ed esegue centinaia di thread concorrenti in hardware, senza
overheads dovuti pianificazione dellâ€™esecuzione.

Una GPU e' essenzialmente un dispositivo progettato per questo tipo di
programmazione. E' un dispositivo esterno (*periferica*), dotato di memoria
propria, e connesso alla CPU e alla memoria del calcolatore attraverso il North
Bridge.

![Architettura Tesla di base di una GeForce 8800
NVIDIA](img/11.1_gpu_architecture.png)

I componenti essenziali di una GPU sono i cosiddetti *Streaming
Multiprocessors* (SM). Ognuno di essi e' composto da:

* Diversi *Streaming Processors* (SP) - anche detti *CUDA cores* nella
  terminologia NVIDIA
* Un grosso insieme di registri multithread (*Register File*)
* Una memoria cache condivisa
* Un'unita' di cache per le costanti
* Un'unita' di lancio delle istruzioni multithread
* Diverse unita' adibite ad operatizoni matematiche speciali (SFU)

Per implementare il modello SIMT, lo SM esegue concorrentemente gruppi di
threads paralleli, chiamati *warps*. I threads che compongono un warp partono
tutti dalla stessa istruzione, ma poi sono liberi di divergere (*thread
divergence*). Ovviamente anche in questo caso si ha una problematica come quella
del modello SIMD, per cui se i threads di un warp divergo, alcuni threads
potrebbero rimanere inattivi durante l'esecuzione di alcune istruzioni.
In generale, si ottiene un livello di efficienza massima quando tutti i threads
di un warp seguono lo stesso flusso di istruzioni.
Un warp in generale e' composto al massimo di 32 threads, che vengono eseguiti
dallo SM in 4 insiemi di 8 threads. Inoltre, ogni warp condivide la memoria
dello SM, per cui e' possibile scrivere programmi che sfruttino questa
possiblita' per ridurre il numero di accessi in memoria.

> Ricapitolando: i threads sono divisi in gruppi di 32 chiamati *warps*. Lo
  streaming multiprocessor schedula i warps e li esegue, facendo eseguire la
  stessa istruzione di tutti i threads ai singoli streaming processors.

## Modello di Programmazione CUDA
Avendo illustrato l'architettura generale, possiamo ora parlare del modello di
programmazione effettivo di questa tipologia di dispositivi. Siccome diversi
costruttori hanno impiegato il proprio modello di programmazione (nonostante si
basino sulla stessa architettura, hanno delle dissimilarita'), ci limiteremo a
considerare la programmazione di GPU CUDA.

La programmazione di acceleratori in CUDA ruota intorno al concetto di kernel:
una funzione che indica cosa deve fare il thread generico in una batteria di
threads. Tale batteria e' piu' propriamente chiamata *blocco* di threads. Ogni
threads all'interno di un blocco condivide la memoria con gli altri. A loro
volta, i blocchi sono poi organizzati in una *griglia*. Tra di loro, i blocchi
condividono la memoria globale, che ha pero' un tempo di accesso molto piu' alto
rispetto a quella condivisa tra blocchi.

I blocchi vengono implementati come (*Cooperative Thread Arrays*), cioe' gruppi
di threads che eseguono lo stesso flusso di istruzioni e possono cooperare per
arrivare al risultato. Tali CTA sono essenzialmente tradotti in dei warps a
livello architetturale.
Possiamo quindi dire che un blocco di threads rappresenta un insieme di threads
concorrenti (siccome sono composti da warps) che possono sincronizzarsi
localmente tra di loro per raggiungere un risultato.
Tali blocchi sono tra di loro indipendenti e possono essere eseguiti in
qualsiasi ordine.

Tecnicamente parlando, CUDA e' un'estensione del linguaggio C/C++, introdotta
dal compilatore di NVIDIA `nvcc`. CUDA essenzialmente distingue due categorie di
codice eseguibile differenti: quello eseguito dall'*host* (CPU) e quello
eseguito dal *device* (GPU).
Per indicare che il codice deve essere eseguito a livello di CPU o GPU, lo si
annota con delle macro `__host__` e `__device__`.
La direttiva `__global__`, invece, serve a indicare che una determinata funzione
e' un *kernel*. Ad ogni chiamata di kernel da parte dell'host, si devono
specificare le dimensioni della griglia e dei blocchi di threads su cui si vuole
lanciare il kernel.
