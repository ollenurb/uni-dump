# Reti Neurali Multiradiali
Sono reti neurali che non hanno avuto tanto successo quanto le reti a
backpropagation (siccome sono il fondamento del deep learning moderno), ma vale
la pena studiarle comunque siccome sono molto simili alle *Extreme Learning
Machines*. Si basano sul concetto di *funzione radiale*, cioe' delle funzioni la
cui uscita dipende dalla **distanza** tra input e un determinato vettore
**centro** memorizzato. L'idea e' quella di utilizzare queste funzioni come
funzioni di attivazione, siccome funzionano come *attrattori locali*: piu' un
vettore e' vicino al centro piu' sara' alta l'attivazione, viceversa se
distante. L'architettura tipica di una RBFN consiste essenzialmente in un
singolo layer hidden, i cui neuroni sono appunto le singole funzioni radiali. Il
layer di output avra' invece un modello di neurone differente, che andra'
essenzialmente a combinare i risultati del layer hidden per i pesi (cosi' come
nel MLP).

> *I pesi sono solo tra layer hidden e layer di ouput*

Siano:

* $\phi_i$ la funzione di attivazione del neurone *i-esimo* del layer hidden
* $t_i$ il centro della funzione di attivazione *i-esima*, detto anche *centro
  di simmetria*
* $m_1$ il numero di neuroni hidden

L'output della rete per un generico esempio $x$ sara':

$$
y = w_1 \phi_1 (\| x - t_1 \|) + \dots + w_{m_1} \phi_{m_1} (\| x - t_{m_1} \|)
$$

Con $\|x - t_i\|$ si indica la distanza Euclidea tra il centro e $x$

Con $\sigma$ si indica un ulteriore parametro che quantifica lo *spread* della
funzione, cioe' la sensibilita' del valore rispetto al centro: piu' sara' alto,
meno la funzione di attivera' anche per esempi lontani dal centro.

> In generale, avere troppi neuroni hidden con spread molto basso, e' correlato
> ad una scarsa capacita' della rete a generalizzare: la rete si e'
> specializzata troppo sugli esempi del training set.

## Apprendimento
Una volta formulata la rete dal punto di vista matematico, possiamo formulare
anche il problema dell'apprendimento della stessa sotto lo stesso punto di
vista.

Come visto, l'apprendimento richiede di trovare diversi parametri:

* I centri $t_i$ delle funzioni radiali
* Gli spread $\sigma$ di ogni funzione radiale
* I pesi $w$ tra l'ouput layer e l'hidden layer

### Centri
In generale, ci sono due metodologie principali per determinare i centri:

1. Inizializzare i centri delle funzioni selezionando $m_1$ esempi dal training
   set in modo casuale
2. I centri vengono calcolati mediante un algoritmo di clustering

La prima e' banale, mentre per la seconda l'algoritmo di clustering consiste
nell'andare ad inizializzare i centri randomicamente, per poi iterativamente
andare ad applicare una correzione al cluster piu' vicino per ogni esempio $x$
del training set. L'algoritmo consiste quindi nei seguenti passaggi:

1. Inizializza tutti i centri scegliendoli dal training set in modo casuale
2. Estrai un esempio $x$ dal training set
3. Determina il centro piu' vicino rispetto a $x$ (sia $t_k$ tale centro)
4. Applica la correzione del centro $t_k' = t_k + \eta [x - t_k]$
5. Ripeti fino a quando i centri non cambiano

La regola di correzione consiste nel *"tirare"* di una certa quantita'
(proporzionata rispetto alla distanza tra il centro e l'esempio) il centro $t_k$
verso i diversi esempi vicini ad esso.

> La metodologia di determinazione che si basa sul clustering permette anche di attenuare l'effetto dato dall'*attrattivita' locale* delle funzioni radiali

### Spread
Lo spread viene determinato per **normalizzazione**. Siano:

* $d_{max}$ la distanza massima tra tutte le coppie possibili dei centri che sono stati selezionati
* $m_1$ il numero di centri (*numero neuroni hidden*)

Allora lo spread e' definito come:

$$
\sigma = \frac{d_{max}}{\sqrt{m_1}}
$$

Definendo $\sigma$ in questo modo, se utilizzassimo delle funzioni gaussiane
come base delle RBF, otterremo che la formula della singola funzione di
attivazione sara'

$$
\phi_i(\| x - t_i\|^2) = exp \left( -\frac{m_1}{d^2_{max}} \| x - t_i \|^2 \right)
$$

### Pesi
Veniamo ora al problema dell'apprendimento dei pesi tra l'hidden layer e
l'output layer. Se noi abbiamo che $F(x)$ e' l'output della nostra rete e $x_j$
un generico esempio del dataset, noi vogliamo che

$$
F(x_j) = d_j
$$

per tutti gli esempi $x_j$. Se noi sostituiamo a $F$ la sua definizione data in
precedenza, otteniamo

$$
F(x_j) = \sum_{i=1}^N w_i \phi_i(\| x_j - x_i \|)
$$

Se vogliamo che questo valga per tutti i $j = 1 \dots N$, avremmo un sistema a
$N$ righe e $N$ incognite, per cui possiamo rappresentarlo in forma matriciale
come

$$
\Phi w = d
$$

> Si noti come in questo punto stiamo ponendo che ogni neurone hidden abbia come
> proprio centro proprio un esempio di train, per cui ci saranno $N$ neuroni
> hidden. 
 
Secondo il teorema di Micchelli, se una matrice di interpolazione e'
nonsignolare, allora possiamo invertire la matrice e ottenere una soluzione al
sistema formulato in precedenza. 

> La matrice possiamo tecnicamente invertirla, ma se abbiamo un determinante
> molto piccolo, otterremo un'inversa che per la presenza di errori numerici al
> calcolatore sarebbe inutile a lato pratico

A lato pratico, pero', questo tipo di risoluzione e' infattibile per due ragioni
principali:

1. La rete andrebbe a fare overfitting siccome ogni neurone viene centrato su un
   particolare esempio del training set
2. La matrice di interpolazione $\Phi$ contiene un numero di entrate pari a
   $N^2$ -> apprendimento pesante su datasets molto grossi

La soluzione e' andare a diminuire il numero di neuroni hidden ponendolo $m1 <
N$, e a risolvere il sistema mediante il calcolo della cosiddetta
*pseudoinversa* $\Phi^+$.

Formalmente, vorremmo solo un'approssimazione per cui

$$
y_i(x_i) \approx d_i
$$

cioe'

$$
w_1 \phi_1 (\| x_i - t_1 \|) + \dots + w_{m_1} \phi_{m_1} (\| x_i - t_{m_1} \|) \approx d_i
$$

Ma quindi anche in questo caso possiamo riformulare il problema come un sistema
lineare di equazioni in forma matriciale:

$$
\Phi w \approx d
$$

questa volta la matrice non e' quadrata, per cui come detto per risolvere il
problema utilizzeremo la **pseudoinversa** della matrice $\Phi$, definita nel
modo seguente: $\Phi^+ = (\Phi^T \Phi)^{-1} \Phi^T$.
Moltiplicando a sinistra entrambi i membri per $\Phi^T$ e facendo diversi
passaggi algebrici otteniamo che:

$$
w = \Phi^+ d
$$

per cui abbiamo trovato una formula per calcolare tutti i pesi. Questa tecnica
e' chiamata appunto della *pseudoinversa*.

### Riassumendo
L'apprendimento di una rete RBFN richiede i seguenti passaggi:

1. Determina i centri delle funzioni radiali mediante
	* Clustering
	* Assegnamento casuale
2. Calcola $\sigma$ mediante normalizzazione
3. Determina i pesi $w$ utilizzando
	* Il metodo basato sulla pseudoinversione
	* Un algoritmo di Least Mean Square come visto sui Percettroni

## Confronto con FFNN
Le RBFN presentano diverse caratteristiche che le differenziano notevolmente
dalle RBFN, anche se entrambe sono degli *approssimatori universali*, cioe' che
sono delle funzioni in grado di approssimare bene a piacere un qualsiasi insieme
di punti $m$-dimensionali.

Anche le RBFN possono risolvere i problemi che sono linearmente non separabili.
Questo perche' in realta' il layer hidden di queste reti non fa altro che
mappare le coordinate in input in uno spazio intermedio, trasformando il
problema in un problema linearmente separabile equivalente.

Nonostante siano entrambi degli approssimatori universali, possiamo identificare
differenze su diversi fronti tra le FFNN e le RBFN:


| Fronte \ Rete           | RBFN                                                                                  | FFNN                                                                |
|-------------------------|---------------------------------------------------------------------------------------|---------------------------------------------------------------------|
| Architettura            | Singolo Hidden Layer                                                                  | Piu' Hidden Layer                                                   |
| Modello Neurone         | Non-lineare per i neuroni dell'hidden layer, lineare per i neuroni dell'output layer  | Tutti i layer condividono lo stesso modello di neurone: non-lineare |
| Funzione di Attivazione | Prende in input la distanza Euclidea tra centro ed esempio                            | Prende in input il prodotto interno tra input e pesi                |
| Approssimazione         | Usano una Gaussiana per costruire approssimazioni locali di mappe non lineari         | Costruiscono approssimazioni globali di mappe non lineari           |
