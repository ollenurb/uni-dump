# Transformers
L'idea principale dell'architettura transformer e' quella di generare un'intero
modello sul meccanismo dell'*attenzione*. Nasce quindi come un'evoluzione dei
modelli Encoder-Decoder basati su modelli ricorrenti, solamente che rimuove la
ricorrenza a favore di moltiplicazioni di matrici per calcolare l'attenzione e
ottenere cosi' ottimi risultati. Per come e' stato concepito, il transformer e'
in parte parallelizzabile rispetto alle RNN che necessitano di processare a
timestep.

Come detto, il transformer e' sempre un'architettura Encoder-Decoder, per cui
l'input e l'output sono sequenze di token di parole. Sia gli encoder che i
decoder sono pensati come delle pile di 6 sotto-encoder/sotto-decoders. Questi
sottopassaggi possono essere visti come diversi layer, in cui il singolo layer
prende in input l'ouput di quello precedente e cosi' via. All'interno di ognuno
di questi layer degli encoder, sono presenti due layer:

1. Self Attention
2. Feed Forward

Mentre all'interno di quelli del decoder:

1. Self-Attention
2. Encode-Decoder Attention
3. Feed-Forward

## Encoder Layer

Vediamo di preciso cosa succede all'interno del layer dell'encoder. Prima di
tutto, in input si ricevono le parole per cui si fa un'operazione di word
embedding, ottenendo cosi' i vettori che rappresentano le parole.

>*A differenza delle reti ricorrenti, nei Transformers esiste un limite al
>numero di parole possibili*

Ognuno di questi vettori passa all'interno del layer di self attention, per cui
il risultato di questa operazione saranno delle *rappresentazioni alternative*
di ogni parola. Infine, queste rappresentazioni alternative vengono passate ad
un layer feedforward per ottenere l'output dell'encoder layer.

Si noti come il layer feedforward prenda in input tutte le singole
rappresentazioni alternativa, ma i parametri del layer sono sempre gli stessi,
per cui e' possibile parallelizzare questa operazione su tutte le
rappresentazioni alternative.

In realta', tra l'output della *self attention* e l'*input* ci sono delle
**connessioni residuali**, cioe' delle connessioni che portano parte dell'input
non processato all'output della self attention. Queste connessioni vanno a
finire come inpout di un *layer normalization*, che applica una normalizzazione
alla somma tra la matrice che contiene gli embeddings delle parole (ottenuta
mediante le connessioni residuali) e la matrice ottenuta dagli outputs della
self attention.

$$
LayerNorm(X + Z)
$$

Dove $X$ e' la matrice degli embedding e $Z$ e' la matrice di output della self
attention.

La layer normalization non fa altro che *rimuovere* la media da ogni elemento e
a normalizzare rispetto alla *varianza*. In altri termini:

$$
y=\frac{x-\mathbb{E}(x)}{\sqrt{Var(x)}}
$$

Lo scopo di questo layer e' quello di evitare che i gradienti esplodano o
svaniscano durante la *backpropagation*.

### Self Attention

Il layer di self attention e' il piu' importante di questa architettura. L'idea
e' la seguente: dato un *input* (una frase) si vorrebbe che il modello ponga
l'attenzione a specifiche *parti* (parole) dell'input.
Per ogni word embedding in input, il self attention va a creare 3 vettori
corrispondenti:

* Query
* Key
* Value

Questi vettori vengono creati semplicemente moltiplicando il vettore
dell'embedding della parola per delle matrici $W^Q, W^K, W^V$ ($Q$ per ottenere
la *query*, $K$ per ottenere la *key*, $V$ per ottenere il *value*).
Tipicamente, questi vettori risultanti sono piu' piccoli delle word embedding in
input.


\begin{align}
Q &= W^Q \cdot x\\
K &= W^K \cdot x\\
V &= W^V \cdot x\\
\end{align}

Questi vettori sono utilizzati per capire dove porre l'attenzione. Una volta
calcolati Q, K, V per ogni word embedding in input:

* Prendi la Q e moltiplicala (*DotProduct*) per tutte le K calcolate (sia la
  propria che quelle delle altre parole) ottenendo un vettore di score
* Dividi ogni elemento dello *score* per la radice della dimensione del vettore
  chiave $\sqrt{d_k}$ (e' semplicemente una normalizzazione)
* Applica una softmax al vettore *score* normalizzato, in modo da darci una
  distribuzione di probabilita' sull'input. Il valore massimo ci dira' su quale
  parola porre di piu' l'attenzione
* A questo punto (come per le reti ricorrenti), calcoliamo il prodotto
  element-wise tra i valori della softmax dello score per i vettori value di
  ogni parola corrispondente $(score_i \cdot V_i)$
* Infine, questi valori vengono sommati tra di loro in modo da ottenere un
  singolo vettore in output per ogni parola.

>*Si chiama self-attention perche' l'output pone l'attenzione sull'input stesso*

#### Versione Matriciale

Siccome gli acceleratori grafici sono molto veloci a fare moltiplicazioni di
matrici, la self attention sarebbe migliorabile se si riformulasse in termini di
operazioni di matrici. Per primo lugoo, le word embeddings possono essere messe
tutte in una stessa matrice $X (w \times v)$ in cui $w$ e' il numero di parole
della frase e $v$ e' il numero di parole del vocabolario. In questo modo, se
moltiplichiamo

* $X \cdot W^Q = Q$
* $X \cdot W^K = K$
* $X \cdot W^V = V$

Otteniamo le matrici $Q, K, V$ le cui singole righe sono i corrispondenti
vettori Query, Key, Values per ogni word embedding in input. Una volta che
abbiamo ottenuto queste matrici e' possibile calcolare l'output della
self-attention $Z$ in formato matriciale

$$
Z = softmax(\frac{Q \cdot K^T}{\sqrt{d_k}}) \cdot V
$$

in cui:

* $Q \cdot K^T$ mette in relazione tutte le parole con tutte le chiavi (e' la
  matrice di *score*, in cui la singola entrata $i, j$ indica l'attenzione che
  da' la parola $i$ alla parola $j$)

### Multi-Head Attention

Una delle altre cose introdotte dal modello transformer e' la *multi head
attention*, che non e' altro che applicare il meccanismo di self-attention piu'
volte. L'idea e' che applicare questo meccanismo piu' volte renda potenzialmente
il modello piu' potente, cosi' da far focalizzare ogni attention-head su
particolari domini. Ad esempio, possiamo pensare che un determinato
attention-head si focalizzi particolarmente sui nomi, uno a cosa si riferiscono
gli articoli, uno sul sesso, ecc.. In questo senso espande l'abilita' del
modello di concentrarsi su posizioni differenti.

Siccome ogni attention head produce un output della self attention, per ogni
parola ci saranno tanti ouput quanti gli attention head. Il problema e' che il
layer dopo la self attention (feed-forward) si aspetta in input un input di
dimensione fissa. Per risolvere questo problema, si concatenano tra loro tutti i
vettori risultato da ogni attention-head e si moltiplicano per una matrice
$W^O$.

$$
MultiHead(Q, K, V) = Concat(head_1, \dots, head_{h}) \cdot W^O
$$

dove:

$$
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
$$

e $h$ e' il numero di *attention-head*.

>*La trasformazionee lineare $W^O$ riporta la concatenazione di questi vettori alla dimensione adatta in input del layer feed-forward*.

### Positional Encoding

Da come vengono processati gli input (ogni embedding e' una riga di $X$), e'
evidente come l'ordine di questi embedding (delle righe) non cambi il risultato
della self-attention. 
L'ordine nel testo e' pero' importante, per cui e' necessario trovare un modo
per tenere conto anche delle posizioni in cui occorrono gli embedding (tokens).
Il *positional embedding* e' un modo per codificare le parole aggiungendo
l'informazione della posizione all'interno della frase. 
Per ottenere questi vettori, si prendono gli embeddings delle parole $x_i$ e si
sommano *element-wise* per dei vettori $t_i$ detti *positional encodings*.

>*Ogni $t_i$ e' creato esclusivamente per tenere conto dell'informazione
>posizionale dell'embedding*

I vettori di positional encoding vengono calcolati per posizioni pari e dispari
mediante due funzioni periodiche

\begin{align}
t_{(pos, 2i)} = sin(pos/1000^{2i/d_{model}})\\
t_{(pos, 2i+1)} = cos(pos/1000^{2i/d_{model}})\\
\end{align}

dove $pos$ e' la posizione e $i$ e' la dimensione.

>*In realta', la matrice di tutti i positional encodings viene appresa
>direttamente come parametro della rete.*

### Encoder-Decoder Layer

Il layer Encoder-Decoder attention funziona proprio come la mulit-headed
attention con l'eccezione che crea le proprie matrici $Q_i$ dal layer del
decoder precendete e prende le matrici $K_i$ e $V_i$ dall'output dell'ultimo
encoder. In un certo modo, simula cio'e che succede nei modelli ricorrenti
basati sull'attenzione.

(Per questo motivo *visualmente* l'ultimo encoder ha connessioni con tutti i
decoders)

Nel layer di self attention nel decoder, inoltre, e' permesso porre l'
attenzione solo su posizioni precedenti nella sequenza di output. Per cui si
pongono le posizioni future ad un valore pari a $-\infty$. Questa operazione e'
detta *masking*.

## GPT (Generative Pre-Training)
GPT e' un modello di OpenAI basato su Transformer. GPT si pone di risolvere al
problema che la quantita' di dati etichettati per un task specifico (Machine
Translation, Summarization, Q&A) siano di fatto molto scarse. La soluzione
proposta con GPT si basa sull'utilizzo di testo non strutturato per far
apprendere alla rete una rappresentazione universale del testo per poi (transfer
learning) riadattare il modello a risolvere determinati task, utilizzando la
minore quantita' di testo etichettato.

La prima idea e' chiamata *unsupervised pre-training*: Dato del testo non
etichettato, predici la prossima parola.

Sia $U = \{u_1, \dots, u_n\}$ il testo non etichettato, l'idea e' apprendere un
modello che massimizzi la seguente likelihood: 

$$
L_1(U) = \sum_i log \; P(u_i \; | \; u_{i-k}, \dots, u_{i-1}; \Theta)
$$

Cioe' si vuole massimizzare la probabilita' di ottenere $u_i$ dato il contesto
(tutte le parole che c'erano prima in una finestra di lunghezza $k$).
$P$ e' modellata da una rete neurale con parametri $\Theta$.

La seconda idea e' invece chiamata *supervised-finetuning*: Dato il modello
pre-trainato in modo non supervisionato, fanne il training con dati etichettati
in modo supervisionato per adattarlo al task specifico.
Si vuole essenzialmente massimizzare la funzione:

$$
L_2(C) = \sum_{(x, y)} log \; P(y \; | \;x^1,\dots,x^m)
$$

dove $P(y \; | \; x^1, \dots, x^m)$ e' approssimato con $softmax(h_l^m W_y)$, in
cui $h_l^m$ e' l'output dell'ultimo layer del transformer appreso in precedenza
e $W_y$ e' una matrice di pesi appresa per predirre $y$.
