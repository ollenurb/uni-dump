# Long Short Term Memory Networks
Come visto precedentemente, le Recurrent Neural Networks soffrono di un problema
tecnico: il vanishing/exploding gradient.
Le LSTM sono nate proprio per cercare di risolvere questso problema, per cui
sono classificate come Reti Ricorrenti.

L'idea che sta alla base di queste reti e' quella di avere una forma di memoria
che permetta ad ogni istante di tenere conto della storia passata.
L'informazione che viene mantenuta viene pero' regolamentata da dei *gates* che
decidono quale informazione tenere e quale buttare via (funzionano come un
filtro).

>*Le RNN **accumulano** tutta l'informazione precedente del layer hidden ad ogni
>passo, mentre le LSTM la **filtrano** senza accumularla indiscriminatamente*

Ogni cella LSTM riceve uno *stato* $C_{t-1}$ in input che contiene
l'informazione che si vuole propagare dallo stadio precedente. Si puo' pensare a
questo stato come ad una flusso di informazioni che arrivano dal livello
precedente. Passando per la cella, lo stato viene modificato man mano dai vari
gates, che sono:

* Output Gate
* Forget Gate

Una volta combinate tutte le trasformazioni, lo stato risultante $C_{t}$ viene
propagato in output allo stadio successivo.

#### Forget Gate
Il compito del forget gate e' quello di decidere quale informazione rimuovere
dallo stato della cella.
Prende in input:

* L'input $x_t$ corrente (nel caso di NLP, la $t$-esima parola)
* L'output $h_{t-1}$ della cella al passo precedente

E calcola l'output con la seguente relazione

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$

In cui $f_t$ e' un vettore delle stesse dimensioni dello *stato* $C_{t-1}$ le
cui componenti sono tra $[0, 1]$. Siccome questo vettore viene poi moltiplicato
*element wise* con lo stato $C_{t-1}$, i valori rappresentano intuitivamente
quanta informazione tenere/eliminare per ogni componente di $C_{t-1}$.
Il vettore $[h_{t-1}, x_t]$ e' la *concatenazione* tra $h_{t-1}$ e $x_t$.

>*Ovviamente, il vettore di pesi $W_f$ e il bias $b_f$ sono un parametro della
>rete.*

L'apprendimento di un forget gate fa imparare alla rete quanta informazione
filtrare per ogni passo.

### Input Gate
L'input gate decide invece quale informazione deve essere aggiunta allo stato
della cella. Per fare cio' calcola:

* L'*input gate*: vettore che decide quali elementi o meno devono essere
  aggiunti allo stato (e in che misura)

  $$
  i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
  $$

* Lo *stato candidato*: vettore che rappresenta il nuovo stato calcolato sulla
  base di quello precedente e l'input attuale

  $$
  \tilde{C}_t = tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
  $$

  Anche in questo caso, questi due vettori vengono combinati con un prodotto
  *element-wise* (*Prodotto Hadamard*)

  $$
  R = i_t \; \odot \; \tilde{C}_t
  $$

  In questo senso, $i_t$ dice quale informazione aggiungere allo stato e in che
  misura. Il risultato $R$ viene poi aggiunto con una *somma* allo stato della
  cella.


### Output Gate
L'output gate definisce l'output dell'intera cella LSTM $h_t$, ottenuto
combinando lo stato finale $C_t$. Funziona in maniera molto simile all'input
gate, per cui calcola:

* L'*output gate*: vettore che decide quali elementi o meno devono essere
  mantenuti nell'output
  $$
  o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
  $$
* L'*output* della cella
  $$
  h_t = o_t \odot tanh(C_t)
  $$

## Architettura Encoder-Decoder
Le singole celle LSTM se prese da sole sono semplicemente un componente di base
per delle architetture piu' complesse. In questa architettura, l'encoder si
occupa di processare l'input e  codificarlo in una rappresentazione piu'
succinta detta *summary vector*. Il decoder fa l'inverso, cioe' prende in input
il summary vector e da in output l'input originale (in questo caso una
sequenza). Ad esempio, nel task della machine translation, il decoder si occupa
di generare la traduzione a partire dal *summary vector.*

Nell'encoder, tutti gli output delle singole celle (considerando la versione
unwrapped) sono scartati, per cui si costruisce il summary vector utilizzando lo
stato $C_t$ dell'ultimo passo.

Come detto, il decoder non fa altro che fare l'inverso, cioe' ottenere parole
dalla rappresentazione sommaria. L'output pero' sono dei vettori, per cui per
ottenere la parola a partire dal vettore di output si applicano le seguenti
trasformazioni:

* Applica una trasformazione lineare in modo da espandere le dimensioni del
  vettore a quelle del vocabolario che si vuole utilizzare in output. Questo
  vettore in output e' chiamato `logits`
* Applica una *softmax* ad ogni elemento di `logits`, ottenendo un vettore di
  probabilita' (ogni parola ha associata una probabilita'). 

> *Il realta' si utilizza una log softmax per rendere il calcolo numericamente
> piu' stabile*

Infine, per ottenere la parola basta fare $argmax \; log\_probs$.

### Training (Teacher Forcing)
Per fare il training di questo tipo di architettura, si utilizza per prima cosa
una loss *Cross-Entropy*

$$
L = -\sum^n_{i=0} y_i \cdot log(\hat{y}_i)
$$

Dove $\hat{y}_i$ e' il risultato della rete al passo $i$

Durante la fase di training, al decoder viene data la sequenza di target $y$, ma
*shiftata* di una parola siccome la prima parola e' un token costante `<START>`.
In questo modo si puo' calcolare la loss per ogni passo.
In altri termini, si da ad ogni timestep in input al decoder la parola target al
passo precedente.

### Inferenza (Autoregressive Generation)
Nell'inferenza si fa una cosa simile al training del punto di vista del decoder.
Essenzialmente ogni output diventa l'input del prossimo timestep. Per cui si
lascia che il decoder generi l'intera frase da solo. 
Questo evidentemente puo' provocare problemi a cascata, per cui anche solo un
errore potrebbe invalidare l'intera sequenza risultato.

Il problema di questa architettura sta nell'encoder: Se la frase e' molto
grande, il summary vector (che ha grandezza finita) deve codificare
semanticamente molta informazione.
Per questo motivo, la rete tende a perdere informazioni sulle parti iniziali
delle frasi al crescere della lunghezza della frase.

Idealmente, si vorrebbe modificare l'architettura in modo da tenere conto di
**tutti** gli hidden state una volta arrivati alla fine dello stage di encoding,
e che ci sia un meccanismo nella fase di decoding che sfrutti queste
informazioni in modo intelligente. Questo meccanismo e' l'*attention*.

L'idea dell'attenzione puo' essere spiegata tramite un esempio. Ipotizziamo di
voler tradurre la seguente frase
 
>"*Are you free tomorrow?*" -> "*Sei libero domani?*" 

Il modello avrebbe molta piu' facilita' se esistesse un meccanismo per
concentrarsi sugli input in modo da generare le traduzioni corrispondenti.
Esempio: sto considerando *"libero"*, allora l'attenzione deve essere posta su
*"free"*.

### Attention
L'attenzione viene implementata solo sul *decoder* perche' e' il *decoder* che
riceve dall'*encoder* tutti gli hidden state delle parole in input su cui
applicare il meccanismo dell'attenzione.
Come primo step dell'attenzione, il decoder confronta l'hidden state al timestep
attuale con tutti gli hidden state risultanti dall'encoder, generando un vettore
di *score*s.
Come secondo passo, applica una softmax a questo vettore e moltiplicalo per ogni
vettore corrispondente agli hidden state dell'encoder.
Infine, somma tutti i vettori risultanti, ottenendo un singolo vettore detto
*context vector* (che e' l'output dell'attenzione).

> La softmax (detta anche softargmax) e' una funzione che normalizza un vettore
> di valori ad un vettore di probabilita' es. $A = (13, 9, 9)$ $$
  \begin{split} argmax(A) &= (1, 0, 0)\\ softmax(A) &= (0.96, 0.02, 0.02)
  \end{split} $$

Riassumendo:

1. Accumula tutti gli hidden states dell'encoder $h = (h_1, \dots, h_{Tx})$
2. La generica componente $j$ del vettore di score al generico passo $i$ del
   decoder e' calcolabile come ($d_i$ e' l'hidden state del decoder al passo
   $i$. $U_a, W_a$ sono dei parametri apprendibili dalla rete)

$$
e_{ij} = (W_a \cdot d_i) \cdot (U_a \cdot h_j)
$$

3. Applica la softmax al vettore $e$. Siccome il risultato e' una distribuzione
   di probabilita', ogni componente ci dira' in quale hidden state si deve porre
   di piu' l'attenzione

$$
a_{ij} = \frac{exp(e_{ij})}{\sum^{Tx}_{k=1} exp(e_{ik})}
$$

4. Moltiplica ogni hidden state dell'encoder per ogni elemento dello score
   corrispondente e somma i risultati per ottenere un unico vettore

$$
C_i = \sum^{Tx}_{j=1} a_{ij} h_j
$$

> *$C_i$ e' il generico elemento $i$ del vettore contesto $C$.*

In questo modo prendiamo solo la informazione che ci interessa sfruttando il
contesto (che e' solo un vettore).

L'output dell'attenzione viene calcolato ad ogni step e viene concatenato con
l'hidden state (risultato) della rete. Quello che si fa e' poi utilizzare il
meccanismo descritto in precedenza (word embedding) per trovare la parola piu'
probabile.

### Bidirectional Encoder-Decoder

Un'architettura tipica di questo tipo e' detta bidirectional ed e' costituita
nel modo seguente:

* Il primo layer e' un layer di *word embedding*, in cui vengono trasformate le
  parole in vettori numerici
* Il secondo layer e' un *encoder* composto da reti neurali ricorrenti, in cui
  le sequenze possono essere codificate non solo da sinistra a destra ma anche
  nella direzione inversa. Questo permette di ottenere rappresentazioni
  aggiuntive che potrebbero essere utili alla rete.
* Il terzo layer tra encoder e decoder e' il layer dell'attenzione in cui
  confluiscono i vettori contesto
* Il quarto layer e' il decoder che sfrutta l'attenzione per dare in output i
  vettori
* L'ultimo layer e' un layer di embedding che serve a tradurre i vettori
  numerici in parole

### Copy Mechanism
Cosa deve succedere se vogliamo una parola che non e' nell'output vocabulary ma
che compare nella input sentence?
Come soluzione, si vorrebbe che questa parola di input venga "copiata".

L'idea e' quella di prendere la distribuzione dell'attenzione (che per
definizione e' costruita a partire dall'input) e ogni qual volta che si vuole
copiare una parola non presente nel vocabolario, si fa semplicemente sampling
sulla distribuzione dell'attenzione (al posto di fare il sampling dalla
distribuzione data in output).

Per scegliere quando fare sampling tra una o l'altra delle due distribuzioni, si
introduce un *soft switch* $p_{gen}$.
$p_{gen}$ e' una parte della rete che viene anch'essa appresa e va in qualche
modo a decidere se generare la prossima parola dalla distribuzione
dell'attenzione oppure dalla distribuzione in output.

$$
p_{gen} = \sigma(w_p \; p_{gen, t-1} + W_s S_{t-1} + W_c C_t + W_o O_t)
$$

L'idea e' quella di generare una distribuzione finale che tenga conto di
entrambe le due distribuzioni: sia quella dell'*attenzione* che quella di
output. Per ottenerla, si fa la somma delle due distribuzioni, ognuna scalata
per i valori di $p_{gen}$. 
Intuiviamente, se $p_{gen}$ tende a 1, allora sara' piu' predominante la
distribuzione dell'*output*, mentre se tende a $0$ sara' piu' predominante
quella dell'*attenzione*.
In altri termini $p_{gen}$ regola quali tra le due devono favorite nella scelta.

$$
p_{final}(c) = p_{out} \; p_{gen} + (1 - p_{gen}) \sum_{i:c_i = c} a_i
$$

Ovviamente i parametri del soft switch sono apprendibili. Una volta appresa la
rete, si andra' a fare sampling direttamente da $p_{final}$ per ottenere le
parole in output.
