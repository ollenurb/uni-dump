{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vWhC1-mMyop"
   },
   "source": [
    "# Classifiers comparison on texts with naive Bayes assumption\n",
    "\n",
    "In this session of laboratory we compare two probabilistic models for categorical data: \n",
    "1. multivariate Bernoulli \n",
    "2. multinomial on a dataset \n",
    "\n",
    "We adopt a dataset on Twitter messages labelled with emotions (Joy vs Sadness).\n",
    "\n",
    "The following program shows the loading of the data from a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i70pgsCOMyop"
   },
   "source": [
    "Data are loaded into a matrix X adopting a sparse matrix representation, in order to save space and time.\n",
    "Sparse matrix representation (in the csr format) represents in three \"parallel\" arrays the value of the matrix cells that are different from zero and the indices of those matrix cells.\n",
    "The arrays are called: \n",
    "- data\n",
    "- row\n",
    "- col\n",
    "\n",
    "- data[i] stores the value of the matrix cell #i whose indexes are contained in row[i] and col[i] \n",
    "- row[i] stores the index of the row in the matrix of the cell #i, \n",
    "- col[i] stores the index of the column of the cell #i.\n",
    "\n",
    "Basically, it's equivalent to an associative array like this [(i, j) ~> data]. Sparse matrices are matrices whose at least 50% of their elements are empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sktoeaw5Myop"
   },
   "source": [
    "The data file is in csv format.\n",
    "Any Twitter message has been preprocessed by a Natural Language pipeline which eliminated stop words and substituted the interesting document elements with an integer identifier.  \n",
    "The interesting document elements might be words, emoji or emoticons. The elements could be repeated in the same document and are uniquely identified in the documents by the same integer number (named \"element_id\" in the program). This \"element_id\" number will be used as the index of the column of the data matrix, for the purposes of storage of data.\n",
    "\n",
    "Each row of the CSV file reports the content of a document (a Twitter message).It is formed as a list of integer number pairs, followed by a string which is the label of the document (\"Joy\" or \"Sadness\").\n",
    "The first number of the pair is the identifier of a document element (the \"element_id\"); \n",
    "the second number of the pair is the count (frequency) of that element in that document.\n",
    "\n",
    "An example of a line is:\n",
    "38,3,264,1,635,1,2780,1,Joy\n",
    "\n",
    "where 38 is the identifier of the first word occurring in that message, and 3 is the number of times (frequency count) in\n",
    "which that word is present in that message.\n",
    "38, 264, 635, 2780 are the identifiers of the words and 3, 1, 1, 1 are the respective frequencies in that message.\n",
    "\n",
    "The dataset has:\n",
    "tot_n_docs (or rows in the file) = n_rows = 11981\n",
    "n_features (total number of distinct words in the corpus) = 11288 (or vocabulary size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Phlf-MPAMyop"
   },
   "source": [
    "The following program reads the data file and loads in a sparse way the matrix using the scipy.sparse library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMyi3U-ZMyop",
    "outputId": "39724efe-040d-470e-dd16-6ccf319e797a"
   },
   "outputs": [],
   "source": [
    "\n",
    "from numpy import ndarray, zeros\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class_labels = [\"Joy\",\"Sadness\"]\n",
    "n_features = 11288                                    # number of columns in the matrix = number of features (distinct elements in the documents)\n",
    "n_rows = 11981                                        # number rows of the matrix\n",
    "n_elements = 71474                                    # number of the existing values in the matrix (not empty, to be loaded in the matrix in a sparse way)\n",
    "\n",
    "path_training = \"../datasets/\"\n",
    "file_name = \"joy_sadness6000.txt\"\n",
    "\n",
    "# declare the row and col arrays with the indexes of the matrix cells (non empty) to be loaded from file\n",
    "# they are needed because the matrix is sparse and we load in the matrix only the elements which are present\n",
    "row = np.empty(n_elements, dtype=int)\n",
    "col = np.empty(n_elements, dtype=int)\n",
    "data = np.empty(n_elements, dtype=int)\n",
    "\n",
    "row_n = 0                                             # Number of current row to be read and managed\n",
    "cur_el = 0                                            # Position in the three arrays: row, col and data\n",
    "twitter_labels = []                                   # List of class labels (target array) of the documents (twitter) that will be read from the input file\n",
    "twitter_target = []                                   # List of 0/1 for class labels\n",
    "with open(path_training + file_name, \"r\") as fi:\n",
    "    for line in fi:\n",
    "        el_list = line.split(',')                     # List of integers read from a row of the file\n",
    "        l = len(el_list)\n",
    "        last_el = el_list[-1]                         # Grab the last element in the list which is the class label\n",
    "        class_name = last_el.strip()                  # Eliminate the '\\n'\n",
    "        twitter_labels.append(class_name)\n",
    "\n",
    "        # Twitter_labels contains the labels (Joy/Sadness);\n",
    "        # twitter_target contains 0/1 for the respective labels\n",
    "        if (class_name == class_labels[0]):\n",
    "           twitter_target.append(0)\n",
    "        else:\n",
    "           twitter_target.append(1)\n",
    "\n",
    "        # Start by reading all the doc elements from the beginning of the list\n",
    "        i = 0\n",
    "        while (i<(l-1)):\n",
    "            element_id = int(el_list[i])             # Identifier of the element in the document equivalent to the column index\n",
    "            element_id = element_id-1                # The index starts from 0 (the read id starts from 1)\n",
    "            i = i+1 \n",
    "            value_cell = int(el_list[i])             # Make access to the following value in the file which is the count of the element in the document \n",
    "            i = i+1\n",
    "            row[cur_el] = row_n                      # Load the data in the three arrays: the first two are the row and col indexes; the last one is the matrix cell value\n",
    "            col[cur_el] = element_id\n",
    "            data[cur_el] = value_cell\n",
    "            cur_el = cur_el+1\n",
    "            # i-th row represent the tweet, j-th col represent the word. \n",
    "        row_n = row_n+1\n",
    "fi.close\n",
    "\n",
    "# Convert the target into a numpy array for later convenient use\n",
    "twitter_target = np.array(twitter_target)\n",
    "\n",
    "# loads the matrix by means of the indexes and the values in the three arrays just filled\n",
    "twitter_data = csr_matrix((data, (row, col)), shape = (n_rows, n_features)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "v4FHZONyMyoq"
   },
   "source": [
    "Write a program in the following cell that splits the data matrix in training and test set (by random selection) and predicts the class (Joy/Sadness) of the messages on the basis of the words. \n",
    "Consider the two possible models:\n",
    "multivariate Bernoulli and multinomial Bernoulli.\n",
    "Find the accuracy of the models and test if the observed differences are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "c44Vk519Myoq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset between training/test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(twitter_data, twitter_target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now focus on building the Multivariate model, which is called `BernoulliNB` inside the ScikitLearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9504867872044507"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "multivariate = BernoulliNB()  \n",
    "multivariate.fit(X_train, y_train)\n",
    "\n",
    "# Compute the accuracy of the classifier\n",
    "multivariate.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same thing, but with a Multinomial Bernoulli model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9471488178025035"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "multinomial = MultinomialNB()  \n",
    "multinomial.fit(X_train, y_train)\n",
    "\n",
    "# Compute the accuracy of the classifier\n",
    "multinomial.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the multivariate bernoulli model seems to perform better on on the test set.\n",
    "\n",
    "## Evaluating differences using *paired t-test*\n",
    "From the observed difference we can say that the multivariate model is performing better than the multinomial. Now we test those assumptions using a statistical test called *paired t-test*.\n",
    "The main idea is to perform a KFold and register the differences on performance between each fold. Then we assume (*null hypothesis*) that the distribution has mean $\\mu = 0$ with unknown variance. To account for this additional uncertainty, we model the distribution of the performance differences as a t-student distribution with *degrees of freedom* equals to the number of folds minus one. ($k-1$) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cross validation on the Multivariate model\n",
      "Running cross validation on the Multinomial model\n",
      "The p_value is: 0.0050500833362442845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from scipy.stats import ttest_rel\n",
    "import time\n",
    "\n",
    "k = 10\n",
    "\n",
    "# Declare the folds\n",
    "kfold = KFold(n_splits=k)\n",
    "print('Running cross validation on the Multivariate model')\n",
    "scores_mv = cross_val_score(BernoulliNB(), twitter_data, twitter_target, cv = kfold)\n",
    "print('Running cross validation on the Multinomial model')\n",
    "scores_mn = cross_val_score(MultinomialNB(), twitter_data, twitter_target, cv = kfold)\n",
    "    \n",
    "# Now we can perform the t-test using the SciPy's library\n",
    "_, p_value = ttest_rel(scores_mv, scores_mn)\n",
    "# Print the p_value\n",
    "print(f'The p_value is: {p_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $p < 0.05$ we reject the null hypothesis, therefore concluding that there are no differences between the Multivariate and the Multinomial models with a significance level of $\\alpha = 0.05$ "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "classification_twitter_emotions_loading_data.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "6008dd7d2032d09ab95516ce981bc44e3e23d76be5bd99d4c5c78840bb39c63a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
