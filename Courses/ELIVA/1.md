# Introduzione

## Il sistema visivo umano
Le principali componenti dell'occhio umano sono:

* **Cornea** e **sclera**: la cornea è un tessuto duro e trasparente che copre
  la superficie anteriore dell'occhio, mentre la sclera è una membrana opaca che
  racchiude il resto dell'occhio che non è coperto dalla cornea. 
* **Coroide**: tessuto ricco di vasi sanguigni per il trasporto di nutrienti
  alle cellule dell'occhio. L'**iride** è parte della coroide e permette di
  controllare la quantità di luce che entra nell'occhio per mezzo di
  contrazioni/espansioni, la sua funzione è quella di mettere a fuoco gli
  oggetti del mondo reale.
* **Lente**: la lente dell'occhio è un tessuto fibroso da cui passa la luce,
  questo tessuto assorbe relativamente poca luce visibile e molta luce non
  visibile.
* **Retina**: membrana che ricopre l'interno dell'occhio su cui viene riflessa
  l'immagine che si sta vedendo. Essa è costituita da:
  * **Coni**: recettori molto sensibili al colore. Sono localizzati in una zona
    centrale della retina detta *fovea*. C'è ne sono tra i 6 e 7 milioni. La
    vista di questi recettori è detta *fotopica*.
  * **Bastoncelli**: molto più numerosi dei coni, tra i 75 e 150 milioni. Molto
    sensibili all'intensità luminosa, servono a creare una visione generale
    dell'immagine (utile in condizioni di scarsa luminosità), detta anche
    visione *scotopica*.

Il numero di bastoncelli e coni decresce a 0 a circa 20 gradi di distanza (a
sinistra) della fovea, chiamato *blind spot* cioè il punto in cui il nervo
ottico è connesso alla retina. I coni sono più densi al centro della retina,
decrescendo via via che ci si sposta (in gradi) verso destra e sinistra. D'altra
parte, il punto con meno bastoncelli è la fovea, mentre crescono
esponenzialmente per raggiungere il massimo a circa 20 gradi e per poi
decrescere mano a mano. Tendenzialmente ciò riprende il fatto che i bastoncelli
sono più presenti in numero per garantire una migliore visione periferica,
mentre i coni per garantire una fedeltà migliore dell'immagine che si sta
direttamente guardando.

La concentrazione della sensibilità dei colori dei coni è la seguente:

* 65% sensibili al giallo-arancione.
* 33% sensibili al verde.
* 2% sensibili al blu.

Il colore percepito è la somma di questi colori primari. Inoltre, la
concentrazione di coni ripartita in questo modo non permette di percepire tutti
i colori nello spettro della luce visibile.

## Fondamenti di immagini digitali
Un'immagine bidimensionale è una rappresentazione nel piano di una sena o di un
fenomeno fisico che ha luogo nel mondo reale. Formalmente possiamo descriverla
come una funzione $f(x, y)$ il cui valore è una quantità scalare positiva il cui
significato fisico dipende dalla sorgente dell'immagine. La funzione $f(x, y)$
può essere caratterizzata dal prodotto di due componenti: la quantità di
*illuminazione* $i(x, y)$ della scena vista e la quantità di *riflettenza* $r(x,
y)$ degli oggetti visti nella scena. Ovviamente, $i(x, y)$ è determinata dalla
sorgente di illuminazione, mentre $r(x, y)$ è determinata dalle caratteristiche
degli elementi che sono presenti nella scena.
Possiamo definire diverse tipologie di immagini:

* **Visibili**: possono essere percepite da un sistema visivo umano oppure ottico.
* **Non direttamente visibili**: create da distribuzioni bi-dimensionali di
  quantità fisiche quali ad esempio temperatura, pressione o campo elettrico.
* **Definite Analiticamente**: definite da funzioni continue a due variabili
  oppure da funzioni discrete a due indici.

Le immagini possono essere ulteriormente categorizzate in immagini:

* **Analogiche**: caratterizzate dall'evoluzione *continua* dei valori $x,y$
  nello spazio, e dei valori di $f(x, y)$ che definiscono il colore/intesità
  luminosa.
* **Digitali**: sono ottenute discretizzando i valori di $f$ e dei valori di $x$
  e $y$ nello spazio. Il *campionamento* (sampling), è il processo di
  discretizzazione spaziale (dei valori $x$ e $y$) e la *quantizzazione*
  (quantization), è la digitalizzazione del valore di $f$ all'intero più vicino.

Le immagini salvate come una serie di linee (o righe) di pixels sono dette
immagini *raster* (aka *bitmap format*). L'acquisizione di un immagine digitale
prevede un processo di campionamento della grandezza fisica di riferimento (in
quanto grandezza fisica continua). Questo processo può essere visto
concettualmente come la superimposizione di una griglia bi-dimensionale. La
grandezza delle singole celle determina direttamente la qualità dell'immagine.
La scelta della grandezza delle celle determina la qualità dell'immagine in un
certo senso. Più esse sono grosse, più si accentua il fenomeno in cui le
immagini diventano *pixelate*. Questa grandezza è determinata direttamente dal
sensore di acquisizione.

Per rappresentare un'immagine digitale, è necessario di fatto salvare in una
matrice i valori (quantizzati) di $f(x, y)$.
Il numero di bit che si scelgono per rappresentare questi valori determina il
*color depth*, rappresentato in *bit per pixel* (bpp). In caso $Im(f) = \{0, 1,
\dots, L-1 \}$, saranno necessari $log_2(L)$ bpp per rappresentarne i valori.
Se abbiamo immagini catturate in *grayscale*, di solito si utilizza $1$ byte
($8$ bpp). Molto comuni invece sono immagini catturate in RGB (chiamati anche
*additive primaries*, in caso siano in CMY sono chiamate *subtractive
primaries*), in questo caso si utilizza un byte per canale, per cui ogni pixel
richiederà $24$ bit. Il numero totale di colori rappresentabile con $24$ bit è
($2^8$ per ogni componente spettrale) $2^8 \cdot 2^8 \cdot 2^8 = 2^{24}$ colori
possibili (circa 16.000.000).

> In generale, per calcolare il numero di bit per pixel, facciamo il logaritmo
> in base 2 del numero *massimo* di valori rappresentabile.

Siccome le imagini non sono altro che la rappresentazione di una grandezza
fisica, vien da sè che è possibile fare diverse operazioni su di esse. Prime tra
tutte sono le operazioni logiche quali `and` o `or`. Altre operazioni sono la
somma, differenza, moltiplicazione e divisione. Il risultato è definito
applicando l'operazione ($\cdot$) nel modo seguente $h(x, y) = f(x, y) \cdot
g(x, y)$.

Un caso di applicazione di queste operazioni è ad esempio la riduzione del
rumore per mezzo della media (si veda esempio sulle slides).
Altre tipologie di operazioni sono le **trasformazioni affini**, che sono in
grado di mantenere line e parallelismi. Appartententi a questa famiglia di
trasformazioni sono le traslazioni, rotazioni, ridimensionamenti e ritagli.

Altro concetto molto importante è la **risoluzione** di un'immagine, che è in
funzione della *grandezza* dell'immagine ($W \times H$) e della sua dimensione
*spaziale*. La risoluzione di un'immagine viene tipicamente espressa in *dots
per inch* (dpi). In generale, per calcolare la risoluzione si fa $resolution =
\frac{size}{spatial dimension}$.

Come citato in precedenza, il campionamento del segnale analogico periodico
(cioè che cambia nel tempo) determina in modo diretto quanto questo segnale
viene rappresentato fedelmente a livello digitale. Tanto più frequentemente si
acquisiscono i campioni di un segnale, tanto più questo sarà rappresentato
fedelmente, valendo invece l'inverso se campionato troppo infrequentemente. Il
teorema di **Nyquist-Shannon** ci da la certezza teorica che un segnale che
contiene frequenze più piccole di $f_c$ Hertz, può essere ricostruito
completamente da campioni presi a intervalli non più grandi di $\frac{1}{2f_c}$
secondi.
