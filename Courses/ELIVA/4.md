# Image Enhancement nel dominio spaziale
Le tipologie di image enhancement si suddividono in tipologie di trasformazione
che possono essere:

* **Puntuali**: trasformazione da pixel a pixel, rappresentabili come una mappa
  $(x_t, y_t) = f(x, y)$;
* **Locali**: trasformazione da insieme di pixel a pixel. L'insieme di pixel è
  rappresentato da una *funzione di neighborhood* $I(x, y)$, per cui la
  trasformazione è rappresentata dalla mappa $(x_t, y_t) = f(I(x, y))$;
* **Globali**: trasforma un'intera immagine in un singolo punto.

## Intensity Transformation
Una tipologia di image enhancement è l'*Intensity Transformation*. Abbiamo visto
questa trasformazione in particolare per le immagini rappresentate in scala di
grigi. Una trasformazione puntuale $T$ di questo tipo, mappa ogni valore di
intensità in un altro valore. Questa funzione può essere implementata attraverso
una tabella di conversione chiamata LUT (*Look Up Table*).
Queste tabelle vengono rappresentate per mezzo di un grafico bidimensionale che
esprime i nuovi valori di $T$ in corrispondenza dei vari valori di grigio.

Una trasformazione $T$ molto importante è la *funzione gamma*, che implementa la
cosiddetta *gamma correction*. Tale operazione permettere di
espandere/comprimere i livelli di grigio scuri/chiari.
La funzione gamma è la seguente:
$$
s = c \cdot r^{\gamma}
$$
dove:

* $c$ è una costante pre-definita;
* $r$ è il livello di intensità di grigio in input;
* $\gamma > 0$  è l'iperparametro della funzione.

Valori di $\gamma > 1$ determinano un'amplificazione dei livelli di grigio
scuri, attenuando il livello di quelli chiari. D'altra parte, con $0 < \gamma <
1$ si ottiene un'amplificazione dei livelli di grigio chiari, attenuando quelli
scuri.

Altre trasformazioni degne di nota di questo tipo sono:

* **Ladder**: grafo a *"scaletta*" che riduce il numero di livelli di intensità,
  creando dei contorni falsi. Questa trasformazione ha il problema che introduce
  un tipo di compressione irreversibile perché si perde informazione data dalle
  linee verticali della funzione (i "salti");
* **Ramp**: grafo che corrisponde ad una funzione a tratti collegata da una
  rapida ascesa tra i valori minimi e massimi di intensità in un intervallo
  compreso tra $r_1$ e $r_2$. Questa trasformazioe essenzialmente serve a
  ridurre il contrasto dei pixel che hanno intensità troppo bassa ($<r_1$) o
  troppo alta ($>r_2$). L'operazione è detta *contrast stretching*;
* **Binarization**: come suggerisce il nome, trasforma i valori di intensità in
  intensità massima (1) e minima (0). Il grafico di tale LUT è una funzione a
  tratti.

Fin'ora si è parlato di LUT per trasformazioni di immagini in scala di grigi.
Nei casi invece di immagini a colori, si impiegano le *pseudocolor* LUT; tabelle
che hanno una LUT per ogni canale di colore.

## Bit-Plane Slicing
La tecnica di bit-plane slicing consiste nel rappresentare un'immagine (a scala
di grigi) inizialmente in binario e poi creare un'immagine binarizzata per ogni
posizione di bit di ogni pixel. Ad esempio, se ci concentriamo su un dato pixel
con valore $010$, l'operazione produrrà 3 piani in cui lo stesso pixel avrà
valore $0$ nel primo, $1$ nel secondo e infine $0$ nel terzo piano.

Concentrandosi ad esempio sui piani dei bit meno significativi si può ridurre il
rumore, mentre su quelli più significativi si potrebbero enfatizzare i dettagli.
Si può utilizzare questa tecnica anche per fare compressione, permettendo di
salvare spazio, pur mantenendo le features dell'immagine intatte.

## Histogram Processing
È una tecnica che ricade nelle tecniche di *global processing*. Innanzitutto,
l'istogramma dei livelli di grigio di un'immagine è una funzione discreta $h$
definita come
$$
h(r_k) = n_k
$$
dove:

* $r_k$: è $k$-esimo livello di grigio ($r_k \in [0; 255]$ nel caso siano 8bpp);
* $n_k$: è il numero (*frequenza*) di pixel nell'immagine con intensità $r_k$.

Tipicamente, per far si che i valori siano tutti tra $0$ e $1$, si utilizza una
versione normalizzata dell'istrogramma:
$$
p(r_k) = \frac{n_k}{M \cdot N}
$$

dove $M$ sono il numero di pixel per riga e $N$ il numero di pixel per colonna.

La versione normalizzata di un istogramma definisce la probabilità che un
qualsiasi pixel abbia uno specifico valore di grigio $r_k$.
La sua rappresentazione grafica dà una descrizione globale dell'immagine. In
questo senso possiamo anche definire la *frequenza cumulativa* (o Cumulative
Density Function)
$$
F(r_k) = \sum_{i=0}^k p(r_i)
$$
Gli istogrammi riassumono quindi la distribuzione dell'intensità di pixel
nell'immagine. Ad esempio, istogrammi di immagini molto scure, avranno una
distribuzione *skewed* verso valori bassi di $r_k$. Altri casi interessanti sono
gli istogrammi delle immagini a basso contrasto (che appariranno come una
distribuzione maggiormente concentrata verso i valori centrali), e quelle a
basso contrasto (con una distribuzione molto omogenea su tutti i possibili
valori).

### Histogram Equalization
Come suggerisce il nome, il metodo della histogram equalization serve ad
aumentare il *contrasto* globale dell'immagine. Lo scopo di questa tecnica è
quindi quello di modificare le intensità dei pixel in modo che l'istogramma
risultante sia il più possibile uniforme. Di conseguenza, i valori dei pixel
dell'immagine saranno quindi distribuiti più uniformemente, sfruttando l'intero
range dei valori disponibili.

Siano $p_r(r_k)$ la distribuzione dell'immagine originale e $p_s(s_k)$ la
distribuzione target (*uniforme*). La trasformazione $s = T(r)$ per fare
histogram equalization è:
$$
T(r) = s = (L-1) \int^r_0 p_r(z)dz
$$
È possibile verificare che $T$ genera di fatto in output una distribuzione
uniforme sfruttando un un risultato della teoria delle probabilità:
$$
p_s(s_k) = p_r(r_k) \left | \frac{dr}{ds} \right |
$$
ma siccome
$$
\begin{aligned}
\frac{ds}{dr} &= \frac{dT(r)}{dr}\\
&= (L-1) \frac{\left[ \int^r_0 p_r(z)dz \right]}{dr}\\
&= (L-1) p_r(r)\\
\end{aligned}
$$
allora $\frac{dr}{ds} = \frac{1}{(L-1) p_r(r)}$, per cui possiamo sostituirlo
nell'equazione precedente e verificare se la distribuzione risultante è
uniforme:
$$
\begin{aligned}
p_s(s_k) &= p_r(r_k) \left | \frac{dr}{ds} \right |\\
&= (L-1) \left | \frac{1}{(L-1) p_r(r)}\right |\\
&= \frac{1}{(L-1)}\\
\end{aligned}
$$
È evidente che l'espressione coincide con quella di una distribuzione uniforme,
per cui $T$ è la trasformazione necessaria.

Si è trattato attualmente il caso le variabili siano continue (si noti
l'integrale), per ottenere la stessa trasformazione nel discreto è sufficiente
sostituire alla formula della trasformazione l'integrale con la distribuzione
cumulativa ed eventualmente normalizzate per ottenere una probabilità.
L'istogramma equalizzato sarà quindi ottenuto applicando la seguente
trasformazione
$$
T(r_k) = (L-1)\frac{H(r_k)}{MN}
$$
dove:

* $H(r_k) = \sum_{i=0}^k h(r_i)$ è *l'istogramma cumulativo*[^2];
* $M$ numero di colonne e $N$ numero di righe;
* $L-1$ livelli di intensità (255 per immagini a 8bpp).

[^2]: È come la distribuzione cumulativa, con la differenza che non viene fatta
    rispetto alle *probabilià*, ma rispetto alle *frequenze*.

Un problema di questa tecnica è che queste trasformazioni potrebbero mappare più
valori di intensità sullo stesso valore, di fatto causando una perdita di
dettagli nell'immagine originale.

### Histogram Specification
L'histogram specification consiste nel mappare i valori di intensità in modo da
cambiare la forma dell'istogramma, senza limitarsi solamente ad una forma
uniforme come nell'equalization. In altri termini, l'histogram equalization non
è nient'altro che un caso specifico di histogram specification, in cui la forma
desiderata è una distribuzione uniforme.

Si vuole trasformare l'istogramma in un istogramma con distribuzione *target*
$p_z(z)$. Si può seguire lo stesso ragionamento dell'*histogram equalization*, e
ottenere la trasformazione $G(z)$ come
$$
G(z) = s = (L - 1) \int_0^z p_z(v) dv
$$
Si ha quindi una trasformazione $T: r \to s$ discussa precedentemente, e una
trasformazione $G: z \to s$. Quello che si vuole ottenere è però una
trasformazione da $r$ a $z$ ($r \to z$).
$$
G: z \to \mathbf{s} \leftarrow r: T 
$$
Per ottenerla basta fare l'inversa $G^{-1}$, per cui la mappa per ottenere $z$ a
partire da $r$ sarà definita nel modo seguente
$$
z = G^{-1}(T(r))
$$
Per fare histogram specification bisogna eseguire 3 passi ben definiti:

1. Utilizza l'immagine originale per ottenere $T(r)$;
2. Utilizza la distribuzione desiderata $p_z(s)$ per calcolare $G(z)$,
   invertendola successivamente;
3. Per ogni pixel, ottieni la sua traformazione $s$ applicando $T(r)$, poi
   applica $G^{-1}$ al risultato $s$, ottenendo $z$.

Valgono le considerazioni fatte in precedenza per il caso discreto, per cui
tutte le trasformazioni equivalenti nel discreto possono essere ottenute da
quelle nel continuo, semplicemente sostituendo le distribuzioni cumulative
continue con quelle discrete nelle varie trasformazioni.

## Filtri Spaziali
Un'altra tecnica di image enhancement consiste nell'applicare dei filtri
spaziali. I filtri spaziali permettono di fare un processing *locale*
all'immagine. L'idea è quella di definire una maschera bidimensionale che
determina *"quanto"* la singola intensità deve essere amplificata o ridotta, e
successivamente sovraimporre tale maschera all'immagine scorrendo via via tutte
le possibili posizioni. Questa operazione è detta *convoluzione* ed è
riassumibile (nel caso bidimensionale e discreto) dalla seguente formula
$$
(f \star h)(n, m) = \sum_{k = -a/2}^{a/2} \sum_{l = -b/2}^{b/2} x(n - k, m -
l) h(k, l)
$$
dove:

* $h(k, l)$ è la *maschera* definita nella regione $(-a, a)$, $(-b, b)$;
* $x(n, m)$ è l'intensità del pixel alla posizione $(n, m)$.

Ovviamente bisogna tenere conto dei pixel ai bordi dell'immagine, poiché la
maschera potrebbe "*uscire*" al di fuori dell'immagine. In questi casi si
effettua un'operazione di *0-padding*, cioè si estende l'immagine opportunamente
di valori pari a 0.

In generale, i valori di $h$ sono normalizzati a $1$. Alternativamente, è
necessario dividere la formula per la somma di tutti i pesi della maschera. I
valori indicano se la tipologia del del filtro è *passa-basso* o *passa-alto*.
Ragionevolmente, valori $=1$ lasciano passare il segnale inalterato, mentre $=0$
lo filtrano.

Possiamo vedere questo tipo di processamento essenzialmente come una media
pesata specificata dalla maschera (o filtro).
Dal punto di vista operativo, i filtri passa basso vanno a rimuovere i dettagli
dell'immagine, evidenziando aree grandi nell'immagine.

### Ordered Statistics Filters
Un'altra tipologia di filtri è quella basata sulle statistiche ordinate come la
*mediana*. Un filtro mediano funziona andando a fare una statistica dei pixel
definiti nella finestra del filtro, ordinandoli in ordine ascendente, e
prendendo il valore centrale come l'output del fitro. Questo tipo di filtri è
particolarmente utile per rimuovere del rumore di tipo *salt&pepper*.

### Filtri Passa Alto
Vengono utilizzati in task come *sharpening* o *contour extraction*. Questa
tipologia di filtri viene ottenuta andando a simulare un operatore di
derivazione. Sono stati visti essenzialmente 2 operatori basati su questa
tecnica: il Laplaciano e il Jacobiano.

#### Operatore Laplaciano
Il primo operatore visto è il *Laplaciano*. Questo operatore permette di fare
*contour extraction*, cioè evidenzia i contoni (o bordi) degli oggetti presenti
nelle immagini. L'idea nasce dal fatto che la derivata seconda è proporzionale
alle variazioni di intensità nell'immagine. Essa è difatti $\neq 0$ solo in
prossimità di cambiamenti dei valori di intensità.
Seguendo questa intuizione, si introduce il Laplaciano, un operatore che
approssima la derivata seconda di una funzione[^3]
$$
\nabla^2 f = \frac{\partial^2 f}{\partial x^2} = f(x+1) + f(x-1) - 2f(x)
$$

[^3]: Dalla definizione di derivata seconda $f''(x) = \lim\limits_{h \to 0}
    \frac{f(x + h) - 2f(x) + f(x - h)}{h^2}$

nel caso delle immagini si utilizza il filtro Laplaciano isotropico (cioè un
filtro *invariante rispetto alle rotazioni*).
$$
\nabla^2 f =\frac{\partial^2 f}{\partial x^2} \frac{\partial^2 f}{\partial y^2}
    = f(x+1, y) + f(x-1, y) + f(x, y+1)+f(x, y -1) - 4f(x, y)
$$
La definizione di questa derivata non è nient'altro che una somma pesata, per
cui è possibile definire un kernel (maschera), che potrà essere combinato con
l'immagine attraverso un'operazione di convoluzione. Il kernel risultante sarà
composto dai coefficienti moltiplicativi dei valori di $f$ presenti nella
definizione
$$
h = 
\begin{bmatrix}
0 & 1 & 0 \\ 
1 & -4 & 1 \\ 
0 & 1 & 0
\end{bmatrix}
$$
Si noti che il kernel precedente non è nient'altro che la somma dei due kernel
calcolati su una specifica direzione
$$
h = 
\begin{bmatrix} 
0 & 1 & 0 \\ 
0 & -2 & 0 \\ 
0 & 1 & 0
\end{bmatrix} + 
\begin{bmatrix}
0 & 0 & 0 \\ 
1 & -2 & 1 \\ 
0 & 0 & 0
\end{bmatrix}
$$
È possibile quindi definire il Laplaciano come
$$
\nabla^2 f(x, y) = f(x, y) \star h
$$
Una volta definito il Laplaciano, possiamo utilizzarlo per fare image
sharpening.
$$
g(x, y) = f(x, y) + c \cdot \nabla^2 f(x, y)
$$
in cui $g$ è l'immagine risultato dell'operazione, mentre $f$ è l'immagine
originale. $c \in [-1; 1]$ è una costante (tipicamente non si scelgono valori
grandi poiché aumenterebbero il rumore nell'immagine). L'evidenza che
l'operatore faccia sharpening è data dal fatto che la derivata seconda assume
valori più grandi in prossimità dei punti in cui ci sono bruschi cambiamenti di
intensità. L'operazione rinforzerà quindi i bordi degli oggetti nell'immagine,
oscurando (rendendo nero) lo sfondo. Il fatto che si aggiunge il Laplaciano può
essere visto intuitivamente come l'aggiunta di un segnale che ha valori alti su
porzioni in cui cambia bruscamente l'immagine, mentre valori bassi in tutte le
altre porzioni.

#### Operatore Gradiente
Il gradiente è invece la derivata prima di una funzione a più variabili (nel
nostro caso parliamo di funzioni bivariate). 
Non è nien'altro che un *vettore* a due componenti che punta nella direzione che
ha l'intensità massima[^4].
$$
\nabla f = \frac{\partial f}{\partial x}\vec{i} + \frac{\partial f}{\partial y}
\vec{j}
$$

[^4]: Dove $\vec{i}, \vec{j}$ sono i *versori*.

Per ottenere una singola componente numerica, si considera il modulo $M$ del
gradiente
$$
M(x, y) = \sqrt{\left( \frac{\partial f}{\partial x} \right )^2 + \left(
\frac{\partial f}{\partial y} \right )^2}
$$
Spesso però, per motivi di linearità, se ne considera la seguente
approssimazione
$$
M(x, y) = \left| \frac{\partial f}{\partial x} \right | + \left|
\frac{\partial f}{\partial y} \right |
$$

Tra gli operatori che vanno a calcolare il gradiente abbiamo l'operatore di
*Roberts*, che calcola la derivata sulla direzione *diagonale*. Le maschere che
implementano l'operatore di Roberts sono le seguenti
$$
\underbrace{
\begin{bmatrix}
-1 & 0 \\ 
0 & 1
\end{bmatrix}}_{\text{Direzione } \searrow}
\quad \quad
\underbrace{
\begin{bmatrix}
0 & -1\\ 
1 & 0
\end{bmatrix}}_{\text{Direzione } \swarrow}
$$
È importante sottolineare che il pixel che viene considerato ogni volta durante
l'operazione di convoluzione, deve essere "allineato" alla componente $(0,0)$
della maschera. Il problema è che essendo l'operatore di Roberts un operatore
con dimensioni pari, non esiste un centro ben definito. Proprio per il fatto che
utilizzare maschere di dimensioni pari provoca questo problema, si preferisce
utilizzare l'operatore di *Sobel*, che è invece definito da una maschera $3
\times 3$. Gli operatori di Sobel non si concentrano sulle componenti diagonali,
ma sulle direzioni verticali e orizzontali. Le seguenti maschere implementano
l'operatore di *Sobel* che enfatizzano queste direzioni
$$
\underbrace{
\begin{bmatrix}
-1 & -2 & -1 \\ 
0 & 0 & 0 \\ 
1 & 2 & 1
\end{bmatrix}}_{\text{Direzione Orizzontale}}
\quad\quad
\underbrace{
\begin{bmatrix}
-1 & 0 & 1 \\ 
-2 & 0 & 2 \\ 
-1 & 0 & 1
\end{bmatrix}}_{\text{Direzione Verticale}}
$$

La somma dell'operatore per le linee verticali e quello per le linee orizzontali
ci da il gradiente.

Un altro operatore che implementa il gradiente è l'operatore di *Prewitt*,
essenzialmente questo operatore dà meno importanza al pixel centrare rispetto
all'operatore di *Sobel*. Gli operatori di Prewitt per le linee orizzontali e
verticali sono implementati nel modo seguente 
$$
\underbrace{\begin{bmatrix}
-1 & -1 & -1\\
0 & 0 & 0\\
1 & 1 & 1
\end{bmatrix}}_{\text{Direzione Orizzontale}}
\quad\quad
\underbrace{\begin{bmatrix}
-1 & 0 & 1\\
-1 & 0 & 1\\
-1 & 0 & 1
\end{bmatrix}}_{\text{Direzione Verticale}}
$$

Anche in questo caso, per ottenere il gradiente si fa la somma delle due
derivate direzionali (prima si calcola e poi si fa la somma).

Per riassumere, sia il Gradiente che il Laplaciano servono a evidenziare i
contorni (sono sensibili ad essi), ma è importante sottolineare varie differenze
tra i due. In generale, il Laplaciano:

* Mantiene i dettagli;
* È più sensibile al rumore (più evidente in zone *smooth*);
* Viene utilizzato per fare *sharpening*.

Mentre il Gradiente ha una risposta più marcata in aree in cui ci sono
transizioni di grigio molto significative, in cui i bordi sono particolarmente
più spessi, per cui è preferibile per fare *edge detection*.

## Contour Enhancement (Local Histogram Statistics)
Un altro metodo di enhancement a livello spaziale è il cosiddetto contour
enhancement. L'obiettivo ad esempio potrebbe essere quello di migliorare delle
aree scure dell'immagine lasciando invariate quelle più chiare. L'idea alla base
è quella di utilizzare delle statistiche *locali*, definite in un'opportuna
finestra di dimensioni $N \times M$. Ad esempio, la media sarà
$$
\mu = \frac{1}{M \cdot N} \sum_{x=0}^{M-1} \sum_{x=0}^{M-1} f(x, y)
$$
ed eventualmente la varianza locale
$$
\sigma^2 = \frac{1}{M \cdot N} \sum_{x=0}^{M-1} \sum_{x=0}^{M-1} [f(x, y) -
\mu]^2
$$
A questo punto, il processing andrà a considerare pixel per pixel, considerando
per ognuno di esso queste statistiche. Il processing verrà effettuato solo se i
seguenti vincoli sono soddisfatti:

* Se $\mu_s \leq k_0 \cdot \mu_G$ (dove $\mu_s$ è il valore medio *locale*,
  $\mu_G$ la media globale e $0 \leq k_0 \leq 1$ un opportuno valore fissato),
  in modo tale da considerare solamente i valori al di sotto di una determinata
  soglia di threshold, ignorando quelli al di sopra della stessa;
* Se $\sigma_s \leq k_2 \cdot \sigma_G$, in modo da considerare solo i pixel che
  non sono già in una zona ad altro constrasto;
* Se $k_1 \cdot \sigma_G \leq \sigma_s$ (ovviamente $k_1 < k_2$) in modo tale da
  evitare di aumentare il contrasto in zone uniformi.

Mettendo insieme tutti questi vincoli, si ottiene la seguente formula
riassuntiva per l'aumento locale di contrasto:
$$
g(x, y) = 
\begin{cases}
E \cdot f(x, y) & \text{if}\; \mu_s \leq k_0 \mu_G \text{ and } k_1 \sigma_G \leq
\sigma_S \leq k_2 \sigma_G\\
f(x, y)& \text{otherwise}
\end{cases}
$$

Questo tipo di processamento può essere applicato per fare *contour
enhancement*, in generale, zone che hanno una varianza locale molto alta si
traducono in un'alta variabilità delle intensità di pixel (all'interno della
finestra). Le zone che contengono bordi sono spesso tali da avere un'alta
varianza, siccome contengono spesso molti pixel di varie intensità. D'altra
parte, zone di *background* avranno un numero più ristretto di intensità,
risultando quindi in zone con varianza più piccola.
