# Image Restoration
L'image restoration ha come obiettivo il *miglioramento* dell'immagine, ma a
differenza dell'*enhancement* (che è un processo soggettivo), si tratta invece
di un processo **oggettivo**. Il *modus operandi* consiste nel modellare il
*difetto* dell'immagine, per poi poterlo riparare. Si utilizzano poi delle
funzioni per misurare la *performance* della tecnica di restoration utilizzata,
in modo da raggiungere un risultato ottimale. Così come l'enhancement, possiamo
effetture la restoration sia nel dominio delle frequenze (es. *blur removal*),
che nel dominio spaziale (es. *additive noise reduction*).

> Tipicamente quando ci sono problemi nell'immagine che hanno una località
> spaziale (che sono più prominenti in alcune zone specifiche dell'immagine) si
> preferisce un processing a livello spaziale. D'altra parte, se il problema è
> distribuito su tutta l'immagine, è preferibile un processing in frequenza
> (soprattutto se si tratta di un pattern periodico).

Ci sono due grandi problemi legati alla restoration:

* *Denoising*: in cui vengono utilizzati modelli che approssimano il rumore
  dell'immagine e un successivo filtering dello stesso. Di solito viene
  approcciato nel dominio spaziale;
* *Inversion*: consistono nel modellare la funzione degradante per ottenerne
  l'inversa, in modo tale che da invertire il processo degradante. Di solito
  viene approcciato nel dominio delle frequenze.
 
Questi due problemi non sono isolati tra loro ma bensì sono legati in diversi
modi che saranno chiariti più avanti nel capitolo.

Prima di iniziare il capitolo, è necesario però introdurre il modello matematico
che descrive il processo di degrado e restauro a cui si farà riferimento per il
resto del capitolo. Si suppone di aver un'immagine ideale $f(x, y)$ a cui viene
applicata una funzione di degrado $H$, utilizzata per modellare difetti di
*acquisizione* (es. movimento del sensore, blurring atmosferico), e
successivamente ne viene aggiunto un certo rumore, modellato dalla funzione
$\eta(x, y)$ (es. rumore del sensore). L'immagine degradata da queste due
operazioni è $g(x, y)$. A questo punto, si applicano i filtri di restoration che
sfrutteranno delle conoscenze specifiche su $H$ e $\eta$, e sarà tanto più
efficace tanto più precise le approssimazioni delle funzioni degradanti. Il
processo di restoration produrrà un'immagine $\hat{f}(x, y)$ che sarà una stima
dell'immagine originle $f(x, y)$ (*ground-truth*).

## Rumore statistico additivo
Ci concentreremo su una classe specifica di problemi, caratterizzata di funzioni
di degrado *lineari* e *position-invariant* e rumore *additivo*. La funzione di
degrato è deterministica, poiché spesso viene modellata sulla base di
osservazioni o specifiche del sensore di acquisizione. Il rumore additivo,
invece, è determinato dall'ambiente per cui è un qualcosa di totalmente casuale
ed è quindi un modello statistico.

Dal punto di vista spaziale, possiamo rappresentare l'intero processo di degrado
come:
$$
g(u, v) = h(u, v) \star f(u, v) + \eta(u, v)
$$
Sfruttando il teorema di Convoluzione possiamo ottenere la rappresentazione
equivalente nel dominio di Fourier:
$$
G(u, v) = H(u, v)F(u, v) + N(u, v)
$$

Il rumore $\eta$ è affetto da diversi fattori, che possono essere anche
introdotti dagli strumenti di cattura. Ogni sensore, infatti, ha una propria
*cifra di rumore*, tant'è che i produttori di sensori provvedono a fornire una
cosiddetta curva che indica il *signal-to-noise* ratio in funzione dell'apertura
ISO. Per ridurre questo rumore, infatti, spesso si vanno a fare delle medie tra
diverse misurazioni per avere delle stime più precise. Anche il rumore si può
analizzare in frequenza. Degno di nota è il cosiddetto *white-noise*, un rumore
uniforme ovunque che viene introdotto impostando a 0 i pixel in modo casuale. La
componente spettrale di questo rumore sarà una funzione costante, cioè contiene
tutte le frequenze di altezza $sigma^2$. In generale, inoltre, il rumore che
considereremo è invariante per posizione.

Il rumore $\eta(x, y)$ viene costruito a partire da una densità di probabilità
$f_{\eta}(z)$ che indica la probabilià che nella matrice $\eta$ ci sia un certo
valore di intensità.
Possiamo modellare questa distribuzione con diverse distribuzioni note, ad
esempio:

* **Gaussiana**: $p(z) = \frac{1}{\sqrt{2 \pi \sigma}} e^{-(z - \bar{z})^2/
  2\sigma^2}$, centrata nel valor medio $\bar{z}$ e avrà tanto più rumore tanto
  più $\sigma$ sarà grande;
* **Uniforme**;
* **Rayleigh**;
* **Esponenziale**;
* **Impulso**.

Per capire sperimentalmente se nell'immagine c'è del rumore è confrontare la
stima della proabilità dell'intensità dei pixel. L'istogramma è proprio una
distribuzione dei livelli di intensità dell'immagine e può fornire
un'approssimazione iniziale della distribuzione ricercata. Ovviamente questa è
un'approssimazione molto grezza poichè modella le varie intensità in modo
indipendente, nonostante queste siano dipendenti dalla loro posizione spaziale
nell'immagine [^fnote2]. Ottenere un modello statistico che tenga conto anche
della correlazione tra i pixel è molto difficile, ed esula dagli scopi di questo
corso.

[^fnote2]: Per convincersene, è sufficiente prendere l'istogramma di un'immagine
  e utilizzarlo per generare una nuova immagine, campionando semplicemente i
  valori dei pixel dall'istogramma in modo proporzionale ad esso. Quello che si
  otterrebbe non è un'immagine simile a quella ottenuta ma semplice rumore,
  proprio perché la distribuzione non tiene conto delle correlazioni spaziali
  tra pixel.

Abbandonando quindi l'idea di modellare statisticamente l'intera immagine, ci si
può focalizzare invece sulla modellazione statistica del rumore. Esso è appunto
un processo indipendente dal punto di vista statistico, per cui l'istogramma
potrebbe rappresentare un'ottima approssimazione della distribuzione. 

Se si fa appunto l'assuzione che il rumore sia uniforme e spazio invariante,
focalizzandosi su determinate porzioni dell'immagine e calcolandone
l'istogramma, saranno evidenti la dispersione rispetto al valore medio (il
valore reale dell'immagine senza rumore) introdotta dal rumore. Difatti, se ci
si focalizza su zone sufficientemente uniformi dell'immagine, l'istogramma
assumerebbe proprio la forma di una distribuzione simile a quella del rumore
stesso, centrata sul valore della porzione dell'immagine (il valore di pixel che
è uguale per tutti i pixel della porzione siccome è uniforme).

Se supponiamo di non conoscere la distribuzione del rumore, l'idea è quindi
quella di prendere più porzioni uniformi dell'immagine e calcolarne
l'istogramma. Se la distribuzione è simile per tutti allora si può dedurre che
il rumore è spazio invariante, additivo e bianco (scorrelato da punto a punto).
Quindi la forma dell'istogramma rappresenta un modo di capire qual'è la densità
di probabilità del rumore e tramite un processo di fitting è anche possibile
andare a stimare quali siano i parametri di questa distribuzione.

Per rimuovere il rumore whitenoise si va a fare una media delle misurazioni in
modo da abbassare la varianza $\sigma^2$. Tale operazione corrisponde di fatto a
fare un filtro passa-basso nel dominio della frequenza. Se si immagina appunto
il rumore presente nell'immagine, nel dominio di Fuorier comparirebbe come una
funzione costante alta $\sigma^2$. L'applicazione di un filtro passa basso
permette di rimuvere tutto quel rumore dopo la soglia del filtro, che
tipicamente non contiene troppe informazioni riguardo l'immagine (frequenze
alte), siccome tipicamente le immagini hanno un'alta concetrazione dell'energia
nelle frequenze basse.

Un'altra tipologia di rumore è il rumore periodico, che a questo punto comparirà
nella rappresentazione spaziale come una delta centrata nella frequenza dello
stesso. Per rimuovere questo rumore si può utilizzare un Notch filter, visto in
precedenza.

Possiamo concludere che il rumore possiamo processarlo sia in frequenza che
nello spazio. Se il rumore è periodico si preferisce processarlo in frequenza,
se invece si tratta di whitenoise, allora si preferisce il processing nello
spazio.

### Filtri lineari
Come già accennato, i filtri spaziali utilizzati sono principalmente delle medie
(dove $S_{xy}$ è l'intorno del pixel $(x, y)$ di dimensioni $m \times n$):

* **Aritmetica**: in cui il rumore è ridotto dal blurring;
  $$
  \frac{1}{nm}\sum_{(s, t) \in S_{xy}} g(s, t)
  $$
* **Geometrica**: i dettagli sono maggiormente preservati;
  $$
  \hat{f}(x, y) = \left[ \prod_{(s, t) \in S_{xy}} g(s, t) \right
  ]^{\frac{1}{nm}}
  $$ 
* **Armonica**: buoni per rumori *"salt"* e Gaussiano;
  $$
  \hat{f}(x, y) =\frac{nm}{\sum_{(s, t) \in S_{xy}} 1/g(s, t)}
  $$
* **Contrarmonico**: generalizzazione delle medie armoniche e aritmetiche.
  $$
  \hat{f}(x, y) = \frac{\sum_{(s, t) \in S_{xy}} g(s,
  t)^{Q+1}}{\sum_{(s, t) \in S_{xy}} g(s, t)^{Q}}
  $$
    * Con $Q>0$ buono per la rimozione del rumore *"pepper"*;
    * Con $Q<0$ buono per la rimozione del rumore *"salt"*;
    * Con $Q=0$ corrisponde alla media *aritmetica*;
    * Con $Q=-1$ corrisponde alla media *armonica*.

È utile osservare che applicare questi smoothing non elimina completamente il
rumore. Inoltre, anche la qualità dell'immagine viene intaccata un minimo.
Bisogna quindi trovare il compromesso tra i due.

### Filtri non lineari
Possiamo usare anche filtri non lineari per rimuovere il rumore, come diversi
filtri basati su statistiche ordinate:

* **Mediana**: rimuove rumore a impulso;
  $$
  \hat{f}(x, y) = \text{median}_{(s,t) \in S_{xy}} g(s, t)
  $$
* **Massimo**: riduce rumore pepe;
  $$
  \hat{f}(x, y) = \max_{(s,t) \in S_{xy}} g(s, t)
  $$
* **Minimo**: riduce rumore sale;
  $$
  \hat{f}(x, y) = \min_{(s,t) \in S_{xy}} g(s, t)
  $$
* **Punto medio**: 
  $$
  \hat{f}(x, y) = \left [ \max_{(s,t) \in S_{xy}} g(s, t) \min_{(s,t) \in
  S_{xy}} g(s, t) \right ]/2
  $$
* **Alpha Trimmed**: rimuove prima del calcolo della media, sia i $d/2$ valori
  più piccoli che più grandi. Lavora bene con rumore sale-pepe e Gaussiano
  mischiati insieme.
  $$
  \hat{f}(x, y) = \frac{1}{mn-d} \sum_{(s,t) \in S_{xy}} g'(s, t)
  $$

### Filtri adattivi
Sono filtri che adattano il loro comportamento al contenuto di una finestra
locale $m \times n$. Questi filtri vengono applicati solamente a zone in cui non
c'è molto rumore per evitare di incentivarlo. Una possibilità è quella di
raccogliere delle statistiche locali di ogni finestra $S_{xy}$ che si va a
processare. Un esempio visto riguarda appunto il calcolo della *media* (che
rappresenta la *luminosità* media) e la *varianza* (che rappresenta il
*contrasto*) locali.
L'idea è che in punti in cui la varianza è molto alta, è molto probabile che ci
si trovi in prossimità di un bordo, per cui rappresenta un dettaglio che
auspicabilmente non si deve processare per evitare di danneggiarlo.
D'altra parte, se la varianza è bassa o è molto simile ad una varianza
sperimentale (ad esempio data dal difetto del sensore di acquisizione), allora
si è in prossimità di rumore, per cui si va a processare con un filtro.
Un filtro adattivo di questo tipo è il seguente:
$$
\hat{f}(x, y) = g(x, y) - \frac{\sigma_{\eta}^2}{\sigma_L^2}[g(x, y) - m_L]
$$
in cui:

* $\sigma^2_{\eta}$ è la varianza del rumore del sensore, oppure di un rumore
  ottenuto sperimentalmente a partire da un dataset di immagini;
* $\sigma^2_L$ è la varianza locale;
* $m_L$ è la media locale.

Dalla formula, è evidente che il rapporto tra $\sigma^2_{\eta}$ e $\sigma_L^2$
sia fondamentale, poiché determina il comportamento del filtro. Nello specifico,
possiamo distinguere i diversi casi:

* $\sigma^2_{\eta}= 0$ allora non c'è rumore, per cui non si processa e si
  ritorna $g(x, y)$;
* Se $\sigma^2_{L} = \sigma^2_{\eta}$ allora $\hat{f}(x, y) =
  \text{mean}_{S_{xy}} g(x,y)$. Si è in prossimità di un punto che ha le stesse
  proprietà globali dell'immagine, per cui vogliamo processarla;
* Se $\sigma^2_{L} >> \sigma^2_{\eta}$ allora $\hat{f}(x, y) \approx g(x, y)$.
  Si è in prossimità di un punto in cui ci sono bordi o dettagli importanti che
  non devono essere processati principalmente per due motivi: si rovinerebbero i
  dettagli e comunque il rumore sarebbe meno visibile poiché i dettagli lo
  *"coprirebbero"*.

Un'altra possibilità per creare filtri adattivi è quella di variare i parametri
dei filtri in base all'immagine, al posto di decidere se applicare o meno il
filtering in base ad essa.
Un tipo di questo filtro è l'*adaptive median filter*, in cui viene cambiato
l'unico parametro del filtro mediano: le dimensioni del filtro stesso. Infatti,
l'idea alla base di questo filtro è quella di *ridurre* le dimensioni in
prossimità di punti in cui la varianza è alta e di *accrescerle* in punti in cui
è bassa. Vale la pena introdurre le notazioni che verranno utilizzate
nell'algoritmo in seguito:

* $S_{xy}$ è l'area centrata nel punto $z_{xy}$
* $z_{min}$ è l'intensità minima nell'area considerata;
* $z_{max}$ è l'intensità massima nell'area considerata;
* $z_{med}$ è l'intensità mediana nell'area considerata;
* $z_{xy}$ è l'intensità del pixel nella posizione $(x, y)$;
* $S_{max}$ è la grandezza massima dell'area consentita.

\begin{algorithm}
\caption{Adaptive Median Filter}
\begin{algorithmic}[1]
\State $A1 \gets z_{med} - z_{min}$
\State $A2 \gets z_{med} - z_{max}$
\If{A1 > 0 and A2 < 0} \Comment{Apply the median filter}
    \State $B1 \gets z_{xy} - z_{min}$
    \State $B2 \gets z_{xy} - z_{max}$
    \If{B1 > 0 and B2 < 0}
        \State output $z_{xy}$
    \Else
        \State output $z_{med}$
    \EndIf
\Else
    \State increase $S_{xy}$
\EndIf 
\If{$S_{xy} \leq S_{max}$}
    \State goto start (recomputing min, max and med)
\Else
    \State output $z_{med}$
\EndIf
\end{algorithmic}
\end{algorithm}

### Rumore additivo periodico
Come già accennato in precedenza, il rumore periodico viene invece processato
nello spazio delle frequenza. L'idea principale è quella di applicare dei filtri
bandreject o Notch per rimuovere le stesse dall'immagine.

Come visto, la rimozione di queste frequenze viene spesso fatta per mezzo di
filtri che hanno un profilo Gaussiano. Questo però ci crea dei problemi siccome
un filtro moltro stretto nel dominio delle frequenze (con $\sigma^2$ piccolo)
corrisponderà ad una Gaussiana molto larga nel dominio dello spazio (con
$1/\sigma^2$).
Per risolvere il problema l'idea è quella di isolare le componenti da rimuovere,
moltiplicarle per un determinato peso e sottrarle dall'immagine rumorosa. Questo
approccio è implementato dall'*optimal Notch filtering*.
Anziché rimuovere direttamente le componenti di frequenza che causano rumore,
l'approccio consiste nell'estrarre le componenti tramite un filtro Notch pass,
per poi ottenere una stima del rumore facendone l'antitrasformata:
$$
\eta(x, y) = \mathscr{F}^{-1}[H_{np}(u, v)G(u, v)]
$$
A questo punto, si sottrae il rumore ottenuto, ma siccome è solo una stima, si
vouole fare attenzione a sottrarlo solo nei punti in cui si è più sicuri che si
tratti di rumore, per cui lo si pesa per un'opportuna funzione che ha il compito
di minimizzare gli effetti dei componenti che non sono presenti in $\eta(x, y)$.
$$
\hat{f}(x, y) = g(x, y) - w(x, y)\eta(x, y)
$$
dove $w(x, y)$ è la funzione di *"pesatura"*. Tale funzione in un certo senso
rende adattivo il filtro, poiché decide quando e di quanto attivare il filtro
Notch. Il criterio per decidere se applicare o meno il filtro è quello di
minimizzare la varianza all'interno dell'intorno locale $(2a +1) \times (2b +
1)$. L'idea è che le immagini in generale hanno una varianza locale bassa per
ogni $(x, y)$, per cui si tratta di un criterio prioristico (si sta di fatto
facendo una stima a priori sulla distribuzione delle immagini).
Per arrivare alla formulazione della funzione $w$ bisogna risolvere di fatto un
problema di ottimizzazione.
Siano:

* $S_{xy}$ l'intorno di dimensioni $(2a + 1) \times (2b + 1)$, centrato in $(x,
  y)$;
* $\mu(x, y) = \frac{1}{(2a + 1)(2b + 1)} \sum_{(x, y) \in S_{xy}} \hat{f}(x,
  y)$ il valor medio locale;
* $\sigma^2(x, y) = \frac{1}{(2a + 1)(2b + 1)} \sum_{(x, y) \in S_{xy}}
  \hat{f}(x, y) - \mu(x, y)$ la varianza locale;

Si sostituiscono inizialmente all'interno della definizione di $\sigma^2$ tutte
le occorrenze di $\hat{f}$ con la definizione data poch'anzi. A questo punto,
per poter ottimizzare questa formulazione si va a calcolare la derivata di $w$,
per poi scegliere i punti di minimo in essa. Per "portar fuori" $w$ dalla
sommatoria, si fa l'assunzione che sia costante nella finestra $S_{xy}$.
Calcolando quindi la derivata $\frac{\partial \sigma^2}{\partial w}$ e ponendola
$=0$ possiamo trovarne il minimo. I calcoli sono ommessi, si riporta solo
l'ottimo:
$$
w(x, y) = \frac{\overline{g(x, y)\eta(x,y)} - \overline{g}(x,
y)\overline{\eta}(x,y)}{\overline{\eta^2}(x, y) - \overline{\eta}^2(x, y)}
$$
dove $\bar{\cdot}$ è la **media** calcolata rispetto alla finestra $S_{xy}$.

## Degradazione
In riferimento allo schema proposto ad inzio capitolo, ci si concentra ora su
metodi per la rimozione della degradazione della funzione $H$. In questo caso si
ipotizza quindi che non ci sia nessun tipo di rumore per non complicare il
problema.
$$
g(x, y) = H(f(x, y)) + \cancel{\eta(x, y)}
$$
Ipotizziamo inoltre che $H$ sia un filtro lineare (rispetta il principio di
sovrapposizione degli effetti) e posizione invariante (l'output non cambia in
base alla posizione), per cui ci si concentrerà su un sottoinsieme delle
possibili funzioni di degradazione. Questa restrizione permette di riscrivere
l'espressione di $g(x, y)$, siccome è dimostrabile che applicare un filtro
lineare e posizione invariante corrisponde a fare la convoluzione tra $H$ e $f$.
Alla luce di questo fatto, è possibile quindi riscrivere la relazione
precedente:
$$
g(x, y) = f(x, y) \star h(x, y)
$$
L'approccio principale è quindi quello di cercare di trovare un modello per la
funzione di degradazione, costruirne il filtro inverso e applicarlo
all'immagine. La restoration è anche detta per questo motivo *deconvoluzione*.
Ci sono true approcci principali per determinare la funzione di degradazione:
per *osservazione*, per *sperimentazione* e per *modellazione*.
Nel primo caso, l'idea è quella di prende l'immagine corrotta e prenderne
un'area specifica $s$ che può essere facilmente restaurata (manualmente come
desideriamo che sia). Considerando successivamente sia la trasformata di questa
area $G_s(u, v)$, che la trasformata di quella non restaurata $\hat{F}_s(u, v)$,
la stima della funzione di degradazione sarà data dal rapporto delle stesse
(siccome $G(u, v) = H(u, v) F(u, v)$):
$$
H_s(u, v) = \frac{G_s(u, v)}{\hat{F}_s(u, v)}
$$
Se la degradazione è realmente lineare e posizione invariante allora la
restaurazione è modellata correttamente e l'applicazione del filtro a tutta
l'immagine restaurerà tutta l'immagine, sfruttando così l'invarianza
posizionale.

La seconda idea è quella di acquisire un'immagine (template) con un singolo
punto illuminato ad alta intensità. Ci si aspetterebbe un singolo pixel acceso,
però si ottiene tipicamente un'ombra (dovuta al difetto di acquisizione $H$). A
questo punto, la TF dell'immagine template è semplicemente $A$ (che è l'ampiezza
del punto acceso), per cui, sfruttando la relazione come fatto precedentemente
si ottiene:
$$
H(u, v) = \frac{G(u, v)}{A}
$$

La terza idea consiste nel andare ad ottenere un modello del fenomeno fisico di
riferimento per mezzo di un modello matematico. Un approccio di questo tipo è la
modellazione del movimento delle immagini. Avendo la traiettoria dello
spostamento nel tempo in ogni direzione descritto dalle funzioni $x_0(t)$ e
$y_0(t)$ (ottenendole ad esempio tramite accelerometro), è possibile modellare
l'acquisizione come:
$$
g(x, y) = \int^T_0 f(x - x_0(t), y - y_0(t)) dt
$$
cioè non è nient'altro che *media* delle acquisizioni che fa il sensore durante
il tempo di acquisizione $T$. Tramite questa modellazione si ottiene che la
funzione di degradazione non è nient'altro che un filtro passa basso, cioè un
filtro che fa blurring in funzione della *velocità*, il che coincide anche con
l'intuizione.

> (*per la trattazione matematica dettagliata si rimanda alle slides*)

Dal punto di vista pratico, un filtro che modella il motion blurring è un filtro
composto da tutti 0 e 1 verso la direzione del blurring. Ad esempio un filtro $N
\times M$ che modella il blurring orizzontale potrebbe essere il seguente
$$
h(x, y) =
\begin{cases}
1 & \text{if } (x, y) \in [(0, N-1), (0, M-1)]\\
0 & \text{otherwise}
\end{cases}
$$
La grandezza $N \times M$ è determinata inoltre dalla velocità direzionale
relativa.

### Filtro Inverso
Una volta ottenuta una modellazione sufficientemente buona del filtro di
degradazione, è possibile utilizzare successivamente la relazione usata in
precedenza per ottenere l'immagine restaurata:
$$
\hat{F}(u, v) = \frac{G(u, v)}{H(u, v)}
$$
Questa soluzione però evidenzia due problemi principali, introdotti dalla
divisione per il filtro $H$. Il primo è la divisione per 0: il filtro potrebbe
avere valori vicino a $0$ per cui renderebbe la procedura numericamente
instabile. Il secondo riguarda invece il rumore, esso difatti è stato ommesso,
ma è pur sempre presente in un modello realistico di restoration. La divisione
del rumore per valori piccoli di $H$, aumenterebbe significativamente lo stesso
di conseguenza, ne consegue che il filtro inverso **può** aumentare il rumore.
La soluzione è evitare di applicare il filtro inverso nelle frequenze in cui
diventa instabile.

#### Filtro di Wiener
Un modo che permette di mitigare il problema del rumore discusso in precedenza è
mediante l'utilizzo di un filtro di Weiner. Il filtro di Wiener è un filtro
ottimo rispetto al MSE (rispetto all'immagine ground truth) per invertire i
sistemi lineari. L'obiettivo per trovare un filtro ottimo è quello di progettare
un filtro $h_i$ tale per cui
$$
h_i = \arg \min_{h_i} E \left \{ (f - \hat{f})^2 \right \}
$$
risolvere questo problema di ottimizzazione è possibile e il risultato è il
seguente:
$$
H_i(u, v) = \frac{H^*(u, v)}{\left | H(u, v) \right |^2 + S_{\nu}(u, v)/S_f(u,v)}
$$
dove:

* $H^*(u, v)$ è il *complesso coniugato* di $H$;
* $S_{\nu}(u, v) = \left | N(u, v)^2 \right |$ spettro di potenza del rumore;
* $S_f(u, v) = \left | F(u, v)^2 \right |$ spettro di potenza dell'immagine.

Per conoscere lo spettro di potenza dell'immagine originale bisogna conoscere la
distribuzione dell'energia dello spettro dell'immagine. Siccome non si conosce a
priori, è sufficiente una stima dello stesso. Lo stesso vale per il rumore.

Il rapporto tra i due spettri di potenza nella formula equivale al rapporto
$\frac{1}{SNR} = NSR$. Più è grande, più rumore e meno segnale sarà presente in
quella banda, per cui il filtro inverso dovrà essere applicato in minore
intensità per evitare di aumentare il rumore.

Se analizziamo la formula, nel caso migliore (in cui $SNR$) tende a $\infty$, si
ha che l'intero rapporto vale 0, per cui il filtro sarà semplicemente applicato
così com'è. D'altra parte, nel caso in cui $SNR$ tenda a $0$, allora il rapporto
tenderà a $\infty$, per cui il valore dell'intero filtro sarà molto basso,
cancellando di fatto le frequenze dominate dal rumore (siccome è dominato
dall'alto valore del denominatore).

Quando non si hanno indizi sullo spettro del rumore, si assume che ci si trovi
in presenza di un rumore bianco, per cui si pone lo spettro a $\sigma^2$. Nel
caso invece dell'immagine, come già accennato si può sia utilizzare un modello,
oppure se non si conosce si utilizza semplicemente una costante.

## Metriche di qualità
Ottenere delle metriche di qualità per la restoration è un problema ancora
parzialmente irrisolto. Supponendo di avere sia l'immagine originale $f(x, y)$
(ground thuth) che l'immagine restaurata $\hat{f}(x, y)$, un modo potrebbe
essere quello di calcolare l'errore tra le due
$$
e(x, y) = f(x, y) - \hat{f}(x, y)
$$
che però non è una metrica. Per avere una metrica per valutare l'errore si
utilizza il *Mean Square Error* (MSE):
$$
MSE = \frac{1}{W \cdot H} \sum_{(x, y) \in W\times H} e(x, y)^2
$$
Questa metrica non tiene conto della qualità soggettiva, ma solo di una qualità
in senso generale. In realtà nelle immagini si utilizza di solito il rapporto
segnale/rumore:
$$
SNR = \frac{S}{MSE}
$$
dove $S = \frac{1}{W \cdot H} \sum_{(x, y)} f(x, y)^2$. Un'altro rapporto è il
rapporto segnale di picco/rumore:
$$
PSNR = \frac{P^2}{MSE}
$$
dove $P = max_{(x, y)} f(x, y)$. Di solito valori $PSNR \approx 30dB$ sono
ritenuti accettabili.

Deve essere chiaro che comunque queste metriche non sempre hanno una
correlazione vera e propria con le qualità soggettive per un umano.
