# Visione Artificiale con Reti Neurali
In questo capitolo ci si concentrerà sulla visione artificiale, interamente per
mezzo di approcci machine learning, nello specifico mediante l'impiego di reti
neurali profonde. Inoltre, ci si concentrerà su problemi di apprendimento
supervisionato come la **classificazione** o la **regressione**.

Un esempio di problema di apprendimento supervisionato nel campo della computer
vision potrebbe essere quello di classificare con una determinata classe un
oggetto presente in un immagine. Ad esempio, se un immagine contiene un gatto
oppure un cane.
Il dataset di input per un problema di apprendimento supervisionato è
rappresentato da una matrice $\mathbf{X}$ con tante righe quante istanze
presenti nel dataset e tante colonne quante *features* del problema. Inoltre si
indicano le classi etichettate delle varie istanze con il vettore $\mathbf{t}$.
Si supporrà inoltre che i dati siano normalizzati, cioè tali per cui abbiano
media $\mu = 0$ e varianza $\sigma = 1$. Formalmente, per ogni vettore colonna
$\mathbf{x}_i$, si applica la seguente formula:
$$
\mathbf{x}_i = \frac{\mathbf{x}_i - \mu(\mathbf{x}_i)}{\sigma(\mathbf{x}_i)}
$$

## Percettrone
La rete più semplice che si può immaginare è quella composta da un singolo
neurone. Esso ha una serie di input $x_i$, una serie di parametri (*pesi*)
$w_i$, un *bias* additivo e una funzione di attivazione *non-lineare* e
differenziabile $g$.
Si prende ispirazione dal funzionamento biologico di un neurone del sistema
nervoso, che consiste nel rappresentare un neurone come una funzione che somma i
semplicemente vari segnali in input (i neuroni ai quali è connesso) pesandoli
per la forza di connessione che ha il neurone con essi. Questa somma poi viene
fatta passare per una funzione di attivazione che indica la risposta del
neurone, siccome nei neuroni biologici esiste una soglia di attivazione. 

Mettendo insieme questi componenti, otteniamo che l'output del neurone è
rappresentabile come
$$
y = g \left (b + \sum_{i} \mathbf{w}_i \mathbf{x}_i \right )
$$

L'apprendimento di questa semplice rete consiste inizialmente nell'impostare i
valori dei pesi $\mathbb{w}$ in modo completamente casuale. Successivamente, si
calcola per ogni istanza di input $\mathbf{x}_i \in X$ (*vettore riga*) l'output
$y_i$ corrispondente del neurone. A questo punto si calcola un errore $E$
differenziabile $t_i$, come il Mean Square Error $E_i(y_i, t_i) = (y_i -
t_i)^2$. Siccome sia l'errore che l'output del neurone sono differenziabili, è
possibile calcolare il *gradiente* dell'errore $\frac{\partial E}{\partial
\mathbf{w}}$. Il calcolo del gradiente può essere fatto semplicemente per mezzo
della regola di derivazione a catena. Difatti, espandendo la definzione di $E$
si ottiene:
$$
E(y_i, t_i) = \left [ g \left ( \underbrace{b + \sum_{j} \mathbf{w}_j \mathbf{x}_ij }_{z_i} \right ) - t_i
\right ]
$$
Quindi, per la formula di derivazione a catena il gradiente corrispondente sarà:
$$
\frac{\partial E}{\partial \mathbf{w}_i} = 
\frac{\partial E}{\partial y_i}
\frac{\partial y_i}{\partial z_i}
\frac{\partial z_i}{\partial \mathbf{w}_i}
$$
dove $z_i$ è indicato nella formula precedente.

Una volta ottenuto il gradiente, per ogni $\mathbf{w}_i$ si applica una regola
di update secondo il principio di *discesa del gradiente*. Essenzialmente questo
principio permette di minimizzare l'errore modificando i pesi in modo da
muoversi verso la direzione opposta all'aumento più grande dell'errore (e
quindi, di conseguenza, verso la massima diminuizione). Dal punto di vista
formale, la regola di update è la seguente:
$$
\mathbf{w}'_i = \mathbf{w}_i - \eta \frac{\partial E}{\partial \mathbf{w}_i}
$$
in cui $\mathbf{w}'_i$ è il nuovo valore del peso $i$-esimo e $\eta$ è un
iperparametro dell'ottimizzatore che indica il *learning rate* (o *step-size*).
Questo passaggio viene re-iterato per tutte le istanze $i$ del dataset,
completando di fatto un *epoca*.

## Fully Connected Layer
Il percettrone, pur rappresentando un fondamentale punto di partenza nel campo
delle reti neurali, presenta alcune limitazioni. Come visto, difatti,
rappresenta solo un output scalare $y$. È possibile concatenare diversi
percettroni per ottenere un FCL (*Fully Connected Layer*). In caso se ne
concatenino $m$, si otterranno $m$ vettori $\mathbf{w}$ di parametri, per cui
vengono rappresentati tutti in una matrice $\mathbf{W}$ con dimensioni $m \times
n$ dove $m$ è il numero di outputs (*percettroni*) e $(n+1)$ il numero di inputs
(dove il $+1$ può essere omesso e rappresenta il *bias*).

## Multilayer Perceptron
Un layer completamente connesso permette di "mappare" le features in input su un
nuovo spazio vettoriale. Concatenare vari layer uno dopo l'altro permette di
"processare" l'informazione in input, mappandola in diversi spazi vettoriali.
Ciò che la rete fa è essenzialmente apprendere delle features latenti del
dataset che possono essere utilizzate per discriminare dei problemi anche non
linearmente separabili (poiché esse lo sono). In un problema di
multi-classificazione, l'ultimo layer sarà composto da tanti neuroni quanti sono
le classi che fanno parte del problema di classificazione, poiché i vari neuroni
fungeranno da classificatori per la stessa classe. Inoltre, in questo tipo di
problemi si applica una funzione *softmax* sul vettore dei neuroni di output
$\mathbf{z}$, per cui l'output della rete sarà
$$
\mathbf{y} = \frac{e^{\mathbf{z}}}{\sum_{k=1} e^{z_k}}
$$
La probprietà della softmax è che crei una *distribuzione* sulle varie
componenti del vettore, che in questo caso sono le varie classi. Per questo è
possibile calcolare una metrica di errore che funziona meglio per problemi di
classificazione, cioè la *cross-entropy loss*:
$$
\mathscr{L}(\mathbf{y}, \mathbf{t}) = -\sum_j t_j log(y_j)
$$

### Overfitting
Più aumenta la complessità del modello (numero di layer e neuroni), più esso
sarà in grado di adattarsi al il dataset di train iniziale. Bisogna però fare
attenzione al tradeoff bias-variance. Un classificatore troppo coomplesso si
adatterà troppo ai dati, provocando *overfitting* (cioè un fenomeno in cui il
classificatore modella così bene i dati di input che non è in grado di
generalizzare). Tipicamente si introduce una regolarizzazione dei pesi in normal
$\mathscr{L}^2$ in modo tale da penalizzare le soluzioni con valori dei pesi
troppo grandi.

In un problema di machine learning, per capire se ci si trova di fronte ad un
modello overfitted, si divide il dataset originale in dataset di train e dataset
di test. Si apprende il modello sullo split del dataset di train e si testa su
quello di test. L'overfit si verifica quando l'errore di train è più alto
dell'errore di test.  

## Reti Feed Forward e Immagini
Fino ad ora si è supposto che le features di un problema di classificazione
fossero delle qualità intrinseche che potessero rappresentare le varie istanze,
come ad esempio il peso, l'altezza, la grandezza ecc.
Viene naturale pensare che le immagini stesse possano essere viste come un
insieme di features, cioè ogni valore di pixel rappresenterebbe di per se una
feature. In questo senso, un'immagine $N \times M$ può essere rappresentata in
un vettore lungo $N \cdot M$, con i valori delle features corrispondenti ai
valori di intensità dei pixel. Nel dataset MNIST, composto da immagini $28
\times 28$ di cifre scritte a mano ed etichettate con la classe corretta
($0-9$), le immagini sono per l'appunto espresse in vettori lunghi $28 \cdot 28
= 768$ elementi.

### LeNet300
LeNet300 fu un'architettura introdotta alla fine degli anni 80 per fare
riconoscimento di cifre scritte a mano e classificarle (*handwritten digit
recognition*). La rete era una rete completamente connessa (composta da soli
layer fully connected), la cui architettura era composta da:

* Input layer di dimensioni $768$ (il che è ragionevole poiché l'input sono
  immagini lunghe $768$);
* Layer hidden di dimensioni $300$;
* Layer hidden di dimensioni $100$;
* Layer di output di dimensioni $10$ (il numero di classi che coincide con il
  numero di cifre).

Nel paper originale venivano fatte diverse esplorazioni di parametri
dell'architettura, ma quella con performare migliori fu appunto quella
presentata precedentemente. 
Il problema di questo approccio è che questo tipo di rete non permette di
sfruttare la correlazione spaziale dei pixel che esiste in un'immagine
qualsiasi. Questo perché le immagini sono trattate semplicemente come dei
vettori, rimuovendo di fatto questa struttura bidimensionale e di conseguenza
l'informazione spaziale che ne consegue.

## Reti Neurali Convoluzionali
Le reti convoluzionali sono un'evoluzione di questo approccio, nate per
l'appunto per risolvere questa problematica e sfruttare la correlazione spaziale
dei pixel nelle immagini.

Gli oggetti nelle immagini sono caratterizzati da features spaziali ad alto
livello, come ad esempio *bordi*, *angoli*, ecc.. L'idea alla base delle reti
convoluzionali è quella di far si che la rete possa apprendere queste features
locali. La rete quindi utilizza dei *features detectors*, che data in input
un'immagine rappresentata per pixel, sottolineano le features significative.
Questa operazione viene implementata per mezzo di una convoluzione, in cui i
valori del filtro vengono appresi dalla rete durante il processo di
apprendimento. Il risultato dell'applicazione di questo filtro su un'immagine,
produce una *feature map*, cioè una rappresentazione dell'immagine nello spazio
delle feature che sono individuate dal filtro.

### Layer Convoluzionale
L'operazione di convoluzione è implementata da un Neurone Convoluzionale, che
non è nient'altro che un neurone in cui gli sono input vincolati spazialmente,
cioè sono connessi solamente ad un sottoinsieme dei neuroni a cui sono connessi
in input. Ovviamente, i valori del filtro (*pesi*) possono essere appresi
attraverso la backpropagation. Formalmente l'output del neurone $i, j$ viene
calcolato come:
$$
\mathbf{y}_{i, j} = \sum_{l = 0}^{L} \sum_{k = 0}^{K} \mathbf{w}_{l, k} \cdot
\mathbf{x}_{(S \cdot i) + l, (S \cdot j) + k}
$$
Questa formula implementa un kernel di dimensioni $L \times K$ e stride $S$. La
dimensione del kernel è un iperparametro che definisce l'estensione spaziale
della connettività dei neuroni rispetto al layer precedente, e viene chiamato
anche *receptive field* (campo recettivo). D'ora in poi si supporrà che la
dimensione del kernel sia sempre $F \times F$. Lo striding indiva invece di
quanto sono spaziati tra di loro i kernel nel layer di input. Siccome la feature
map risultante è più piccola in dimensioni del layer in input, si applica uno
*zero-padding* ($P = (F - 1)/2$), in modo tale che l'output preservi le
dimensioni del layer in input. Ovviamente è possibile applicare questa tipologia
di rete anche ad immagini RGB, semplicemente applicando un filtro per ogni
canale e poi si sommandoi risultati in modo da accorpare l'informazione delle
varie features apprese nei vari canali.

Si noti che il numero di parametri è più ristretto rispetto ad un layer fully
connected, siccome ogni neurone è connesso solo ad un sottoinsieme ristretto di
neuroni di input ($F \times F$). Più precisamente, il numero di parametri per un
layer convoluzionale è pari a:
$$
n_{out} \cdot (S + F^2) \cdot n_{ch}
$$
dove:

* $n_{out}$ numero di neuroni convoluzionali;
* $n_{ch}$ numero di canali ($=1$) per immagini *grayscale* e $=3$ per immagini
  RGB;
* $F$ dimensione del kernel ($F \times F$).

Questo minore numero di parametri permette sia di alleggerire molto la fase di
apprendimento che di migliorare la generalizzazione del modello risultante.

Si rappresenta quindi un layer convoluzionale con la notaziona $Conv(F, S, P)$,
in cui ovviamente $F$ è la dimensione del filtro, $S$ lo stride e $P$ lo
zero-padding.

### MaxPooling Layer
Siccome le features map estratte dai layer convoluzionali possono essere viste
come delle immagini *filtrate*, anche queste immagini filtrate possono essere
sotto-campionate proprio come le immagini. Questa operazione permette di ridurre
ulteriormente i parametri totali della rete, siccome permette di ridurre la
dimensione dell'output, mantenendo però una certa quantità di informazione. 

Il layer di max-pooling permette di campionare dei valori dall'immagine secondo
il criterio di *massimo*. L'idea è sempre quella di applicare un filtro
*non-lineare* che faccia passare solamente il massimo tra tutti gli elementi
della finestra definita dal filtro. Anche in questo caso il filtro sarà $F
\times F$ e può essere applicato con un determinato stride $S$. Il layer di
max-pooling si applica tipicamente dopo un layer di convoluzione.

### Optical Character Recognition 
È possibile usare le reti convoluzionali per fare OCR. Ad esempio, si può
aggiungere una classe "undefined" (o background) che non rappresenta nessun
carattere. La rete poi può essere applicata a tutte le posizioni dell'immagine
attraverso un approccio a *sliding-window*. Per fare questa operazione in modo
più efficiente, però, si può sfruttare il dualismo tra layer fully connected e
convoluzionale. Essenzialmente, se un layer convoluzionale ha le stesse
dimensioni del kernel del suo input, diventa equivalente ad un layer fully
connected. La trasformazione dei layer fully connected in convolutional, ha però
il vantaggio di poter applicare la rete in parallelo con un approccio sliding
window su immagini di grandezza arbitraria. L'output saranno non più una singola
classe ma 10 feature maps con la grandezza della finestra (sliding window), in
cui i valori di ogni feature map indicano il valore di classe per quel
particolare punto in cui può essere posizionata la finestra. Questa accortezza
permette di ridurre notevolmente la complessità computazionale richiesta,
anzichè applicare per ogni posizione della sliding window la rete.

> Le reti che hanno solo layer convoluzionali e non fully-connected sono
> ragionevolmente chiamate *fully-convolutional*.

Una volta ottenuti i caratteri si va a fare poi una fase di postprocessing che
tramite tecniche statistiche e probabilistiche ri-aggiusta il risultato. Ci sono
quindi due possibilità per apprendere una rete in un task di OCR:

1. Apprendere una rete convoluzionale con layer fully connected su singoli
   caratteri (quindi campioni piccoli), per poi fare un reshape dei layer fully
   connected in layer convoluzionali, mantenendone i pesi appresi (si prendono
   dai layer fully connected);
2. Apprendere una rete già *fully-convolutional* direttamente sulle immagini. In
   questo caso però le immagini devono essere tutte etichettate nelle varie
   posizioni.

### LeNet5
Questa rete fu una delle prime reti convoluzionali in cui si applicava il
pattern *convolve-and-pool*. Di seguito verrà utilizzata una versione
leggermente modificata di LeNet5. L'input sono immagini $32\times 32$, a cui si
applica un layer convoluzionale composto da 6 neuroni convoluzionali $Conv(5, 2,
1)$. In questo modo, vengono così generate in output $6$ differenti features
map, ognuna in grado di estrarre una particolare feature dall'input. Dopo questo
layer ne segue un layer di max-pooling su tutte le features map risultanti. Si
ri-appplicano poi un layer di convoluzione uguale al precedente, questa volta
però composto da $16$ neuroni, seguito da un layer di max-pooling ($2 \times
2$). Infine, seguono due layer fully connected, in modo da implementare la parte
discriminativa della rete. Riassumendo, l'architettura di LeNet5 è così
composta:

* Layer input (immagine padded $32 \times 32$);
* Layer di $6$ neuroni convoluzionali $Conv(5, 2, 1)$, Output: $6 \times 28
  \times 28$;
* Layer di max pool $2 \times 2$, Output: $6 \times 14 \times 14$;
* Layer di $16$ neuroni convoluzionali $Conv(5, 2, 1)$, Output: $16 \times 14
  \times 14$;
* Layer di max pool $2 \times 2$, Output: $16 \times 7 \times 7$;
* *(Layer di flatten)*;
* Layer fully connected $784 \times 100$;
* Layer fully connected $100 \times 10$ (seguito da un layer di *softmax*).

Nel caso invece si voglia considerare la versione fully-convolutional della
rete, basta rispettare la regola precedente, per cui bisogna sostituire i layer
fully connected con i seguenti layer (in ordine):

* Layer di $100$ neuroni convoluzionali $Conv(7, 1, 0)$ (cioè $7 \times 7$);
* Layer di $10$ neuroni convoluzionali $Conv(10, 1, 0)$ (cioè $1 \times 1$).

## Tecniche avanzate di apprendimento
Come visto in precedenza, l'apprendimento delle reti neurali viene fatto per
mezzo di un algoritmo di discesa del gradiente. Il *learning rate* è un
iperparametro dell'algoritmo di ottimizzazione molto importante, perché può fare
la differenza nell'ottenimento di un modello di buona qualità. Scegliere il
valore corretto però non è semplice, siccome valori di $\eta$ molto piccoli,
renderebbero lento l'apprendimento e porterebbero spesso ad una configurazione
associata ad un minimo locale. D'altra parte, un alto valore di $\eta$
renderebbe l'apprendimento troppo instabile (provoncando un effetto "bouncing",
o di "overshoot" dell'errore intorno al valore minimo).

Tipicamente il learning rate viene ridotto nel tempo in modo da dare
inizialmente la possibilità all'ottimizzatore di non finire subito in minimi
locali. Col crescere delle iterazioni, però, il learning rate descresce in modo
tale da focalizzare la convergenza sul minimo trovato.

Un modo per implementare questa strategia è il cosiddetto *esponential decay*,
in cui il learning rate decresce esponenzialmente col crescere delle epoche
$$
\eta = \eta_0 \cdot e^{-kt}
$$
Questa tipologia di aggiustamento del learning rate permette anche di evitare
l'overfitting.


