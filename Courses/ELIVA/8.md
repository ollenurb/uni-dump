# Visione Artificiale con Reti Neurali
In questo capitolo ci si concentrerà sulla visione artificiale, interamente per
mezzo di approcci machine learning, nello specifico mediante l'impiego di reti
neurali profonde. Inoltre, ci si concentrerà su problemi di apprendimento
supervisionato come la **classificazione** o la **regressione**.

Un esempio di problema di apprendimento supervisionato nel campo della computer
vision potrebbe essere quello di classificare con una determinata classe un
oggetto presente in un immagine. Ad esempio, se un immagine contiene un gatto
oppure un cane.
Il dataset di input per un problema di apprendimento supervisionato è
rappresentato da una matrice $\mathbf{X}$ con tante righe quante istanze
presenti nel dataset e tante colonne quante *features* del problema. Inoltre si
indicano le classi etichettate delle varie istanze con il vettore $\mathbf{t}$.
Si supporrà inoltre che i dati siano normalizzati, cioè tali per cui abbiano
media $\mu = 0$ e varianza $\sigma = 1$. Formalmente, per ogni vettore colonna
$\mathbf{x}_i$, si applica la seguente formula:
$$
\mathbf{x}_i = \frac{\mathbf{x}_i - \mu(\mathbf{x}_i)}{\sigma(\mathbf{x}_i)}
$$

## Percettrone
La rete più semplice che si può immaginare è quella composta da un singolo
neurone. Esso ha una serie di input $x_i$, una serie di parametri (*pesi*)
$w_i$, un *bias* additivo e una funzione di attivazione *non-lineare* e
differenziabile $g$.
Si prende ispirazione dal funzionamento biologico di un neurone del sistema
nervoso, che consiste nel rappresentare un neurone come una funzione che somma
semplicemente i vari segnali in input (i neuroni ai quali è connesso) pesandoli
per la forza di connessione che ha il neurone con essi. Questa somma poi viene
fatta passare per una funzione di attivazione che indica la risposta del
neurone, siccome nei neuroni biologici esiste una soglia di attivazione. 

Mettendo insieme questi componenti, otteniamo che l'output del neurone è
rappresentabile come
$$
y = g \left (b + \sum_{i} \mathbf{w}_i \mathbf{x}_i \right )
$$

L'apprendimento di questa semplice rete consiste inizialmente nell'impostare i
valori dei pesi $\mathbf{w}$ in modo completamente casuale. Successivamente, si
calcola per ogni istanza di input $\mathbf{x}_i \in X$ (*vettore riga*) l'output
$y_i$ corrispondente del neurone. A questo punto si calcola un errore $E$
differenziabile $t_i$, come il Mean Square Error $E_i(y_i, t_i) = (y_i -
t_i)^2$. Siccome sia l'errore che l'output del neurone sono differenziabili, è
possibile calcolare il *gradiente* dell'errore $\frac{\partial E}{\partial
\mathbf{w}}$. Il calcolo del gradiente può essere fatto semplicemente per mezzo
della regola di derivazione a catena. Difatti, espandendo la definzione di $E$
si ottiene:
$$
E(y_i, t_i) = \left [ g \left ( \underbrace{b + \sum_{j} \mathbf{w}_j \mathbf{x}_ij }_{z_i} \right ) - t_i
\right ]
$$
Quindi, per la formula di derivazione a catena, il gradiente corrispondente
sarà:
$$
\frac{\partial E}{\partial \mathbf{w}_i} = 
\frac{\partial E}{\partial y_i}
\frac{\partial y_i}{\partial z_i}
\frac{\partial z_i}{\partial \mathbf{w}_i}
$$
Una volta ottenuto il gradiente, per ogni $\mathbf{w}_i$ si applica una regola
di update secondo il principio di *discesa del gradiente*. Questo principio
permette di minimizzare l'errore modificando i pesi in modo tale da muovere al
configurazione del neurone verso la direzione opposta all'aumento più grande
dell'errore (e quindi, di conseguenza, verso la massima diminuizione). Dal punto
di vista formale, la regola di update è la seguente:
$$
\mathbf{w}'_i = \mathbf{w}_i - \eta \frac{\partial E}{\partial \mathbf{w}_i}
$$
in cui $\mathbf{w}'_i$ è il nuovo valore del peso $i$-esimo e $\eta$ è un
iperparametro dell'ottimizzatore che indica il *learning rate* (o *step-size*).
Questo passaggio viene re-iterato per tutte le istanze $i$ del dataset,
completando di fatto un *epoca*.

## Fully Connected Layer
Il percettrone, pur rappresentando un fondamentale punto di partenza nel campo
delle reti neurali, presenta alcune limitazioni. Come visto, difatti,
rappresenta solo un output scalare $y$. È possibile concatenare diversi
percettroni per ottenere un FCL (*Fully Connected Layer*). In caso se ne
concatenino $m$, si otterranno $m$ vettori $\mathbf{w}$ di parametri, per cui
vengono rappresentati tutti in una matrice $\mathbf{W}$ con dimensioni $m \times
n$ dove $m$ è il numero di outputs (*percettroni*) e $(n+1)$ il numero di inputs
(dove il $+1$ può essere omesso e rappresenta il *bias*).

## Multilayer Perceptron
Un layer completamente connesso permette di "mappare" le features in input su un
nuovo spazio vettoriale. Concatenare vari layer uno dopo l'altro permette di
"processare" l'informazione in input, mappandola in diversi spazi vettoriali.
Ciò che la rete fa è essenzialmente apprendere delle features latenti del
dataset che possono essere utilizzate per discriminare dei problemi anche non
linearmente separabili (poiché esse lo sono). In un problema di
multi-classificazione, l'ultimo layer sarà composto da tanti neuroni quanti sono
le classi che fanno parte del problema di classificazione, poiché i vari neuroni
fungeranno da classificatori per la stessa classe. Inoltre, in questo tipo di
problemi si applica una funzione *softmax* sul vettore dei neuroni di output
$\mathbf{z}$, per cui l'output della rete sarà
$$
\mathbf{y} = \frac{e^{\mathbf{z}}}{\sum_{k=1} e^{z_k}}
$$
La probprietà della softmax è che crei una *distribuzione* sulle varie
componenti del vettore, che in questo caso sono le varie classi. Per questo è
possibile calcolare una metrica di errore che funziona meglio per problemi di
classificazione, cioè la *cross-entropy loss*:
$$
\mathscr{L}(\mathbf{y}, \mathbf{t}) = -\sum_j t_j log(y_j)
$$

### Overfitting
Più aumenta la complessità del modello (numero di layer e neuroni), più esso
sarà in grado di adattarsi al dataset di train iniziale. Bisogna però fare
attenzione al tradeoff bias-variance. Un classificatore troppo coomplesso si
adatterà troppo ai dati, provocando *overfitting* (cioè un fenomeno in cui il
classificatore modella così bene i dati di input che non è in grado di
generalizzare). Tipicamente si introduce una regolarizzazione dei pesi in norma
$\mathscr{L}^2$ in modo tale da penalizzare le soluzioni con valori dei pesi
troppo grandi.

In un problema di machine learning, per capire se ci si trova di fronte ad un
modello overfitted, si divide il dataset originale in dataset di train e dataset
di test. Si apprende il modello sullo split del dataset di train e si testa su
quello di test. L'overfit si verifica quando l'errore di train è più alto
dell'errore di test.  

## Reti Feed Forward e Immagini
Fino ad ora si è supposto che le features di un problema di classificazione
fossero delle qualità intrinseche che potessero rappresentare le varie istanze,
come ad esempio il peso, l'altezza, la grandezza ecc.
Viene naturale pensare che le immagini stesse possano essere viste come un
insieme di features, cioè ogni valore di pixel rappresenterebbe di per se una
feature. In questo senso, un'immagine $N \times M$ può essere rappresentata in
un vettore lungo $N \cdot M$, con i valori delle features corrispondenti ai
valori di intensità dei pixel. Nel dataset MNIST, composto da immagini $28
\times 28$ di cifre scritte a mano ed etichettate con la classe corretta
($0-9$), le immagini sono per l'appunto espresse in vettori lunghi $28 \cdot 28
= 768$ elementi.

### LeNet300
LeNet300 fu un'architettura introdotta alla fine degli anni 80 per fare
riconoscimento di cifre scritte a mano e classificarle (*handwritten digit
recognition*). La rete era completamente connessa (composta da soli layer fully
connected), la cui architettura era composta da:

* Input layer di dimensioni $768$ (il che è ragionevole poiché l'input sono
  immagini lunghe $768$);
* Layer hidden di dimensioni $300$;
* Layer hidden di dimensioni $100$;
* Layer di output di dimensioni $10$ (il numero di classi che coincide con il
  numero di cifre).

Nel paper originale venivano fatte diverse esplorazioni di parametri
dell'architettura, ma quella con performare migliori fu appunto quella
presentata precedentemente. 
Il problema di questo approccio è che questo tipo di rete non permette di
sfruttare la correlazione spaziale dei pixel che esiste in un'immagine
qualsiasi. Questo perché le immagini sono trattate semplicemente come dei
vettori, rimuovendo di fatto questa struttura bidimensionale e di conseguenza
l'informazione spaziale che ne consegue.

## Reti Neurali Convoluzionali
Le reti convoluzionali sono un'evoluzione di questo approccio, nate per
l'appunto per risolvere questa problematica e sfruttare la correlazione spaziale
dei pixel nelle immagini.

Gli oggetti nelle immagini sono caratterizzati da features spaziali ad alto
livello, come ad esempio *bordi*, *angoli*, ecc.. L'idea alla base delle reti
convoluzionali è quella di far si che la rete possa apprendere queste features
locali. La rete quindi utilizza dei *features detectors*, che data in input
un'immagine rappresentata per pixel, sottolineano le features significative.
Questa operazione viene implementata per mezzo di una convoluzione, in cui i
valori del filtro vengono appresi dalla rete durante il processo di
apprendimento. Il risultato dell'applicazione di questo filtro su un'immagine,
produce una *feature map*, cioè una rappresentazione dell'immagine nello spazio
delle feature che sono individuate dal filtro.

### Layer Convoluzionale
L'operazione di convoluzione è implementata da un Neurone Convoluzionale, che
non è nient'altro che un neurone in cui gli input sono vincolati spazialmente,
cioè sono connessi solamente ad un sottoinsieme dei neuroni a cui sono connessi
in input. Ovviamente, i valori del filtro (*pesi*) possono essere appresi
attraverso la backpropagation. Formalmente, l'output del neurone $i, j$ viene
calcolato come:
$$
\mathbf{y}_{i, j} = \sum_{l = 0}^{L} \sum_{k = 0}^{K} \mathbf{w}_{l, k} \cdot
\mathbf{x}_{(S \cdot i) + l, (S \cdot j) + k}
$$
Questa formula implementa un kernel di dimensioni $L \times K$ e stride $S$. La
dimensione del kernel è un iperparametro che definisce l'estensione spaziale
della connettività dei neuroni rispetto al layer precedente, e viene chiamato
anche *receptive field* (campo recettivo). D'ora in poi si supporrà che la
dimensione del kernel sia sempre $F \times F$. Lo striding indica invece di
quanto sono spaziati tra di loro i kernel nel layer di input. Siccome la feature
map risultante è più piccola in dimensioni del layer in input, si applica uno
*zero-padding* ($P = (F - 1)/2$), in modo tale che l'output preservi le
dimensioni del layer in input. Ovviamente è possibile applicare questa tipologia
di rete anche ad immagini RGB, semplicemente applicando un filtro per ogni
canale e poi si sommandoi risultati in modo da combinare l'informazione delle
varie features apprese nei vari canali.

Si noti che il numero di parametri è più ristretto rispetto ad un layer fully
connected, siccome ogni neurone è connesso solo ad un sottoinsieme ristretto di
neuroni di input ($F \times F$). Più precisamente, il numero di parametri per un
layer convoluzionale è pari a:
$$
n_{out} \cdot (S + F^2) \cdot n_{ch}
$$
dove:

* $n_{out}$ numero di neuroni convoluzionali;
* $n_{ch}$ numero di canali ($=1$) per immagini *grayscale* e $=3$ per immagini
  RGB;
* $F$ dimensione del kernel ($F \times F$).

Questo minore numero di parametri permette sia di alleggerire molto la fase di
apprendimento che di migliorare la generalizzazione del modello risultante.

Si rappresenta quindi un layer convoluzionale con la notaziona $Conv(F, S, P)$,
in cui ovviamente $F$ è la dimensione del filtro, $S$ lo stride e $P$ lo
zero-padding.

### MaxPooling Layer
Siccome le features map estratte dai layer convoluzionali possono essere viste
come delle immagini *filtrate*, anche queste immagini filtrate possono essere
sotto-campionate proprio come le immagini. Questa operazione permette di ridurre
ulteriormente i parametri totali della rete, siccome permette di ridurre la
dimensione dell'output, mantenendo però una certa quantità di informazione. 

Il layer di max-pooling permette di campionare dei valori dall'immagine secondo
il criterio di *massimo*. L'idea è sempre quella di applicare un filtro
*non-lineare* che faccia passare solamente il massimo tra tutti gli elementi
della finestra definita dal filtro. Anche in questo caso il filtro sarà $F
\times F$ e può essere applicato con un determinato stride $S$. Il layer di
max-pooling si applica tipicamente dopo un layer di convoluzione.

### Optical Character Recognition 
È possibile usare le reti convoluzionali per fare OCR. Ad esempio, si può
aggiungere una classe "undefined" (o background) che non rappresenta nessun
carattere. La rete poi può essere applicata a tutte le posizioni dell'immagine
attraverso un approccio a *sliding-window*. Per fare questa operazione in modo
più efficiente, però, si può sfruttare il dualismo tra layer fully connected e
convoluzionale. Essenzialmente, se un layer convoluzionale ha le stesse
dimensioni del kernel del suo input, diventa equivalente ad un layer fully
connected. La trasformazione dei layer fully connected in convolutional, ha però
il vantaggio di poter applicare la rete in parallelo con un approccio sliding
window su immagini di grandezza arbitraria. L'output saranno non più una singola
classe ma 10 feature maps con la grandezza della finestra (sliding window), in
cui i valori di ogni feature map indicano il valore di classe per quel
particolare punto in cui può essere posizionata la finestra. Questa accortezza
permette di ridurre notevolmente la complessità computazionale richiesta,
anzichè applicare per ogni posizione della sliding window la rete.

> Le reti che hanno solo layer convoluzionali e non fully-connected sono
> ragionevolmente chiamate *fully-convolutional*.

Una volta ottenuti i caratteri si va a fare poi una fase di postprocessing che
tramite tecniche statistiche e probabilistiche per ri-aggiustare il risultato.
Ci sono quindi due possibilità per apprendere una rete in un task di OCR:

1. Apprendere una rete convoluzionale con layer fully connected su singoli
   caratteri (quindi campioni piccoli), per poi fare un reshape dei layer fully
   connected in layer convoluzionali, mantenendone i pesi appresi (si prendono
   dai layer fully connected);
2. Apprendere una rete già *fully-convolutional* direttamente sulle immagini. In
   questo caso però le immagini devono essere tutte etichettate nelle varie
   posizioni.

### LeNet5
Questa rete fu una delle prime reti convoluzionali in cui si applicava il
pattern *convolve-and-pool*. Di seguito verrà utilizzata una versione
leggermente modificata di LeNet5. L'input sono immagini $32\times 32$, a cui si
applica un layer convoluzionale composto da 6 neuroni convoluzionali $Conv(5, 2,
1)$. In questo modo, vengono così generate in output $6$ differenti features
map, ognuna in grado di estrarre una particolare feature dall'input. Dopo questo
layer ne segue un layer di max-pooling su tutte le features map risultanti. Si
ri-appplicano poi un layer di convoluzione uguale al precedente, questa volta
però composto da $16$ neuroni, seguito da un layer di max-pooling ($2 \times
2$). Infine, seguono due layer fully connected, in modo da implementare la parte
discriminativa della rete. Riassumendo, l'architettura di LeNet5 è così
composta:

* Layer input (immagine padded $32 \times 32$);
* Layer di $6$ neuroni convoluzionali $Conv(5, 2, 1)$, Output: $6 \times 28
  \times 28$;
* Layer di max pool $2 \times 2$, Output: $6 \times 14 \times 14$;
* Layer di $16$ neuroni convoluzionali $Conv(5, 2, 1)$, Output: $16 \times 14
  \times 14$;
* Layer di max pool $2 \times 2$, Output: $16 \times 7 \times 7$;
* *(Layer di flatten)*;
* Layer fully connected $784 \times 100$;
* Layer fully connected $100 \times 10$ (seguito da un layer di *softmax*).

Nel caso invece si voglia considerare la versione fully-convolutional della
rete, basta rispettare la regola precedente, per cui bisogna sostituire i layer
fully connected con i seguenti layer (in ordine):

* Layer di $100$ neuroni convoluzionali $Conv(7, 1, 0)$ (cioè $7 \times 7$);
* Layer di $10$ neuroni convoluzionali $Conv(10, 1, 0)$ (cioè $1 \times 1$).

## Tecniche avanzate di apprendimento
Come visto in precedenza, l'apprendimento delle reti neurali viene fatto per
mezzo di un algoritmo di discesa del gradiente. Il *learning rate* è un
iperparametro dell'algoritmo di ottimizzazione molto importante, perché può fare
la differenza nell'ottenimento di un modello di buona qualità. Scegliere il
valore corretto però non è semplice, siccome valori di $\eta$ molto piccoli,
renderebbero lento l'apprendimento e porterebbero spesso ad una configurazione
associata ad un minimo locale. D'altra parte, un alto valore di $\eta$
renderebbe l'apprendimento troppo instabile (provoncando un effetto "bouncing",
o di "overshoot" dell'errore intorno al valore minimo).

Tipicamente il learning rate viene ridotto nel tempo in modo da dare
inizialmente la possibilità all'ottimizzatore di non finire subito in minimi
locali. Col crescere delle iterazioni, però, il learning rate descresce in modo
tale da focalizzare la convergenza sul minimo trovato.

Un modo per implementare questa strategia è il cosiddetto *esponential decay*,
in cui il learning rate decresce esponenzialmente col crescere delle epoche
$$
\eta = \eta_0 \cdot e^{-kt}
$$
Questa tipologia di aggiustamento del learning rate permette anche di evitare
l'overfitting.

## Varie
* **Batch training**: Anziché applicare l'update rule ad ogni sample, di fatto
  rendendo più instabile SGD, si cacola il gradiente per un batch, accumulando
  il grandiente sample per sample ed applicando l'update rule sulla media del
  gradiente. In questo modo si stabilizza meglio l'apprendimento.
* **Overfitting**: Quando l'errore sul training set è più basso dell'errore del
  test-set. Accade quando la complessità della rete è molto più grande rispetto
  ai campioni di train disponibili. Come evitarlo:
    * Ridurre la complessità del modello, riducendone il numero di parametri;
    * Introdurre un termine di regolarizzazione per penalizzare i pesi grandi e
      promorre soluzioni con pesi più piccoli;
    * Migliorare il training set per mezzo di data augmentation, ad esempio
      applicando varie trasformazioni alle immagini (scaling, illuminazione,
      blurring, contrasto ecc..);
    * Mixture of experts: Apprendere più modelli semplici e fare la media dei
      voti maggioritari sulla classe. Ovviamente aumenta la complessità
      computazionale;
    * Applicare Dropout: per ogni layer hidden, per ogni campione e per ogni
      iterazione dell'apprendimento, ignora ogni neurone con probabilità
      $p_drop$ (tipicamente posta a $0.5$). Durante il deployement si riduce
      questo parametro significativamente.
* **Batch Normalization**: Processo che consiste nell'applicare un
  ri-centramento e un re-scaling degli input di un layer, in modo tale che
  abbiano varianza unitaria e media nulla. Per far ciò, la correzione è
  calcolata sulla base della media e della varianza del mini-batch. Se $X =
  \{x_1, \dots, x_n\}$ è il minibatch in input, il layer di `BatchNorm` calcola
  $\sigma^2$ e $\mu$ rispetto a $X$. Successivamente, applica ad ogni $x_i$ la
  normalizzazione per generare il minibatch normalizzato $Y$.
* **Vanishing/Exploding Gradient**: Problema numerico introdotto durante il
  passo di backward in cui il gradiente viene moltiplicato per valori che
  possono renderlo molto grande oppure molto piccolo. Problema prevalentemente
  presente nelle reti profonde.
    * **Vanishing**: Si verifica quando i gradienti diventano estremamente
      piccoli durante la backpropagation. Ciò può succedere sopratutto in reti
      con molti layer, specialmente utilizzando funzioni di attivazione che
      compimono il loro input (Sigmoide, TanH). Quando i gradienti diventano
      troppo piccoli, i pesi dei layer precedenti potrebbero non essere
      aggiornati in modo appropriato, rendendo l'apprendimento dei layer più
      vicini agli input molto lento.
    * **Exploding**: Si verifica quando il gradiente dell'errore è molto grande
      e una volta retropropagato genera dei pesi molto grandi, causando una
      divergenza dei parametri del modello e di conseguenza rendendo il processo
      di apprendimento instabile.
  
  Per risolvere il problema si possono utilizzare varie tecniche come:
    * Inizializzare la rete tramite Xavier initialization, che consiste
      nell'inizializzare i pesi di un layer con $n_{in}$ parametri in input e
      $n_{out}$ parametri di output, campionando da una Gaussiana con media
      nulla e varianza pari a $1/n_{in} + n_{out}$.
    * Cambiare le funzioni di attivazione in funzioni ReLU nei layer hidden.
    * Fare Batch Normalization.
    * Applicare delle connesioni residuali (*skip connections*).
* **Transfer Learning**: Permette di partire da un modello già appreso su un
  task e cambiarne il task, evitando di dover ripetere l'apprendimento from
  scratch, e quindi di possedere grandi quantità di dati e potenza di calcolo.
  Se si volesse cambiare ad esempio il task di una rete appresa su un task di
  object detection su altre classi di oggetti i passaggi potrebbero essere:
    * Rimuovere l'ultimo layer fully-connected con un nuovo layer
      fully-connected per classificare le nuove classi.
    * "Congela" l'apprendimento (impostando $\eta = 0$) dei (primi) layer
      convoluzionali, in modo da evitare che i feature detectors appresi
      precedentemente non vengano modificati. 
    * Apprendi la rete risultante su un dataset ridotto di dati custom.
* **Object detection**: task che comprende sia la *localizzazione* che la
  *classificazione* di un oggetto presente in un immagine.
    * Localizzazione: per localizzare un oggetto, si racchiude in una bounding
      box che può essere rappresentata: 
        1. Mediante le coordinate dei due punti $(x_1, y_1), (x_2, y_2)$ più
        distanti tra loro (top left, bottom right corners).
        2. Mediante le coordinate del punto top-left e della sua grandezza in
        termini di larghezza e altezza. Entrambi gli approcci necessitano di
        *normalizzare* le coordinate delle immagini in un range tra $-1$ e $1$.

  Per fare quindi detection si prende una rete (possibilmente anche già appresa)
  e se ne considerano i features-extractors. Si sostituiscono i layer di
  discriminazione con due classificatori:
    1. *Localization Head*: avrà 4 neuroni in output (uno per valore della
       coordinata della bounding box).
    2. *Classification Head*: avrà tanti neuroni in output quante le classi.

   L'apprendimento deve tenere conto sia dell'errore della classificazione
   che della localizzazione, per cui si crea una funzione di errore $E =
   \gamma E_C + (1 - \gamma) E_L$, dove $E_C$ è l'errore di classificazione
   (es. cross-entropy loss), $E_L$ è l'errore di localizzazione (es. mean
   square error) e $\gamma$ un iperparametro della funzione di loss che
   regola quale dei due errori prediligere.
* **Intersection Over Union (IOU)**: metrica che si utilizza nel contesto
  dell'object detection per valutare l'accuratezza delle predizioni sulle
  bounding-box. Essenzialmente si calcola come il rapporto tra l'area di
  intersezione tra la bounding box data dalla rete e quella ground-truth
  (taggata nel dataset) e l'area dell'unione tra le due. Se $=1$ c'è un overlap
  perfetto, mentre se $0$ non c'è nessun overlap.
* **Multiple Object Recognition**: si imposta la rete sempre con un approccio
  sliding-window. Potrebbe riconoscere lo stesso oggetto in diverse sliding
  window, per cui si fa di solito un operazione di thresholding (es. confidenza
  $>90$%). Per calcolare il thresholding si provano diversi parametri di
  thresholding e si cerca quello che massimizza il massimo a priori di
  (che massimizza sia precision che recall).
* **Multiscale Object Detection**: nelle immagini possono comparire oggetti di
  diverse dimensioni. I feature extractors con un numero alto di pooling
  funzionano bene per oggetti grandi, mentre per oggetti piccoli è vero il
  contrario (meno pooling layers). Per tenere conto di queste differenze, si
  preferisce una rete con un pooling più piccolo possibile e poi fare
  augmentation facendo *scaling* delle immagini nel dataset.
* **R-CNN**: le reti *fully-convolutional* non sono computazionalmente
  efficienti, perché devono calcolare il loro output per tutte le possibili
  sliding window per cui non sono adatte a task *real-time*. Un approccio più
  efficiente è R-CNN, che consiste nell'estrarre delle "proposte" di regioni che
  potrebbero contenere oggetti e poi farle classificare dalla rete.
* **Fast R-CNN**: è come R-CNN ma al posto di proporre delle zone direttamente
  sull'immagine, prima si va a fare una feature extraction sull'immagine e poi
  si va a fare proposal sulla feature map ottenuta.
* **Faster R-CNN**: si utilizza semplicemente una rete per fare region proposal
  (*Region Proposal Network*).
* **YOLO** (*You Only Look Once*): algoritmo di object detection che processa
  l'immagine in un singolo foward pass, rendendolo di fatto efficiente e adatto
  per fare real-time object detection. L'algoritmo funziona nel modo seguente:
    1. Si divide l'immagine in una griglia di dimensioni predefinite
     (iperparametro).
    2. Si predicono le bounding box rappresentate da 5 valori $(x, y, w, h,
     confidence)$, dove:
        * $(x, y)$ sono le coordinate del centro della bounding box relativo
          alla cella.
        * $(w, h)$ sono le dimensioni della BB relative all'intera immagine.
        * $confidence$ rappresenta la confidenza (score) che la BB contenga un
          oggetto qualsiasi.
    3. Si predice la classe per ogni BB trovata. Si moltiplica poi la
       probabilità della classe per il confidence score.
    4. Si rimuovono le BB ridondanti con il principio di *non maximum
       suppression* (si tiene solo la BBc con il massimo score).
    5. Le BB rimanenti avranno uno score classe/confidenza massimo e
       rappresentano l'output della rete.
* **Semantic Segmentation** (aka *Pixel-wise Image Classification*): task che
  consiste nel generare una "maschera" che colori gli oggetti di una scena in
  modo tale che tutti gli oggetti della stessa classe abbiano lo stesso colore.
  Si può implementare sempre per mezzo di un approccio *sliding-window* con uno
  stride di 1 pixel tra le varie finestre. L'idea è quella di apprendere un
  classificatore su "pezzi" di immagini (grandi quanto la sliding window) e
  taggarli opportunamente. Per ridurre la complessità computazionale si utilizza
  spesso una rete *fully-convolutional*. Il problem di questo approccio è che la
  risoluzione risultante è ridotta di $2^{\text{nr. pooling layers}}$. Invece,
  in caso si rimuovessero i pooling layers, l'immagine sarebbe preservata in
  dimensioni, ma la complessità sarebbe particolarmente elevata siccome le
  feature map "nel mezzo" rimarrebbero della stessa dimensione. L'intuizione per
  la soluzione di questo task è quella di creare una rete fully convolutional
  che faccia un "downsampling" nel layer iniziali (fino ad ottenere una FM con
  risoluzione più bassa), per poi fare upsampling, ri-scalando ad una dimensione
  che sia uguale all'immagine in input. Fare upsampling ci sono due approcci
  principali:
    1. *Upsample and Convolve*: consiste nel fare prima un upsampling ricopiando
       i pixel e poi applicando una convoluzione. Ovviamente i parametri della
       convoluzione saranno apprendibili per cui la rete imparerà ricostruire
       l'input al meglio dopo l'upsampling.
    2. *Deconvolution (Transposed Convolution)*: si fa sempre uno sliding window
       di un filtro, ma l'output saranno *copie del filtro* pesate per l'input,
       in cui gli elementi nell'output che si *intersecano* vengono sommati tra
       loro.
  

  Per fare l'apprendimento sui singoli pixel si utilizza una funzione di loss
  come la cross entropy, siccome è un singolo problema di classificazione.
  Globalmente invece, si vuole massimizare l'IoU.

* **U-Net**: architettura neurale per fare semantic segmentation particolarmente
  popolare nell'ambito biomedico. L'architettura U-Net è caratterizzata da una
  struttura a forma di "U", che consiste in un percorso di compressione e uno di
  decompressione. I componenti principali di U-Net sono:
    * **Encoder**: parte iniziale della "U" che segue la tipica struttura di una
      rete convoluzionale, con layer convoluzionali, max-pool e con funzioni di
      attivazione.
    * **Collo di bottiglia**: al fondo della "U", c'è un layer che fa da "collo
      di bottiglia" che cattura le più astratte e compresse rappresentazioni
      dell'input.
    * **Decoder**: parte finale della "U", che consiste in una fase "espansiva"
      (decompressione). Utilizza dei layer di deconvoluzione (*Transposed
      Convolution*) che aumentano gradualmente le dimensioni spaziali delle
      rappresentazioni. Inoltre, vengono utilizzate delle *skip connections* per
      connettere i layer corrispondenti della fase di contrazione a quelli della
      fase di espansione. In questo modo si mitigano le perdite di informazioni
      che potrebbero occorrere durante il downsampling (max-pooling) nella fase
      di contrazione.
* **3D U-Net**: si estendono le convoluzioni ad un *cubo* 3D, essenzialmente
  sono gli input sono immagini *stacked* (es. ottenute con da una TAC).
* **Autoencoders**: modelli *generativi* non supervisionati composti da un
  *encoder* e un *decoder*. Il primo serve a comprimere l'input in uno spazio
  latente $h$, mentre il secondo serve a ricostruire l'input partendo dalla sua
  rappresentazione nello spazio latente $h$. L'apprendimento viene fatto
  essenzialemente in modo non supervisionato; dato un input si calcola l'errore
  tra l'input stesso e l'output dell'autoencoder.
* **Convolutional Autoencoders**: autoencoder che vengono utilizzati in caso
  l'input sia un'immagine. Sono commposti non solo da proiezioni non-lineari ma
  da convoluzioni/sampling e de-convoluzioni/upsampling.
  Possono essere usati in diversi tasks come:
  * *Super Resolution*: si prende un'immagine e si cerca di ricostruirla
    raddoppiandola di risoluzione.
  * *Image Encoding*: si prendono le rappresentazioni latenti delle immagini per
    comprimerle.
  * *Image Colorization*: partendo da un'immagine grayscale, si vogliono
    ottenere come output del decoder 3 feature map che rappresentano i vari
    canali di colore dell'immagine.
  * *Hole Inpainting*: partendo da un immagine a cui è stata rimossa una
    porzione, si cerca di ricostruire la parte mancante.
  * *Style Transfer*: data un immagine in uno stile, la si applica in un altro.
* **SegNet**: architettura fully convolutional per fare segmentation basata
  sulla struttura encoder-decoder. L'upsampling viene fatto attraverso un layer
  di max unpooling. Questo layer essenzialmente ricorda ad ogni layer di
  max-pool quale degli elementi è il massimo, per poi utilizzare le posizioni
  salvate per poter applicare la trasformazione inversa e re-inserire il massimo
  nella posizione salvata. Ovviamente, il max-unpooling layer funziona a coppie
  con un pooling layer.
* **Atrous Convolution**: un filtro di questo tipo è una convoluzione, spaziata
  di un parametro $d$ del filtro. Ad esempio, un filtro $2 \times 2$ con $d = 2$
  è equivalente a un filtro $5 \times 5$ in cui ogni seconda riga e colonna non
  sono presenti.
* **Self-Supervised pretraining**: si prende la rete (U-Net) e si apprende per
  generare l'immagine in input in modo non-supervisionato. Successivamente, si
  apprende solamente il decoder nel task di riferimento (es. segmentation) in
  modo supervisionato, utilizzando meno dati. Questo permette alla rete di
  apprendere dei feature extractor che siano utili (come se vedesse già dei
  dati). Nel caso invece della segmentation si sostituisce il decoder con un
  classificatore lineare e si procede sempre nel modo già visto.
* **GAN** (*Generative Adversarial Networks*): architettura di rete generativa
  composta da due componenti:
    1. *Generatore*: prende in input un'immagine che è rumore casuale e lo
    trasforma in un'immagine sintentica, che sia il più possibile
    indistinguibile dai dati reali (es. cani, visi, ecc..).
    2. *Discriminatore*: classificatore binario appreso a distinguere tra
    immagini reali e immagini sintetiche.
  
  L'apprendimento è basato su un processo competitivo tra il generatore e il
  discriminatore. L'obiettivo è apprendere un generatore a produrre immagini
  realistiche in modo tale da "ingannare" il discriminatore a credere che siano
  dei campioni reali. L'apprendimento avviene alternando i seguenti passaggi:
    1. **Update del discriminatore**: crea un batch composto sia da dati reali
       presi dal dataset che da campioni creati utilizzando il generatore. Fai
       poi uno step di learning del discriminatore su questo batch (calcola la
       loss e fai backpropagation).
    2. **Update del generatore**: genera un batch di esempi col generatore, poi
       utilizza il discriminatore per taggarli e fai un learning step del
       generatore su questo batch.

  Gli step vengono ripetuti fino alla convergenza o fino ad un massimo di steps.
* **Diffusion models**: modelli appresi nel ricostruire un'immagine originale a
  cui è stato aggiunto del rumore. Permettono di evitare la complessità delle
  reti GANs.
