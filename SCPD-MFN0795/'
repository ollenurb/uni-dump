\newpage
# Modello di programmazione Shared Memory
Programmare sistemi a memoria condivisa e' molto piu' semplice di programmare
sistemi message passing. Questo perche' il programmatore ha una visione globale
della memoria in questo tipo di sistemi, mentre nel message passing no. Si pensi
ad esempio il caso in cui si voglia sapere il valore di una determinata
variabile. Nei sistemi shared memory sarebbe immediato dal momento che la
variabile risiederebbe sicuramente in memoria, mentre nei sistemi message
passing tale variabile deve essere ricevuta da qualche altro processo ignoto.
Anche se piu' conveniente dal punto di vista di programmazione, tale paradigma
necessita di controllare gli accessi alla memoria in modo esplicito dal
programmatore tramite opportune sincronizzazioni.

> Il problema di questo tipo di sistemi e' che non hanno un'alta scalabilita' per
  delle questioni che verranno trattate successivamente. In generale, se si vuole
  vere un'alta scalabilita' e' preferibile il paradigma *message passing*, mentre
  se si si vuole avere un'applicativo con numero di UC basso si preferisce il
  paradigma *shared memory*. Questo perche' per un numero basso di UC il paradigma
  message passing e' molto piu' efficiente perche' riduce significativamente
  l'overhead della comunicazione.

Partiamo ora dalla definizione di un sistema multiprocessore a memoria
condivisa. In generale un sistema a memoria condivisa e' qualsiasi sistema in
cui ogni locazione di memoria possa essere acceduta da qualsiasi processore.
Ogni locazione di memoria ha inoltre un indirizzo univoco all'interno del range
possibile di indirizzi. In altri termini, hanno un *singolo spazio di
indirizzamento*.
Essenzialmente possiamo descrivere un sistema shared memory come un sistema di
processori connessi tra di loro e a loro volta connessi alla memoria attraverso
un sistema di interconnessione. Come gia' visto la rete di interconnessione puo'
essere di varie tipologie. Tipicamente, in qeusto tipo si sistema si impiega una
crossbar switch. I processori hanno a loro volta delle memorie di piccole
dimensioni molto performanti chiamate memorie *cache*.

**TODO: Inserire immagine Shared Memory**

Ci sono diversi metodi per programmare un sistema a memoria condivisa
multiprocessore, tra cui alcuni molti diversi tra loro:

* Utilizzando processi (*heavyweight*)
* Utilizzando threads (*lightweight*)
* Utilizzando un linguaggio completamente progettato per la programmazione
  parallela (es. Ada)
* Utilizzando routines di liberire di un linguaggio di programmazione
  sequenziale
* Modificando la sintassi di un linguaggio sequenziale creando di fatto un
  linguaggio parallelo
* Utilizzando un linguaggio sequenziale e "decorarlo" con delle direttive di
  compilazione (es. OpenMP)

Il primo approccio non e' molto utilizzato in parallel computing per il troppo
overhead causato dallo scheduling dei processi. I processi hanno un programma
completamente separato con le proprie variabili, il proprio stack e il proprio
heap, mentre i threads necessitano solamente di uno stack e un istruction
pointer. `pthreads` e' una libreria POSIX che fornisce dei costrutti di basso
livello (livello del sistema operativo) per operare con i threads. Per eseguire
un thread si utilizza la seguente chiamata, dove `&thread1` e' l'handle e
`proc1` e' una funzione da far eseguire dal thread.
```c
pthread_create(&thread1, NULL, proc1, &arg);
```
con la chiamata `pthread_join(&thread1, *status)` si puo' aspettare invece il
completamento del thread. I threads che non sono joined vengono chiamati
*detached*. Quando un thread termina, le sue risorse vengono di conseguenza
rilasciate.
Quando compiliamo un programma multitreaded, il compilatore potrebbe applicare
delle ottimizzazioni a livello di istruzioni, riodinandone l'ordine di
esecuzione. Ad esempio, lo statement
```c
a = b + 5
x = y + 4
```
potrebbe essere compilato per essere eseguito nell'ordine inverso
```c
x = y + 4
a = b + 5
```
ed essere comunque logicamente corretto. Questo tipo di ottimizzazioni viene
fatto da quasi tutti i compilatori moderni.

> Un'operazione e' detta *thread safe* se puo' essere chiamata da piu' threads
  simultaneamente e produrre sempre risultati corretti. L'I/O standard e' thread
  safe, poiche' se `println` viene chiamata da piu' threads simultaneamente, i
  caratteri non sono interfogliati.

## Accesso ai dati condivisi
Come detto in precedenza, nei sistemi a memoria condivisa bisogna prestare
particolarmente attenzione agli accessi in memoria per poter evitare eventuali
*data races*. Consideriamo due processi che vogliano aggiungere un'unita' alla
stessa variabile contenuta in memoria.
Ogni processo dovra' quindi preliminarmente leggere il contenuto della
variabile, calcolarne il risultato e poi scriverlo all'interno della variabile.
In base all'ordine in cui vengono eseguite le istruzioni di `read` si otterranno
diversi risultati. Per poter risolvere questo problema si utilizza un meccanismo
molto comune chiamato *sezione critica*

> La sezione critica e' un meccanismo che assicura che un solo processo (o
  thread) possa accedere ad una particolare risorsa alla volta

Il meccanismo di base per implementare una sezione critica e' attraverso dei
*lock* (o piu' generalmente dei *semafori*). Un lock e' una variabile binaria
che indica se un arbitrario processo/thread e' dentro la sezione critica o meno.
Funziona concettualmente come una serratura di una porta che puo' essere
chiusa/aperta.  Per implementare la lock a sua volta e' necessario che sia
implementata un'istruzione a livello hardware di lettura e scrittura chiamata
`CAS`.
