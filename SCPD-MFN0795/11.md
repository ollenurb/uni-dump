\newpage
# Calcolo parallelo su GPU
Fino ad ora abbiamo visto modelli che si basano sulle CPU. In generale,
l'architettura di una CPU moderna a pipeline e' costituita da diversi componenti
pricipali quali:

* Registri
* ALU
* Cache
* Componenti di OOO, Branch Prediction, Memory

Siccome i registri e l'ALU formano essenzialmente un core minimale per
l'esecuzione effettiva delle istruzioni, una direzione possibile per aumentare
la densita' di calcolo potrebbe essere quella di aumentare effettivamente il
numero di questi core sullo stesso die di silicio.
Possiamo inoltre notare come tutti questi componenti (a parte i registri e
l'ALU), siano stati progettati appositamente per fare pipelining in questa
tipologia di processori.
Inoltre, di tutti questi componenti, l'unita' di calcolo effettiva richiede
veramente poco spazio rispetto ad altri componenti come la cache. Questo
perche' la cache si e' essenzialmente sviluppata per avere uno storage sempre
piu' grande in modo da ridurre al minimo i cache misses.

Il pipelining e' una tecnica molto comoda e nascosta al programmatore, che ci
permette di eseguire codice sequenziale il parallelo, ma per questa ragione ci
impone anche dei vincoli che possono essere sorpassati solo cambiando modello di
esecuzione in primo luogo.

Un'idea possibile, quindi, potrebbe essere quella di rimuovere completamente
tutti i componenti adibiti al pipelining (compresa la cache) in favore di un
maggiore numero di unita' di calcolo. Il problema, pero', e' che se metto un
numero molto alto di queste unita' di calcolo, ogni core dovra' andare a leggere
il proprio flusso di istruzion indipendente dagli altri. Poter usare tutti
questi core contemporaneamente sarebbe impossibile, perche' il collo di
bottiglia dato dal traffico in memoria sarebbe enorme (immagina ogni processore
indipendente che va a leggere dalla memoria ogni volta dati indipendenti).

Per risolvere il problema, quindi, si puo' pensare di condividere con tutti i
cores un solo program counter (*Instuction Stream Sharing*). In questo modo,
tutti i threads eseguono *la stessa istruzione* contemporaneamente, ma su dati
diversi. La conseguenza di questo punto di vista e' che si vanno a ridurre tutti
i trasferimenti in memoria, andando a raggrupparli in "*blocchi*". Se prima ogni
threads accedeva potenzialmente ad elementi di array differenti, con questo
approccio ogni thread opera su una particolare cella dello stesso array,
riducendo di conseguenza il numero di accessi totali. Per questa ragione, il
modello di programmazione di questi sistemi e' detto **SIMD** (*Single
Instruction Multiple Data*).

Questo modo di vedere le cose ha delle implicazioni profonde sulla
programmazione di sistemi SIMD. Una di queste e' che il codice sequenziale non
puo' essere utilizzato in nessun modo senza essere ristrutturato per supportare
questa tipologia di architettura. Un'altra problematica e' quella della *thread
divergence*, che si verifica quando ogni unita' di calcolo deve fare un
branching. Si consideri, ad esempio, il seguente codice:

```c
for(i=0; i < N; i++)
    if(A[i] < 0) A[i] = -A[i];
    else A[i] = A[i] + B[i];
```

Quale delle due istruzioni dovranno eseguire tutti i threads? Ovviamente, non e'
possibile farlo, per cui l'unico modo e' far eseguire il corpo dell'`if` da
tutti i threads per cui e' vera mantenendo quelli per cui e' falsa in *idle*, e
poi fare lo stesso per il branch `else`.

## Da SMD a SMT
Per mitigare queste problematiche, le GPU utilizzano un modello **SIMT** (*Single
Instruction Multiple Threads*). Il focus essenzialmente non e' quello di
eseguire la stessa istruzione per tutti i cores, ma invece per tutti i threads.
In questo modo e' possibile eseguire altri stream di istruzioni mentre si stanno
aspettando i dati dalla memoria, una tecnica chiamata *latency hiding*.

La programmazione di GPU ruota intorno al concetto di kernel, una funzione che
indica coda deve fare il thread generico di una batteria di threads.
Tale batteria e' piu' propriamente chiamata *blocco* di threads. Ogni threads
all'interno di un blocco condivide la memoria con gli altri.

