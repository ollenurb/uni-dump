\newpage
# Modello di programmazione Shared Memory
Programmare sistemi a memoria condivisa e' molto piu' semplice di programmare
sistemi message passing. Questo perche' il programmatore ha una visione globale
della memoria in questo tipo di sistemi, mentre nel message passing no. Si pensi
ad esempio il caso in cui si voglia sapere il valore di una determinata
variabile. Nei sistemi shared memory sarebbe immediato dal momento che la
variabile risiederebbe sicuramente in memoria, mentre nei sistemi message
passing tale variabile deve essere ricevuta da qualche altro processo ignoto.
Anche se piu' conveniente dal punto di vista di programmazione, tale paradigma
necessita di controllare gli accessi alla memoria in modo esplicito dal
programmatore tramite opportune sincronizzazioni.

> Il problema di questo tipo di sistemi e' che non hanno un'alta scalabilita' per
  delle questioni che verranno trattate successivamente. In generale, se si vuole
  vere un'alta scalabilita' e' preferibile il paradigma *message passing*, mentre
  se si si vuole avere un'applicativo con numero di UC basso si preferisce il
  paradigma *shared memory*. Questo perche' per un numero basso di UC il paradigma
  message passing e' molto piu' efficiente perche' riduce significativamente
  l'overhead della comunicazione.

Partiamo ora dalla definizione di un sistema multiprocessore a memoria
condivisa. In generale un sistema a memoria condivisa e' qualsiasi sistema in
cui ogni locazione di memoria possa essere acceduta da qualsiasi processore.
Ogni locazione di memoria ha inoltre un indirizzo univoco all'interno del range
possibile di indirizzi. In altri termini, hanno un *singolo spazio di
indirizzamento*.
Essenzialmente possiamo descrivere un sistema shared memory come un sistema di
processori connessi tra di loro e a loro volta connessi alla memoria attraverso
un sistema di interconnessione. Come gia' visto la rete di interconnessione puo'
essere di varie tipologie. Tipicamente, in qeusto tipo si sistema si impiega una
crossbar switch. I processori hanno a loro volta delle memorie di piccole
dimensioni molto performanti chiamate memorie *cache*.

**TODO: Inserire immagine Shared Memory**

Ci sono diversi metodi per programmare un sistema a memoria condivisa
multiprocessore, tra cui alcuni molti diversi tra loro:

* Utilizzando processi (*heavyweight*)
* Utilizzando threads (*lightweight*)
* Utilizzando un linguaggio completamente progettato per la programmazione
  parallela (es. Ada)
* Utilizzando routines di liberire di un linguaggio di programmazione
  sequenziale
* Modificando la sintassi di un linguaggio sequenziale creando di fatto un
  linguaggio parallelo
* Utilizzando un linguaggio sequenziale e "decorarlo" con delle direttive di
  compilazione (es. OpenMP)

Il primo approccio non e' molto utilizzato in parallel computing per il troppo
overhead causato dallo scheduling dei processi. I processi hanno un programma
completamente separato con le proprie variabili, il proprio stack e il proprio
heap, mentre i threads necessitano solamente di uno stack e un istruction
pointer. `pthreads` e' una libreria POSIX che fornisce dei costrutti di basso
livello (livello del sistema operativo) per operare con i threads. Per eseguire
un thread si utilizza la seguente chiamata, dove `&thread1` e' l'handle e
`proc1` e' una funzione da far eseguire dal thread.
```c
pthread_create(&thread1, NULL, proc1, &arg);
```
con la chiamata `pthread_join(&thread1, *status)` si puo' aspettare invece il
completamento del thread. I threads che non sono joined vengono chiamati
*detached*. Quando un thread termina, le sue risorse vengono di conseguenza
rilasciate.
Quando compiliamo un programma multitreaded, il compilatore potrebbe applicare
delle ottimizzazioni a livello di istruzioni, riodinandone l'ordine di
esecuzione. Ad esempio, lo statement
```c
a = b + 5
x = y + 4
```
potrebbe essere compilato per essere eseguito nell'ordine inverso
```c
x = y + 4
a = b + 5
```
ed essere comunque logicamente corretto. Questo tipo di ottimizzazioni viene
fatto da quasi tutti i compilatori moderni.

> Un'operazione e' detta *thread safe* se puo' essere chiamata da piu' threads
  simultaneamente e produrre sempre risultati corretti. L'I/O standard e' thread
  safe, poiche' se `println` viene chiamata da piu' threads simultaneamente, i
  caratteri non sono interfogliati.

## Accesso ai dati condivisi
Come detto in precedenza, nei sistemi a memoria condivisa bisogna prestare
particolarmente attenzione agli accessi in memoria per poter evitare eventuali
*data races*. Consideriamo due processi che vogliano aggiungere un'unita' alla
stessa variabile contenuta in memoria.
Ogni processo dovra' quindi preliminarmente leggere il contenuto della
variabile, calcolarne il risultato e poi scriverlo all'interno della variabile.
In base all'ordine in cui vengono eseguite le istruzioni di `read` si otterranno
diversi risultati. Per poter risolvere questo problema si utilizza un meccanismo
molto comune chiamato *sezione critica*

> La sezione critica e' un meccanismo che assicura che un solo processo (o
  thread) possa accedere ad una particolare risorsa alla volta

Il meccanismo di base per implementare una sezione critica e' attraverso dei
*lock* (o piu' generalmente dei *semafori*). Un lock e' una variabile binaria
che indica se un arbitrario processo/thread e' dentro la sezione critica o meno.
Funziona concettualmente come una serratura di una porta che puo' essere
chiusa/aperta. Per implementare la lock a sua volta e' necessario che sia
implementata un'istruzione a livello hardware di lettura e scrittura chiamata
`CAS` (*Compare And Swap*). Tale istruzione essenzialmente viene impiegata per
scrivere e leggere nello stesso momento una locazione in memoria, cioe' in modo
atomico.

> Se si volesse implementare un lock, un processo utilizzerebbe `CAS` andando a
  leggere e scrivere successivamente il valore 1 nella variabile lock, ignorando
  preliminarmente cio' che c'era scritto precedentemente. Una volta scritto,
  viene poi visto se la variabile era effettivamente a 0 o a 1. In caso fosse a
  1 ricicla con la stessa operazione.

In pthread i locks vengono chiusi/aperti con le primitive
`pthread_mutex_lock(&mutex1)` e `pthread_mutex_unlock(&mutex1)`. Se un thread
raggiunge un lock e lo trova chiuso, aspettera' fino a quando il lock non sara'
stato aperto. Se piu' di un thread aspetta il lock, il sistema scegliera' un
thread tra quelli in attesa per poter continuare. Ovviamente solo il thread che
chiude il lock puo' rilasciarlo aprendolo.
L'implementazione della lock puo' essere fatta mediante attesa *attiva* o
*passiva*. L'attesa attiva consiste nel ciclare fin quando il lock non e'
aperto, mentre l'attesa passiva consiste nel sospendere il processo/thread e
mandarlo in coda di sleep.
Il problema di determinare una delle due implementazioni e' difficile perche'
varia in base al contesto. In casi in cui i processi occupano i lock per poco
tempo, conviene implementarli con attesa attiva, mentre la sospensione quando
occupano molto tempo.

> Task fine grained $\rightarrow$ active waiting
>
> Task coarse grained $\rightarrow$ passive waiting

Il deadlock e' una condizione che si verifica quando un thread/processo che ha
ottenuto una risorsa richiede una risorsa che e' bloccata da un'altro thread,
nello stesso momento in cui tale thread necessita della risorsa bloccata.

Uno dei modi per *rompere* la dipendenza bloccante che puo' causare un deadlock,
e' rimuovere l'aspetto bloccante del lock dei mutex. Ad esempio
`pthread_mutex_trylock()` e' una primitiva che testa se il lock e' stato preso o
meno, senza bloccare il codice. Ovviamente l'utilizzo di tale primitiva e' una
soluzione non particolarmente buona che non risolve realmente il problema alla
radice.

Le lock come detto derivano essenzialmente da un oggetto piu' generico che e' il
semaforo. Un semaforo e' un oggetto che supporta due operazioni principali
indivisibili `P` e `V`. Possiamo descrivere il semaforo come una variabile
arbitraria che viene inizializzata ad un valore. La semantica di `P` e' quella
di decrementare il valore di uno, mentre quella di `V` e' quella di
incrementarlo di uno. La peculiarita' e' che quando un processo/thread cerca di
utilizzare `P` quando il valore e' a $0$ (e il risultato sarebbe
conseguentemente $< 0$) viene bloccato fino a quando il valore non e' $>0$
(cioe' un'altro processo/thread ha incrementato la variabilec con `V`).
Possiamo quindi notare come una lock sia semplicemente un semaforo impostato con
valore iniziale a 1.

Un *monitor* e' un'altro costrutto di sincronizzazione che permette ai threads
di avere mutua esclusione, con la possiblita' di sospendersi all'interno di essa
per aspettare una determinata condizione. Un monitor consiste in una **lock** e
una **condition variable**. Le condition variables sono essenzialmente dei
contenitori di threads che aspettano una determinata condizione. I monitor
provvedono dei meccanismi di sincronizzazione che permettono ai thread che hanno
acquisito il lock di rilasciarlo temporaneamente in modo che una determinata
condizione possa verificarsi. In Pthread questa operazione e' codificata dalla
funzione `pthread_cond_wait(condition, lock)`, che mette il thread in stato
*wait* fino a quando `condition` non e' vera, rilasciando il `lock` del monitor.
Con la funzione `pthread_cond_signal(condition)` si sveglia invece un qualsiasi
thread che e' bloccato su quella condizione. Il thread scelto dipende dalle
policy del sistema, per cui non possiamo determinarlo a priori.
La funzione `pthread_cond_broadcast(condition)` e' invece una variante della
signal che permette di svegliare tutti i threads.

> Alternativamente possiamo definire un monitor come una classe, oggetto o modulo
  thread-safe con un lock (o mutex) che permette l'accesso sicuro ad un metodo o
  ad una variabile in modo mutualmente esclusivo. Semplicemente permette a dei
  metodi di avere una lock intrinseca in modo che solo un processo/thread possa
  eseguire il codice contenuto contemporaneamente. Inoltre, all'interno di ogni
  metodo e' possibile dare il controllo ad altri threads in modo da aspettare il
  verificarsi di una determinata condizione (tramite condition variables).

## Analisi delle dipendenze
Il vantaggio della programmazione di sistemi a memoria condivisa e' la relativa
portabilita' da codice sequenziale a codice parallelo. Una via alternativa ad
un porting totale del codice sequenziale ad un codice parallelo, e'
rappresentata dalla parallelizzazione *"automatica"* di alcuni costrutti
iterativi, come ad esempio i *loops*. Siccome la parallelizzazione automatica e'
eseguita da un compilatore parallelizzante, e' necessario trovare un metodo
algoritmico per determinare le porzioni di codice che possano essere eseguite in
parallelo. Facciamo un esempio e consideriamo il codice seguente
```c
for (i = 0; i < 5; i++)
    a[i] = 0
```
E' possibile notare che le operazioni eseguite nel corpo del loop siano
*indipendenti* tra di loro e quindi possono essere eseguite in qualsiasi ordine.
In altri termini, *cambiare l'ordine di esecuzione delle istruzioni dentro al
corpo del ciclo non cambia la semantica dell'intero programma*.
L'*analisi delle dipendenze* non e' nient'altro che la determinazione automatica
delle porzioni di codice che hanno questa proprieta'.

Per fare cio', gli algoritmi si basano su dei risultati teorici, primi tra tutti
le *Condizioni di Bernstein*.

> **Condizioni di Bernstein**: Insieme di condizioni *sufficienti* a determinare
> se due statements possono essere eseguiti in qualsiasi ordine. Dati i
> seguenti insiemi
>
>   * $I_i$ che indica l'insieme delle locazioni di memoria *lette* (input)
>     dallo statement $S_i$
>   * $O_j$ che indica l'insieme delle locazioni di memoria *scritte* (output)
>     dallo statement $S_j$
>
> diciamo che due processi $P_1$ e $P_2$ possono essere eseguiti in parallelo se
> le tre condizioni seguenti sono soddisfatte
>
>   1. $I_1 \cap O_2 = \emptyset$
>   2. $I_2 \cap O_1 = \emptyset$
>   3. $O_1 \cap O_2 = \emptyset$

Facciamo ora alcuni esempi per ogni condizione partendo dalla prima, che modella
la *True Data Dependency*. Essenzialmente puo' essere riassunta dal seguente
codice:
```c
A = B + C;
D = A - 2;
```
in cui il problema e' la dipendenza della seconda istruzione rispetto alla
prima. La seconda modella invece la *Anti-Dependency*, riassunta mediante:
```c
A = B + C;
B = 3;
```
in questo caso non si ha una TDD, ma il problema risiede nel fatto che `B` possa
essere scritta prima che la prima riga possa essere eseguita.
Infine, la terza modella la *Output Dependency*:
```c
C=3;
C=4;
```
in cui il problema risiede nel fatto che non sia possibile determinare il vero
valore di `C`.

## OpenMP
Parlando di compilatori parallelizzanti e' impossibile non parlare anche di
**OpenMP**, uno standard che introduce delle direttive di compilazione per la
parallelizzazione di statements. L'idea e' quindi utilizzare dei pragma di
compilazione, mantenendo sempre un linguaggio sequenziale come base (quali
Fortran e C/C++). OpenMP essenzialmente nasce per parallelizzare i loop, anche
se le nuove versioni si discostano dall'obiettivo inziale.
Tutte le direttive di OpenMP hanno la forma `#pragma omp directive_name`. Ad
esempio la direttiva `#pragma omp parallel` indica di parallelizzare lo snippet
che segue. Il codice viene parallelizzato poi seguendo un modello *"fork-join"*,
ma basato sui threads. Cio' significa che le porzioni sequenziali vengono
eseguite da il thread master (mentre gli altri threads non fanno nulla), mentre
le porzioni parallele vengono naturalmente eseguite da tutti i threads. Il
numero di threads di default e' pari al numero di threads della macchina
(in modo da ottenere il throughtput massimo), ma e' possibile specificare un
numero differente nelle pragmas.

**TODO**: Aggiungi immagine fork-join vs OpenMP

Alcune direttive che mette a disposizione OpenMP sono:

1. `Sections`: definisce blocchi che vengono condivisi dai thread
2. `For`: indica che il loop che segue la pragma deve essere eseguito in
   parallelo
3. `Single`: specifica che una sezione all'interno di una sezione parallela
   debba essere eseguita da un solo thread
4. `Atomic`: definisce una sezione che viene eseguita da un thread per volta
5. `Flush`: rende la memoria consistente con lo stato dei thread fino a quel
   momento. Il flush viene eseguito automaticamente all'inizio e/o fine di
   alcune direttive

OpenMP e' particolarmente semplice da utilizzare, ma necessita di diverse
accortezze nella programmazione, per cui e' fondamentale saper riconoscere
alcuni *code smells*. Ad esempio, il seguente snippet e' considerato code smell
```c
i=0
for(i = 0; i < N; i++) { ... }
```
perche' si sta utilizzando una variabile solamente per fare il conteggio del
loop in uno scope globale. Inoltre, il problema e' evidenziato anche dal fatto
che se si parallelizzasse il loop
```c
i=0
#pragma OMP parallel
for(i = 0; i < N; i++) { ... }
println(i)
```
il valore di output di `i` non e' deterministico. E siccome lo scope di tale
variabile e' globale, e' possibile di conseguenza utilizzarla anche fuori dal
loop, per cui il compilatore OMP darebbe sicuramente errore.
La soluzione e' data dal seguente codice
```c
#pragma OMP parallel
for(int i = 0; i < N; i++) { ... }
```
Con questa piccola accortezza, otteniamo diversi vantaggi. Il primo e' che il
compilatore (anche sequenziale) assegnerebbe la variabile ad essere un registro
(e quindi velocizzando di conseguenza le operazioni di count). La seconda e' che
la varibile rimane nello scope del loop per cui non e' possibile utilizzarla al
di fuori di esso. Infine, la terza (conseguenza della seconda) e' che e'
possibile parallelizzarlo automaticamente.

## Problemi di Prestazioni in Sistemi Shared Memory
Come detto in precedenza, la programmazione di sistemi a memoria condivisa ci
permette di parallelizzare in modo relativamente facile un programma
sequenziale. In alcuni casi, pero', e' possibile che i programmi parallelizzati
non abbiano delle buone prestazioni, proprio per problematiche legate al fatto
che la memoria sia condivisa (si pensi ad esempio al caso in cui un grosso
numero di threads debba scrivere nella stessa variabile).
La prima cosa da capire bene per comprendere al meglio queste problematiche e'
la *memoria cache*. Una memoria cache e' una memoria ad alte prestazioni molto
vicina (fisicamente) al processore. Le memorie cache si basano su un principio
detto *principio di localita'*, che a sua volta si suddivide in:

* **Localita' spaziale**: secondo la quale e' molto probabile che dopo
  un'istruzione che accede ad una determinata locazione di memoria, la
  successiva dovra' accedere ad una locazione vicina (nella stesssa *pagina* di
  memoria). Un esempio e' l'accesso agli array in memoria
* **Localita' temporale**: secondo la quale e' molto probabile che la prossima
  istruzione da eseguire sia la stessa di quella eseguita. Un esempio sono i
  loops

La probabilita' per cui una porzione di memoria richiesta stia gia' dentro alla
cache e' detto *hit rate*. Se parliamo di tempo di accesso in memoria, possiamo
formulare una legge abbastanza generica per il tempo di accesso. Supponiamo che
il processore voglia accedere alla memoria e se ne voglia calcolrare il tempo di
accesso ($t_{a}$) in memoria. Se ipotizziamo inoltre che la cache abbia 2
livelli intermedi, allora possiamo dire che
$$
t_{a} = p_{L_1} \cdot t_{L_1} + (1 - p_{L_2}) \cdot t_{L_2}
$$
dove $p_{L_i}$ e' la probabilita' di hit del livello $i$ di cache. Se
ipotizzassimo che ci sia un'ulteriore livello di cache (L3), otterremmo che
$$
t_{a} = p_{L_1} \cdot t_{L_1} + (1 - p_{L_2}) \cdot p_{L_2} \cdot t_{L_2} + (1 -
p_{L_3}) \cdot t_{L_3}
$$
L'ottimizzazione di programmin paralleli che utilizzano la shared memory passa
attraverso la minimizzazione di questa formula. In genere e' un problema molto
difficile perche' le $p_i$ dipendono da molti fattori.
