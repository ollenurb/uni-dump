# Tecniche di Clustering
* Un cluster e' un insieme omogeneo di oggetti che sono tutti vicini tra loro
  secondo una relazione di distanza
* Il clustering puo' essere sia supervisionato che non supervisionato. Di
  solito, le tecniche di clustering piu' utilizzate sono di tipo non
  supervisionato
* Un buon clustering e' caratterizzato dalle seguenti caratteristiche:
    - I dati all'interno di un singolo cluster sono molto simili tra loro
    - I dati tra clusters differenti sono molto dissimili tra loro
* Ovviamente la qualita' di un clustering e' determinata:
    * Dalla misura con cui si definisce il concetto di similarita' (*distanza*)
    * Dal modello utilizzato (algoritmo di inferenza del modello)
* Le tecniche principali di clustering possono essere classificate in 5 approcci
  differenti:
    - *Algoritmi di Partizionamento*: costruiscono diverse partizioni e le
      valutano secondo determinati criteri. Ripetono la procedura fin quando la
      soluzione di clustering non soddisfa i criteri
    - *Clustering Gerarchico*: ritornano in output una descrizione dei cluster
      secondo una relazione gerarchica
    - *Density-Based*: formano i clusters in base alla densita' degli oggetti
    - *Grid-Based*: si basano su una struttura a diversi livelli di granularita'.
      Essenzialmente trattano i dati in basi a livelli livelli di dettaglio
    - *Model-Based*: derivano dall'ambito statistico. Inferisce delle
      distribuzioni di probabilita' che descrivono i clusters

## Algoritmi di Partizionamento
> **Obiettivo**: Costruisci una partizione di un dataset $D$ di $n$ oggetti in
un insieme $k$ di clusters

* Dato $k$, trova una partizione di $k$ clusters che ottimizzino un dato
  criterio di Partizionamento
* Siccome e' un algoritmo di ottimizzazione, possiamo differenziarli in metodi
    - **Esaustivi**: ritornano l'ottimo globale, ma hanno una complessita' elevata
      solitamente
    - **Euristici**: utilizzano un qualche tipo di euristica per ritornare l'ottimo
      locale, ma senza aver la garanzia che si tratti di un ottimo globale
* Alcuni algoritmi famosi (di tipo euristico) sono *k-means* e la sua variante
  *k-medoids*
    * *k-means*: i clusters sono rappresentati mediante i centroidi del cluster
    * *k-medoids*: i clusters sono rappresentati mediante i medoidi del cluster
* Come gia' detto, il clustering puo' essere visto come un problema di
  ottimizzazione, in cui bisogna massimizzare una funzione obiettivo.
  Tale funzione e' la funzione *errore*, definita nel modo seguente
  $$
  E = \sum^k_{i=1} \sum_{p \in C_i} \lVert p-m_i \rVert^2
  $$
  dove:
    * $k$ e' il numero di clusters
    * $m_i$ e' il medoide del cluster $i-esimo$
    * $p$ e' un oggetto appartenente al cluster $C_i$
    * $C_i$ e' il cluster *i-esimo*
* Si vogliono trovare delle soluzioni di cluster tali per cui le distanze di
  tutti gli elementi rispetto ai loro centri sono minori possibili

### K-Means
* L'algoritmo *k-means* riesce a minimizzare questa funzione, trovando un ottimo
  locale, e lo fa mediante 4 passaggi:
    1. Partiziona gli oggetti in $k$ clusters non vuoti
    2. Calcola i centroidi dei $k$ clusters (il centroide e' l'oggetto che ha
       come valore degli attributi la media dei valori di ogni attributo)
    3. Assegna ad ogni oggetto al cluster corrispondente al centroide piu' vicino
    4. Se la soluzione di clustering non e' cambiata, termina, altrimenti ripeti
       dal passo 2.
* La complessita' di *k-means* e' pasi a $O(kn)$, per cui e' molto efficiente
* Il problema e' che e' possibile che non converga mai ad una soluzione, per cui
  e' opportuno prevedere un numero di iterazioni prefissato massimo
* Alcune limitazioni di *k-means* sono:
    * Difficile applicarlo quando le features non sono di tipo numerico
    * Bisogna per forza specificare in anticipo il numero di clusters $k$
    * Non e' in grado di gestire bene dati *rumorosi* e gli *outliers*
    * Non e' in grado di trovare soluzioni di clusters che hanno una forma *non
      convessa*
* Alcune varianti come *k-modes* servono ad ovviare ad alcuni di queste
  limitazioni. Esso difatti consiste nel calcolare la *moda* al posto della
  *media*.
* *k-prototype* utilizza invece dei metodi misti per calcolare i medoidi,
  applicando la moda o la media in base al fatto che le features siano
  categoriche oppure numeriche

## Algoritmi Gerarchici
* Un algoritmo di clustering gerarchico puo' funzionare in due modi:
    * Agglomerativo: si parte da $n$ clusters composti da un solo elemento e si
      fondono mano a mano fino ad ottenere la soluzione finale
    * Divisivo: si fa l'opposto, cioe' si parte da un solo cluster singolo e si
      va a suddividere iterativamente man mano
* La maggior parte degli algoritmi gerarchici sono di tipo agglomerativo,
  siccome sono di piu' facile implementazione
* La soluzione di questa tipologia di algorimi e' un albero gerarchico, chiamato
  *dendrogramma*, in cui i nodi sono connessi mediante (solitamente, ma non
  necessariamente) una relazione binaria di inclusione
* Alcuni di questi algoritmi sono `AGNES` (*Agglomerative Nesting*), `DIANA`
  (*Divisive Analysis*) e `CURE` (*Clustering Using REpresentatives*)

```
Hierarchical Clustering(d, n)
  form n clusters each with one element
  construct a graph T by assigning one vertex to each cluster
  while there is more than one cluster
    find two closest clusters C_1 and C_2
    merge C_1 and C_2 ito new cluster  C with |C_1| + |C_2| elements
    compute distance from C to all other clusters
    add a new vertex C to T and connect to vertic C_1 and C_2
    remove rows and columns of d corresponding to C_1 and C_2
    add a row and column to d corresponding to new cluster C
    return T
```

### CURE
* Uno dei piu' interessanti e' proprio `CURE`, poiche' impiega dei meccanismi
  differenti per effettuare il Clustering
* Essenzialmente, fino ad ora, ogni algoritmo utilizzava un solo rappresentativo
  (es. medoide, centroide) per rappresentare il cluster.
* `CURE` utilizza invece una molteplicita' di rappresentativi per rappresentare
  i clusters
* Difatti, il drawback dei metodi basati sulla distanza, sono buoni solamente in
  casi in cui i clusters hanno una forma convessa, una dimensione relativamente
  piccola e se $k$ puo' essere stimato in modo semplice
* L'algoritmo `CURE(s, p, q)` mira a sopperire a questi drawbacks. Il concetto
  dell'algoritmo e' il seguente:
    1. Estrai un cluster *"campione"* $s$ in modo casuale
    2. Partiziona il campione in $p$ partizioni con dimensione $s/p$
    3. Clusterizza parzialmente le partizioni in $p/pq$ clusters
    4. Elimina gli outliers
    5. Raggruppa i clusters intermedi piu' vicini in un solo cluster
* Si noti che con l'operazione di raggruppamento un singolo cluster avra' piu'
  rappresentativi e non solo uno. In questo modo, le istanze rappresentative
  seguono la forma del cluster qualunque essa sia.
* In realta' l'algoritmo applica poi ulteriomente un'operazione di *shrinking*
  verso un centro di gravita' di una certa frazione $\alpha$, su tutti i punti
  rappresentativi

## Clustering basato sulla densita'
* Sono metodi che utilizzano come criterio per il clustering la densita' di
  punti (tipicamente all'interno di un determinato raggio)
* Questi algoritmi hanno la capacita' di rappresentare clusters di qualsiasi
  forma e riescono anche a gestire bene il rumore nei dati.
* Diversi famosi algoritmi quali:
    - `DBSCAN`
    - `OPTICS`
    - `DENCLUE`
    - `CLIQUE`
* Tutti questi algoritmi necessitano di solito di specificare un *raggio* per
  determinarne la terminazione

### DBSCAN
* *(Density Based Spatian Clustering of Applications with Noise)*
* L'algoritmo utilizza due parametri principali:
    * $\epsilon$: determina il raggio
    * $MinPts$: determina il numero minimo di punti (densita') che devono stare
      all'interno del cerchio di raggio $\epsilon$
* *Density Reachable*: Se un punto $p$ fa parte di uno dei punti $MinPts$ che
  stanno all'interno del raggio di un punto $q$
* *Density Connected*: Se un punto $p$ e' connesso tramite un "percorso" di
  punti density reachable tra loro a un punto $q$
* L'algoritmo essenzialmente va a considerare punto per punto di tutto il
  dataset, andando a costruire man mano gli insiemi di densita'
* Una volta calcolati gli insiemi di densita', va a fondere tutti i punti che
  sono density connected in un singolo cluster
* **Punti di forza**:
    * Fa una sola scansione dei dati (complessita' lineare)
    * I cluster hanno dimensione e forma arbitraria
    * E' in grado di gestire gli outliers e i dati rumorosi
    * Non c'e' bisogno di assegnare un numero di cluster predefinito
* **Punti deboli**:
    * La qualita' di clustering dipende sull'ordine con cui si analizzano i dati
    * La qualita' di clustering dipende dalla definizione di vicinato che si da
      in input

## Algoritmi Grid-Based
* Partizionano lo spazio andando a determinare degli intervalli sui valori degli
  attributi

### CLIQUE
* (*Clustering In QUEst*) puo' essere considerato come una combinazione tra un
  algoritmo basato su densita' e uno basato sul concetto di griglia
* Utilizza un meccanismo per partizionare i **valori** degli attributi
  descrittivi, e non le istanze vere e proprie
    1. Partiziona lo spazio dei dati e trova il numero di punti che ricadono
       all'interno di ogni cella della partizione
    2. Identifica i sottospazi che contengono clusters utilizzando un pricipio
       *a-priori*:
         * Determina le unita' dense in tutti i sottospazi di interesse
         * Determina le unita' densamente connesse in tutti i sottospazi di interesse

* Guardare esempio sulle slides

* Punti di forza:
    * Determina automaticamente i sottospazi di piu' alta dimensionalita' tali
      per cui la densita' di punti e' alta
    * Non e' sensibile rispetto all'ordine con cui si considerano le istanze
    * Scala linearmente con la dimensione degli input e ha una buona
      scalabilita' all'aumentare il numero di dimensioni
* Punti deboli:
    * L'accuratezza della soluzione di clustering potrebbe degradare alle spese
      di un'implementazione del metodo piu' semplice

## Clustering basato su Modelli
* L'idea e' quella di avere un qualche modello di come si distribuiscono i dati
  all'inteno del cluster. Essenzialmente utilizzano dele distribuzioni di
  probabilita' per determinare i clusters
* `SOM` (*Self Organizing Feature Map*) che e' basato sulle reti neurali
* Basati su una funzione di densita' di probabilita' quali `COWEB`. Questo
  algoritmo assume pero' ch ele distribuzioni dei valori degli attributi
  sono *indipendenti* tra di loro
* Un'altro esempio probabilistico e' un metodo che si chiama *Gaussian Mixture
  Model*, che rappresenta in un certo senso la variante probabilistica di
  *k-means*.
    * Si rappresentano i dati come distribuzioni Gaussiane
    * Si stimano i parametri iniziali delle Gaussiane
    * Si assegnano i punti al cluster in base alla probabilita' piu' alta
    * Si vanno a ri-stimare i parametri delle Gaussiane utilizzando i punti di
      appartenenza ottenuti precedentemente

## Clique Clustering
* Un'altro modo per far clustering e' quello che si basa sulla rappresentazione
  di clusters con i *cliques*
* Un **clique graph** e' un grafo in cui ogni vertice e' connesso a qualsiasi altro
  vertice
* E' possibile trasformare la matrice di distanze in un grafo. Ogni riga e' un
  vertice del grafo, per cui ogni vertice viene connesso ad un altro se la loro
  distanza e' minore di un threshold definito
* Il grafo risultante puo' contenere dei *cliques*, per cui per il modo in cui
  si e' costruito il grafo, i cliques rappresentano clusters composti da punti
  vicini tra loro
* Trasformando il grafo delle distanze in un clique graph, si possono ottenere i
  diversi clusters dei dati. Per trasformarlo, si recidono o aggiungono
  connessioni al grafo. Il risultato possono essere piu' clique graphs.
* Il problema di trovare una soluzione chiusa al problema di trovare i clique
  graphs e' NP-Completo, per cui si utilizzano delle opportune euristiche per
  ricercare lo spazio delle soluzioni

### CAST
* (*Cluster Affinity Search Technique*) e' un algoritmo che si basa sul clique
  Clustering
* $d(i, C) = $ distanza media tra l'istanza $i$ e tutte le altre istanze
  appartenenti al cluster $C$
* L'istanza $i$ e' vicina al cluster $C$ se $d(i, C) < \theta$, dove $\theta$ e'
  il threshold
* Utilizzando questa nozione di distanza, l'algoritmo prende in input il grafo e
  va a iterare su di esso andando a suddividerlo in sottografi

```
Cast(S, G, Theta)
  P <- Empty Set
  while S is not empty
    V <- vertex of maximal degree in the distance graph G
    C <- {V}
    while a close istance i not int C or distant gene i in C exists
      Find the nearest close istance i not in C and add it to C
      remove the farthest distant istance i in C
    Add cluster C to partition P
    S <- S \ C
    Remove vertices of cluster C from the distance graph G
return P
```

## Clustering per dati Biologici
* Il clustering e' un task che viene applicato molto in ambito biologico
* Esso e' utile sopratutto per *visualizzare* e *analizzare* una quantita' di
  dati biologici enorme
* Un esempio tipico e' l'analisi della funzionalita' dei geni, cioe' il task che
  ha l'obiettivo di capire quale funzione ha un gene appena sequenzializzato.
  Tipicamente consiste nel confrontare i geni sconosciuti con geni la cui
  funzionalita' e' gia' nota
* Per il 40% dei geni sequenzializzati, pero', comparare le sequenze non e'
  sufficiente a determinarne la funzionalita'
* Gli sperimenti con microarrays servono appunto a inferire la funzionalita' di
  un gene, anche quando la similarita' non e' sufficiente a determinarne la
  funzione
    * I microarray misurano l'attivita' espressiva dei geni sotto diverse
      condizioni
    * Il livello di espressione e' *stimato* misurando la quantita' di mRNA che
      un certo gene produce (piu' il gene e' attivo, piu' trascrizione di quel
      gene ci sara')
* Negli esperimenti a microarray, l'output e' essenzialmente una matrice di dati
  in cui sono indicati i diversi livelli di expressivita' dei geni scelti per
  l'esperimento. Ogni riga e' un gene differente, e ogni colonna indica il tempo
  di esposizione. Il valore e' l'espressione genica per quel lasso di tempo.
    * Ogni osservazione e' un punto nello spazio $N$-dimensionale
    * Calcolando la distanza tra ogni punto, i punti piu' vicini tra loro
      possono rappresentare geni che hanno le stesse caratteristiche espressive
      e possono quindi essere funzionalmente correlati tra loro
    * Attraverso il clustering, ad esempio, e' possibile trovare gruppi di geni
      che sono funzionalmente correlati tra di loro
* Di solito, gli algoritmi utilizzati su risultati degli esperimenti con
  microarrays sono quelli di tipo [gerarchico](gerarchico), siccome si vuole fissare a
  posteriori il numero di clusters
* Il clustering gerarchico viene anche utilizzato per fare la ricostruzione
  evolutiva delle specie

### Valutazione e Interpretazione dei Risultati
* Quando si fanno esperimenti di microarrays si rilevano le espressioni geniche
  di diversi geni
* Gli algoritmi di clustering applicati a risultati di esperimenti microarrays
  sono quindi dei *raggruppamenti di geni*.
* Come detto, molto spesso si utilizzano algoritmi di clustering gerarchico,
  cosi' che si possa "tagliare" il dendrogramma e ottenere cosi' un numero
  variabile di clusters a posteriori
* Dal momento che si possono utilizzare diversi algoritmi di clustering, nasce
  spontanea la domanda di come si possa effettivamente valutare la bonta' di una
  soluzione di clustering
* In generale, formulare una metrica di clustering necessita di fare delle
  assunzioni su cosa rappresentino i dati

> I Geni con dei patterns di espressione molto simili sono quelli che
partecipano allo stesso processo biologico

* Una direzione possibile, e' quindi quella di sfruttare un'ontologia dei geni
  per poter formulare una metrica di bonta'
* La *Gene Ontology* e' un'ontologia che correla la conoscenza dei geni,
  rappresentandolo in termini di

    * **Processo Biologico** a cui prende parte
    * **Funzione Molecolare** che rappresenta il gene (se codifica un enzima, una
      proteina, ecc..)
    * **Componente Cellulare** dove avviene il processo

* Un modo possibile di procedere utilizzando la **GO**, e' quello di etichettare
  ogni elemento dei clusters con la sua rappresentazione corrispondente
  nell'ontologia. Se la maggior parte degli elementi di un cluster prendondono
  parte allo stesso processo biologico, allora e' probabile che la soluzione di
  clustering sia buona
* In questo modo si ottiene una valutazione in termini di *coerenza*, ma non e'
  sufficiente per determinare la bonta' dei clusters
* Essenzialmente, si utilizza il *p-value* come metrica addizzionale
* Si puo' calcolare la probabilita' di avere $x$ di $n$ possibili geni che hanno
  la stessa annotazione nella Gene Ontology, sapendo inoltre che nel genoma, $M$
  di $N$ geni hanno quell'annotazione. Tale probabilita' e' modellata da una
  distribuzione ipergeometrica:
  $$
  p = \frac{\binom{M}{x}\binom{N-M}{n-x}}{\binom{N}{n}}
  $$
* In sostanza, abbiamo un cluster di $n$ geni, $x$ di questi hanno una
  determinata annotazione nella GO. Si va quindi a confrontare questo valore
  rispetto all'insieme totale dei geni nel genoma, e rispetto al numero totale
  dei geni nel genoma che hanno quella specifica annotazione
* Quindi, se il numero di geni che sono stati clusterizzati nello stesso cluster
  e' simile al numero di geni totale che hanno la stessa annotazione, allora $p$
  sara' molto alta
* E' possibile migliorare questa stima, considerando la probabilita' di avere
  almeno $x$ su $n$ annotazioni
  $$
  \text{p-val} = 1 - \sum_{i=0}^{x-1} \;
  \frac{\binom{M}{i}\binom{N-M}{n-i}}{\binom{N}{i}}
  $$
* Questa formula, ritorna la probabilita' che l'insieme di geni clusterizzati
  all'interno di un cluster, abbiano un particolare insieme di annotazioni
  rispetto a un insieme totale dei geni che hanno la stessa annotazione
* Questo valore ci permette di calcolare la significativita' dei geni
  clusterizzati rispetto alla *Gene Ontology*

## Co-Clustering
* Per giustificare lo studio del co-clustering, si consideri un esempio
* Supponiamo di essere nel caso in cui due geni abbiano inizialmente
  un'espressione molto simile. Dopo un determinato lasso di tempo, l'espressione
  degli stessi diventa molto diversa.
* Un algoritmo di clustering, clusterizzerebbe lo stesso questi geni nello
  stesso cluster, a causa della somiglianza dei valori iniziali
* Questo perche' gli algoritmi visti considerano solo i valori delle *righe* (e
  quindi dei geni)
* Gli algoritmi di co-clustering determinano invece i clusters per righe e
  colonne, anziche' solamente per righe
* L'idea e' quindi quella di fare clustering simultaneamente sia per righe che
  per colonne
* In questo senso, determinano un sottoinsieme di *geni* che condividono gli
  stessi profili di esperessione in un sottoinsieme di *condizioni* (nel caso
  visto, sono e' un lasso di tempo)
* ***Es.** G1, G2, G4 condividono gli stessi profili di espressione nell'esposizione
  nel lasso di tempo T2, T3*
* Questo tipo di clusters e' piu' preferito e gestibile dai biologi

### Algoritmo di Co-Clustering Generico
* Un algoritmo di co-custering famoso e' il *Generic Co-clustering Algorithm*
  (Banerjee et al. 2007)
* L'algoritmo ha i seguenti parametri e ouputs:
    * **Input**: Matrice $X$ di dati, $k$ numero di suddivisioni nelle righe,
      $l$ numero di suddivisioni nelle colonne
    * **Output**: Partizioni $C_{cols}$, $C_{rows}$
* Algoritmo:

```
Repeat until convergence (clusters don't change):
    Compute the approximation matrix w.r.t C_cols and C_rows
    Keep $C_{cols}$ fixed and find a better $C_{rows}$
    Keep $C_{rows}$ fixed and find a better $C_{cols}$
```

* Essenzialmente, consiste nel creare un cluster e provare a ottimizzarlo con un
  algoritmo di ottimizzazione sia sulle righe che sulle colonne
* Un buon algoritmo di co-clustering dovrebbe essere inoltre in grado di
  sfruttare della conoscenza sul dominio di applicazione. Facendo un esempio
  pratico:
    * Sappiamo che il gene `XYZ` e il gene `ABC` agiscono spesso insieme in
      condizioni cancerogene
    * L'algoritmo di co-clustering non deve inserire `XYZ` e `ABC` in due
      cluster differenti
    * Analogamente, se `XYZ` e `ABC` non agiiscono insieme, non devono essere
      clusterizzati nello stesso cluster
* Piu' formalmente possiamo definire questi vincoli come:
    * **Must-Link** ($c_{=}(i, j)$) e' un vincolo che indica due le due righe (o
      colonne) $i$ e $j$ devono essere clusterizzate insieme
    * **Cannot-Link** ($c_{\neq}(i, j)$) indica che le due righe (o colonne) $i$ e
      $j$ non devono essere clusterizzate insieme
* E' possibile integrare questi due vincoli, per cui il risultato dell'algoritmo
  finale sara' il seguente:
    * **Input**: Matrice $X$ di dati, $k$ numero di suddivisioni nelle righe,
      $l$ numero di suddivisioni nelle colonne, insieme di righe e colonne
      cannot link $C_r, C_c$, insieme di righe e colonne must link $M_r, M_c$
    * **Output**: Partizioni $C_{cols}$, $C_{rows}$
* Algoritmo:

```
Initialize C_cols and C_rows
Repeat until convergence (clusters don't change):
    Compute the approximation matrix w.r.t C_cols and C_rows
    Foreach column j do:
        If exists M in M_c such that j belongs to M then:
            Assign all columns in M to the closest column cluster such that no
            cannot-link is violated
        else
            Assign j to the closest column cluster such that no cannot-link is
            violated
    Define C_cols w.r.t the resulting assignment
    Foreach row i do:
        If exists M in M_r such that i belongs to M then:
            Assign all rows in M to the closest row cluster such that no
            cannot-link is violated
        else
            Assign i to the closest row cluster such that no cannot-link is
            violated
    Define C_rows w.r.t the resulting assignment
```

* Se invece utilizziamo un approccio di tipo di ottimizzazione, possiamo
  definire due funzioni obiettivo differenti:
    1. **Hartigan**'s Residue: $h_{ij} =  x_{ij} - \mathbb{E}(I, J)$ (scarto
       rispetto al centroide del cluster)
    2. **Cheng&Church**'s Residue: $h_{ij} = x_{ij} - \mathbb{E}(I) -
       \mathbb{E}(J) + \mathbb{E}(I,J)$
