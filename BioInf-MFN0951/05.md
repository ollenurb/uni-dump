# Tecniche di Clustering

* Un cluster e' un insieme omogeneo di oggetti che sono tutti vicini tra loro
  secondo una relazione di distanza
* Il clustering puo' essere sia supervisionato che non supervisionato. Di
  solito, le tecniche di clustering piu' utilizzate sono di tipo non
  supervisionato
* Un buon clustering e' caratterizzato dalle seguenti caratteristiche:
    - I dati all'interno di un singolo cluster sono molto simili tra loro
    - I dati tra clusters differenti sono molto dissimili tra loro
* Ovviamente la qualita' di un clustering e' determinata:
    * Dalla misura con cui si definisce il concetto di similarita' (*distanza*)
    * Dal modello utilizzato (algoritmo di inferenza del modello)
* Le tecniche principali di clustering possono essere classificate in 5 approcci
  differenti:
    - *Algoritmi di Partizionamento*: costruiscono diverse partizioni e le
      valutano secondo determinati criteri. Ripetono la procedura fin quando la
      soluzione di clustering non soddisfa i criteri
    - *Clustering Gerarchico*: ritornano in output una descrizione dei cluster
      secondo una relazione gerarchica
    - *Density-Based*: formano i clusters in base alla densita' degli oggetti
    - *Grid-Based*: si basano su una struttura a diversi livelli di granularita'.
      Essenzialmente trattano i dati in basi a livelli livelli di dettaglio
    - *Model-Based*: derivano dall'ambito statistico. Inferisce delle
      distribuzioni di probabilita' che descrivono i clusters

## Algoritmi di Partizionamento
> **Obiettivo**: Costruisci una partizione di un dataset $D$ di $n$ oggetti in
un insieme $k$ di clusters

* Dato $k$, trova una partizione di $k$ clusters che ottimizzino un dato
  criterio di Partizionamento
* Siccome e' un algoritmo di ottimizzazione, possiamo differenziarli in metodi
    - **Esaustivi**: ritornano l'ottimo globale, ma hanno una complessita' elevata
      solitamente
    - **Euristici**: utilizzano un qualche tipo di euristica per ritornare l'ottimo
      locale, ma senza aver la garanzia che si tratti di un ottimo globale
* Alcuni algoritmi famosi (di tipo euristico) sono *k-means* e la sua variante
  *k-medoids*
    * *k-means*: i clusters sono rappresentati mediante i centroidi del cluster
    * *k-medoids*: i clusters sono rappresentati mediante i medoidi del cluster
* Come gia' detto, il clustering puo' essere visto come un problema di
  ottimizzazione, in cui bisogna massimizzare una funzione obiettivo.
  Tale funzione e' la funzione *errore*, definita nel modo seguente
  $$
  E = \sum^k_{i=1} \sum_{p \in C_i} \lVert p-m_i \rVert^2
  $$
  dove:
    * $k$ e' il numero di clusters
    * $m_i$ e' il medoide del cluster $i-esimo$
    * $p$ e' un oggetto appartenente al cluster $C_i$
    * $C_i$ e' il cluster *i-esimo*
* Si vogliono trovare delle soluzioni di cluster tali per cui le distanze di
  tutti gli elementi rispetto ai loro centri sono minori possibili
* L'algoritmo *k-means* riesce a minimizzare questa funzione, trovando un ottimo
  locale, e lo fa mediante 4 passaggi:
    1. Partiziona gli oggetti in $k$ clusters non vuoti
    2. Calcola i centroidi dei $k$ clusters (il centroide e' l'oggetto che ha
       come valore degli attributi la media dei valori di ogni attributo)
    3. Assegna ad ogni oggetto al cluster corrispondente al centroide piu' vicino
    4. Se la soluzione di clustering non e' cambiata, termina, altrimenti ripeti
       dal passo 2.
* La complessita' di *k-means* e' pasi a $O(kn)$, per cui e' molto efficiente
* Il problema e' che e' possibile che non converga mai ad una soluzione, per cui
  e' opportuno prevedere un numero di iterazioni prefissato massimo
* Alcune limitazioni di *k-means* sono:
    * Difficile applicarlo quando le features non sono di tipo numerico
    * Bisogna per forza specificare in anticipo il numero di clusters $k$
    * Non e' in grado di gestire bene dati *rumorosi* e gli *outliers*
    * Non e' in grado di trovare soluzioni di clusters che hanno una forma *non
      convessa*
* Alcune varianti come *k-modes* servono ad ovviare ad alcuni di queste
  limitazioni. Esso difatti consiste nel calcolare la *moda* al posto della
  *media*.
* *k-prototype* utilizza invece dei metodi misti per calcolare i medoidi,
  applicando la moda o la media in base al fatto che le features siano
  categoriche oppure numeriche

## Algoritmi Gerarchici
* Un algoritmo di clustering gerarchico puo' funzionare in due modi:
    * Agglomerativo: si parte da $n$ clusters composti da un solo elemento e si
      fondono mano a mano fino ad ottenere la soluzione finale
    * Divisivo: si fa l'opposto, cioe' si parte da un solo cluster singolo e si
      va a suddividere iterativamente man mano
* La maggior parte degli algoritmi gerarchici sono di tipo agglomerativo,
  siccome sono di piu' facile implementazione
* La soluzione di questa tipologia di algorimi e' un albero gerarchico, chiamato
  *dendrogramma*, in cui i nodi sono connessi mediante (solitamente, ma non
  necessariamente) una relazione binaria di inclusione
* Alcuni di questi algoritmi sono `AGNES` (*Agglomerative Nesting*), `DIANA`
  (*Divisive Analysis*) e `CURE` (*Clustering Using REpresentatives*)
* Uno dei piu' interessanti e' proprio `CURE`, poiche' impiega dei meccanismi
  differenti per effettuare il Clustering
* Essenzialmente, fino ad ora, ogni algoritmo utilizzava un solo rappresentativo
  (es. medoide, centroide) per rappresentare il cluster.
* `CURE` utilizza invece una molteplicita' di rappresentativi per rappresentare
  i clusters

