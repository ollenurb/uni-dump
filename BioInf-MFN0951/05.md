# Tecniche di Clustering
* Un cluster e' un insieme omogeneo di oggetti che sono tutti vicini tra loro
  secondo una relazione di distanza
* Il clustering puo' essere sia supervisionato che non supervisionato. Di
  solito, le tecniche di clustering piu' utilizzate sono di tipo non
  supervisionato
* Un buon clustering e' caratterizzato dalle seguenti caratteristiche:
    - I dati all'interno di un singolo cluster sono molto simili tra loro
    - I dati tra clusters differenti sono molto dissimili tra loro
* Ovviamente la qualita' di un clustering e' determinata:
    * Dalla misura con cui si definisce il concetto di similarita' (*distanza*)
    * Dal modello utilizzato (algoritmo di inferenza del modello)
* Le tecniche principali di clustering possono essere classificate in 5 approcci
  differenti:
    - *Algoritmi di Partizionamento*: costruiscono diverse partizioni e le
      valutano secondo determinati criteri. Ripetono la procedura fin quando la
      soluzione di clustering non soddisfa i criteri
    - *Clustering Gerarchico*: ritornano in output una descrizione dei cluster
      secondo una relazione gerarchica
    - *Density-Based*: formano i clusters in base alla densita' degli oggetti
    - *Grid-Based*: si basano su una struttura a diversi livelli di granularita'.
      Essenzialmente trattano i dati in basi a livelli livelli di dettaglio
    - *Model-Based*: derivano dall'ambito statistico. Inferisce delle
      distribuzioni di probabilita' che descrivono i clusters

## Algoritmi di Partizionamento
> **Obiettivo**: Costruisci una partizione di un dataset $D$ di $n$ oggetti in
un insieme $k$ di clusters

* Dato $k$, trova una partizione di $k$ clusters che ottimizzino un dato
  criterio di Partizionamento
* Siccome e' un algoritmo di ottimizzazione, possiamo differenziarli in metodi
    - **Esaustivi**: ritornano l'ottimo globale, ma hanno una complessita' elevata
      solitamente
    - **Euristici**: utilizzano un qualche tipo di euristica per ritornare l'ottimo
      locale, ma senza aver la garanzia che si tratti di un ottimo globale
* Alcuni algoritmi famosi (di tipo euristico) sono *k-means* e la sua variante
  *k-medoids*
    * *k-means*: i clusters sono rappresentati mediante i centroidi del cluster
    * *k-medoids*: i clusters sono rappresentati mediante i medoidi del cluster
* Come gia' detto, il clustering puo' essere visto come un problema di
  ottimizzazione, in cui bisogna massimizzare una funzione obiettivo.
  Tale funzione e' la funzione *errore*, definita nel modo seguente
  $$
  E = \sum^k_{i=1} \sum_{p \in C_i} \lVert p-m_i \rVert^2
  $$
  dove:
    * $k$ e' il numero di clusters
    * $m_i$ e' il medoide del cluster $i-esimo$
    * $p$ e' un oggetto appartenente al cluster $C_i$
    * $C_i$ e' il cluster *i-esimo*
* Si vogliono trovare delle soluzioni di cluster tali per cui le distanze di
  tutti gli elementi rispetto ai loro centri sono minori possibili

### K-Means
* L'algoritmo *k-means* riesce a minimizzare questa funzione, trovando un ottimo
  locale, e lo fa mediante 4 passaggi:
    1. Partiziona gli oggetti in $k$ clusters non vuoti
    2. Calcola i centroidi dei $k$ clusters (il centroide e' l'oggetto che ha
       come valore degli attributi la media dei valori di ogni attributo)
    3. Assegna ad ogni oggetto al cluster corrispondente al centroide piu' vicino
    4. Se la soluzione di clustering non e' cambiata, termina, altrimenti ripeti
       dal passo 2.
* La complessita' di *k-means* e' pasi a $O(kn)$, per cui e' molto efficiente
* Il problema e' che e' possibile che non converga mai ad una soluzione, per cui
  e' opportuno prevedere un numero di iterazioni prefissato massimo
* Alcune limitazioni di *k-means* sono:
    * Difficile applicarlo quando le features non sono di tipo numerico
    * Bisogna per forza specificare in anticipo il numero di clusters $k$
    * Non e' in grado di gestire bene dati *rumorosi* e gli *outliers*
    * Non e' in grado di trovare soluzioni di clusters che hanno una forma *non
      convessa*
* Alcune varianti come *k-modes* servono ad ovviare ad alcuni di queste
  limitazioni. Esso difatti consiste nel calcolare la *moda* al posto della
  *media*.
* *k-prototype* utilizza invece dei metodi misti per calcolare i medoidi,
  applicando la moda o la media in base al fatto che le features siano
  categoriche oppure numeriche

## Algoritmi Gerarchici
* Un algoritmo di clustering gerarchico puo' funzionare in due modi:
    * Agglomerativo: si parte da $n$ clusters composti da un solo elemento e si
      fondono mano a mano fino ad ottenere la soluzione finale
    * Divisivo: si fa l'opposto, cioe' si parte da un solo cluster singolo e si
      va a suddividere iterativamente man mano
* La maggior parte degli algoritmi gerarchici sono di tipo agglomerativo,
  siccome sono di piu' facile implementazione
* La soluzione di questa tipologia di algorimi e' un albero gerarchico, chiamato
  *dendrogramma*, in cui i nodi sono connessi mediante (solitamente, ma non
  necessariamente) una relazione binaria di inclusione
* Alcuni di questi algoritmi sono `AGNES` (*Agglomerative Nesting*), `DIANA`
  (*Divisive Analysis*) e `CURE` (*Clustering Using REpresentatives*)

```
Hierarchical Clustering(d, n)
  form n clusters each with one element
  construct a graph T by assigning one vertex to each cluster
  while there is more than one cluster
    find two closest clusters C_1 and C_2
    merge C_1 and C_2 ito new cluster  C with |C_1| + |C_2| elements
    compute distance from C to all other clusters
    add a new vertex C to T and connect to vertic C_1 and C_2
    remove rows and columns of d corresponding to C_1 and C_2
    add a row and column to d corresponding to new cluster C
    return T
```

### CURE
* Uno dei piu' interessanti e' proprio `CURE`, poiche' impiega dei meccanismi
  differenti per effettuare il Clustering
* Essenzialmente, fino ad ora, ogni algoritmo utilizzava un solo rappresentativo
  (es. medoide, centroide) per rappresentare il cluster.
* `CURE` utilizza invece una molteplicita' di rappresentativi per rappresentare
  i clusters
* Difatti, il drawback dei metodi basati sulla distanza, sono buoni solamente in
  casi in cui i clusters hanno una forma convessa, una dimensione relativamente
  piccola e se $k$ puo' essere stimato in modo semplice
* L'algoritmo `CURE(s, p, q)` mira a sopperire a questi drawbacks. Il concetto
  dell'algoritmo e' il seguente:
    1. Estrai un cluster *"campione"* $s$ in modo casuale
    2. Partiziona il campione in $p$ partizioni con dimensione $s/p$
    3. Clusterizza parzialmente le partizioni in $p/pq$ clusters
    4. Elimina gli outliers
    5. Raggruppa i clusters intermedi piu' vicini in un solo cluster
* Si noti che con l'operazione di raggruppamento un singolo cluster avra' piu'
  rappresentativi e non solo uno. In questo modo, le istanze rappresentative
  seguono la forma del cluster qualunque essa sia.
* In realta' l'algoritmo applica poi ulteriomente un'operazione di *shrinking*
  verso un centro di gravita' di una certa frazione $\alpha$, su tutti i punti
  rappresentativi

## Clustering basato sulla densita'
* Sono metodi che utilizzano come criterio per il clustering la densita' di
  punti (tipicamente all'interno di un determinato raggio)
* Questi algoritmi hanno la capacita' di rappresentare clusters di qualsiasi
  forma e riescono anche a gestire bene il rumore nei dati.
* Diversi famosi algoritmi quali:
    - `DBSCAN`
    - `OPTICS`
    - `DENCLUE`
    - `CLIQUE`
* Tutti questi algoritmi necessitano di solito di specificare un *raggio* per
  determinarne la terminazione

### DBSCAN
* *(Density Based Spatian Clustering of Applications with Noise)*
* L'algoritmo utilizza due parametri principali:
    * $\epsilon$: determina il raggio
    * $MinPts$: determina il numero minimo di punti (densita') che devono stare
      all'interno del cerchio di raggio $\epsilon$
* *Density Reachable*: Se un punto $p$ fa parte di uno dei punti $MinPts$ che
  stanno all'interno del raggio di un punto $q$
* *Density Connected*: Se un punto $p$ e' connesso tramite un "percorso" di
  punti density reachable tra loro a un punto $q$
* L'algoritmo essenzialmente va a considerare punto per punto di tutto il
  dataset, andando a costruire man mano gli insiemi di densita'
* Una volta calcolati gli insiemi di densita', va a fondere tutti i punti che
  sono density connected in un singolo cluster
* **Punti di forza**:
    * Fa una sola scansione dei dati (complessita' lineare)
    * I cluster hanno dimensione e forma arbitraria
    * E' in grado di gestire gli outliers e i dati rumorosi
    * Non c'e' bisogno di assegnare un numero di cluster predefinito
* **Punti deboli**:
    * La qualita' di clustering dipende sull'ordine con cui si analizzano i dati
    * La qualita' di clustering dipende dalla definizione di vicinato che si da
      in input

## Algoritmi Grid-Based
* Partizionano lo spazio andando a determinare degli intervalli sui valori degli
  attributi

### CLIQUE
* (*Clustering In QUEst*) puo' essere considerato come una combinazione tra un
  algoritmo basato su densita' e uno basato sul concetto di griglia
* Utilizza un meccanismo per partizionare i **valori** degli attributi
  descrittivi, e non le istanze vere e proprie
    1. Partiziona lo spazio dei dati e trova il numero di punti che ricadono
       all'interno di ogni cella della partizione
    2. Identifica i sottospazi che contengono clusters utilizzando un pricipio
       *a-priori*:
         * Determina le unita' dense in tutti i sottospazi di interesse
         * Determina le unita' densamente connesse in tutti i sottospazi di interesse

* Guardare esempio sulle slides

* Punti di forza:
    * Determina automaticamente i sottospazi di piu' alta dimensionalita' tali
      per cui la densita' di punti e' alta
    * Non e' sensibile rispetto all'ordine con cui si considerano le istanze
    * Scala linearmente con la dimensione degli input e ha una buona
      scalabilita' all'aumentare il numero di dimensioni
* Punti deboli:
    * L'accuratezza della soluzione di clustering potrebbe degradare alle spese
      di un'implementazione del metodo piu' semplice

## Clustering basato su Modelli
* L'idea e' quella di avere un qualche modello di come si distribuiscono i dati
  all'inteno del cluster. Essenzialmente utilizzano dele distribuzioni di
  probabilita' per determinare i clusters
* `SOM` (*Self Organizing Feature Map*) che e' basato sulle reti neurali
* Basati su una funzione di densita' di probabilita' quali `COWEB`. Questo
  algoritmo assume pero' ch ele distribuzioni dei valori degli attributi
  sono *indipendenti* tra di loro
* Un'altro esempio probabilistico e' un metodo che si chiama *Gaussian Mixture
  Model*, che rappresenta in un certo senso la variante probabilistica di
  *k-means*.
    * Si rappresentano i dati come distribuzioni Gaussiane
    * Si stimano i parametri iniziali delle Gaussiane
    * Si assegnano i punti al cluster in base alla probabilita' piu' alta
    * Si vanno a ri-stimare i parametri delle Gaussiane utilizzando i punti di
      appartenenza ottenuti precedentemente

## Clique Clustering
* Un'altro modo per far clustering e' quello che si basa sulla rappresentazione
  di clusters con i *cliques*
* Un **clique graph** e' un grafo in cui ogni vertice e' connesso a qualsiasi altro
  vertice
* E' possibile trasformare la matrice di distanze in un grafo. Ogni riga e' un
  vertice del grafo, per cui ogni vertice viene connesso ad un altro se la loro
  distanza e' minore di un threshold definito
* Il grafo risultante puo' contenere dei *cliques*, per cui per il modo in cui
  si e' costruito il grafo, i cliques rappresentano clusters composti da punti
  vicini tra loro
* Trasformando il grafo delle distanze in un clique graph, si possono ottenere i
  diversi clusters dei dati. Per trasformarlo, si recidono o aggiungono
  connessioni al grafo. Il risultato possono essere piu' clique graphs.
* Il problema di trovare una soluzione chiusa al problema di trovare i clique
  graphs e' NP-Completo, per cui si utilizzano delle opportune euristiche per
  ricercare lo spazio delle soluzioni

### CAST
* (*Cluster Affinity Search Technique*) e' un algoritmo che si basa sul clique
  Clustering
* $d(i, C) = $ distanza media tra l'istanza $i$ e tutte le altre istanze
  appartenenti al cluster $C$
* L'istanza $i$ e' vicina al cluster $C$ se $d(i, C) < \theta$, dove $\theta$ e'
  il threshold
* Utilizzando questa nozione di distanza, l'algoritmo prende in input il grafo e
  va a iterare su di esso andando a suddividerlo in sottografi

```
Cast(S, G, Theta)
  P <- Empty Set
  while S is not empty
    V <- vertex of maximal degree in the distance graph G
    C <- {V}
    while a close istance i not int C or distant gene i in C exists
      Find the nearest close istance i not in C and add it to C
      remove the farthest distant istance i in C
    Add cluster C to partition P
    S <- S \ C
    Remove vertices of cluster C from the distance graph G
return P
```

## Clustering per dati Biologici
* Il clustering e' un task che viene applicato molto in ambito biologico
* Esso e' utile sopratutto per *visualizzare* e *analizzare* una quantita' di
  dati biologici enorme
* Un esempio tipico e' l'analisi della funzionalita' dei geni, cioe' il task che
  ha l'obiettivo di capire quale funzione ha un gene appena sequenzializzato.
  Tipicamente consiste nel confrontare i geni sconosciuti con geni la cui
  funzionalita' e' gia' nota
* Per il 40% dei geni sequenzializzati, pero', comparare le sequenze non e'
  sufficiente a determinarne la funzionalita'
* Gli sperimenti con microarrays servono appunto a inferire la funzionalita' di
  un gene, anche quando la similarita' non e' sufficiente a determinarne la
  funzione
    * I microarray misurano l'attivita' espressiva dei geni sotto diverse
      condizioni
    * Il livello di espressione e' *stimato* misurando la quantita' di mRNA che
      un certo gene produce (piu' il gene e' attivo, piu' trascrizione di quel
      gene ci sara')
* Negli esperimenti a microarray, l'output e' essenzialmente una matrice di dati
  in cui sono indicati i diversi livelli di expressivita' dei geni scelti per
  l'esperimento. Ogni riga e' un gene differente, e ogni colonna indica il tempo
  di esposizione. Il valore e' l'espressione genica per quel lasso di tempo.
    * Ogni osservazione e' un punto nello spazio $N$-dimensionale
    * Calcolando la distanza tra ogni punto, i punti piu' vicini tra loro
      possono rappresentare geni che hanno le stesse caratteristiche espressive
      e possono quindi essere funzionalmente correlati tra loro
    * Attraverso il clustering, ad esempio, e' possibile trovare gruppi di geni
      che sono funzionalmente correlati tra di loro
* Di solito, gli algoritmi utilizzati su risultati degli esperimenti con
  microarrays sono quelli di tipo [gerarchico](gerarchico), siccome si vuole fissare a
  posteriori il numero di clusters
* Il clustering gerarchico viene anche utilizzato per fare la ricostruzione
  evolutiva delle specie

