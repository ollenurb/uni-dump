{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Esercitazione 4b - New Language\n",
        "\n",
        "Studenti:\n",
        "\n",
        "- Brunello Matteo (mat. 858867)\n",
        "- Caresio Lorenzo (mat. 836021)\n",
        "\n",
        "*Consegna*: si richiede un'implementazione di un metodo per la generazione di una nuova lingua (che chiameremo NL). In particolare, partendo da una lingua di partenza L1 (ad es. la lingua Inglese), si prendano i termini di L1 usando un dizionario elettronico (ad es. WordNet o BabelNet). Per ogni termine $t$ ed i suoi sensi $S_t$, dovrete cercare un nuovo termine $tt$ (in una seconda lingua L2 a vostra scelta) da accoppiare a $t$, per la costruzione del termine $t{-}tt$, da inserire in NL. Il termine $tt$ in L2 va selezionato tra quelli meno ambigui per il concetto $S_t$ di riferimento. Si richiede di calcolare un valore di riduzione dell'ambiguità della nuova lingua rispetto a quella di partenza (ad es. calcolando il numero di sensi associabili ai termini $t{-}tt$ in NL rispetto a quelli associabili ai termini $t$ in L1. Una volta implementato il sistema, potrete cambiare la lingua L2 per valutare il potere \"disambiguante\" di diverse lingue rispetto a quella di partenza L1.\n"
      ],
      "metadata": {
        "id": "PcMTPmzLq5H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "import sys\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWd40DiJbxtZ",
        "outputId": "3cb9e175-3653-4f79-8263-9544017382c8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dizionario L1 e scelta L2"
      ],
      "metadata": {
        "id": "fplFQGPMNgvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Come risorsa linguistica di riferimento, piuttosto che BabelNet, divenuta una risorsa proprietaria, si è scelto di utilizzare Open Multilingual Wordnet (OMW), un progetto che raccoglie [decine di WordNet nazionali](http://globalwordnet.org/resources/wordnets-in-the-world/) sviluppate nel corso degli anni. OMW è integrato nella libreria NTLK di Python, risultando quindi una scelta particolarmente efficace in relazione allo sviluppo dell'implementazione.\n",
        "\n",
        "Come L1 si è scelto l'Italiano invece dell'Inglese per utilizzare una WordNet con una dimensione simile alle altre WN nazionali non-inglesi, mentre per L2 si è da subito pensato di utilizzare linguaggi che nel senso comune vengono riconosciuti come particolarmente *specifici* o *precisi*, come il Tedesco, il Latino o una lingua ugrofinnica come l'Ungherese. Le WordNet relative al primo e all'ultimo non sono però integrate in OMW, mentre non si considera il Latino un linguaggio con una copertura adatta al task attuale (verificato empiricamente esplorando *MultiWordNet*). Si è quindi scelto di utulizzare come L2 lo Spagnolo, con cui la verifica di correttezza dell'implementazione è immediata, e il Finlandese (si veda più avanti)."
      ],
      "metadata": {
        "id": "k7WAlStPNjP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l1_lang = 'ita'\n",
        "l2_lang = 'spa'\n",
        "# sorted(wordnet.langs()) # To list available languages"
      ],
      "metadata": {
        "id": "oUdggA3fSfHc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si definisce un insieme di termini a cui andare ad associare quelli della seconda lingua (come sottolineato a lezione, non si andrà a creare una vera e propria nuova lingua. ma piuttosto un nuovo vocabolario, con maggior potere di disambiguazione). Sarà anche utile memorizzare il numero di sensi totali associati a questo limitato vocabolario, così da poter valutare la performance delle scelte implementative."
      ],
      "metadata": {
        "id": "EyEGJN3oMwSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l1_dict = ['capo', 'terra', 'scuola', 'porta']\n",
        "\n",
        "l1_total_senses = sum(len(wordnet.synsets(l1_term, lang=l1_lang)) for l1_term in l1_dict)\n",
        "print(f\"{l1_total_senses} senses for the terms in L1.\")"
      ],
      "metadata": {
        "id": "OMO8X_I9Mj1v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac3aaa56-0188-4071-8ff5-24df5eb661b0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38 senses for the terms in L1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generazione NL"
      ],
      "metadata": {
        "id": "BCfmYuE48edE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per comodità si definisce un oggetto per rappresentare un termine in NL, rappresentazione minimale che può essere eventualmente arricchita."
      ],
      "metadata": {
        "id": "iBjgqu7nEQ-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NL_Term:\n",
        "  def __init__(self, first_term, second_term):\n",
        "    self.first_term = first_term\n",
        "    self.second_term = second_term\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"\\x1B[3m{self.first_term}-{self.second_term}\\x1B[0m\""
      ],
      "metadata": {
        "id": "7aPnXOyVU4WV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seguendo la consegna, per ogni senso $S_t$ si va ad associare al suo termine d'origine $t$ nel dizionario L1 il termine $tt$ *meno abiguo* corrispondente a $S_t$ in L2, ovvero il termine associato a $S_t$ con minor numero di sensi associati totali, andando così a creare un dizionario in NL composto dai termini compositi L1-L2 (se $t$ ha più $n$ sensi associati, si avrà un numero $n$ di $t-tt$ termini).\n",
        "\n",
        "Bisogna sempre sottolineare la distinzione tra *termine* e *sensi* associati a esso: si partirà infatti da ogni termine in L1, e per ogni senso associato a esso si estrarrano e collezioneranno i termini corrispondenti in L2 (il tutto viene ottenuto con un numero limitato di funzioni di NLTK relative a WN). Una volta ottenuti i termini in L2 si andrà a scegliere (per ogni senso) quello con minor numero di sensi associato. Il risultato saranno quindi tante coppie  *termine*-*termine*, e non *termine*-*senso*, per ogni senso $S_t$ (la cui verbalizzazione è disponibile in L2)."
      ],
      "metadata": {
        "id": "ZZKowWiC4Ug_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_less_ambigous_associated_terms(term, first_lang, second_lang):\n",
        "\n",
        "  second_lang_terms = {}\n",
        "  original_term_senses = wordnet.synsets(term, lang=first_lang)\n",
        "\n",
        "  # For each L1 term's sense collect the terms in L2\n",
        "  for original_sense in original_term_senses:\n",
        "    second_lang_terms[original_sense] = []\n",
        "  for original_sense in original_term_senses:\n",
        "    second_lang_terms[original_sense].extend(original_sense.lemma_names(second_lang))\n",
        "\n",
        "  # Trivial search for the least ambiguous term (min # of associated senses) for each sense\n",
        "  less_ambigous_terms = {}\n",
        "\n",
        "  for original_sense, terms in second_lang_terms.items():\n",
        "\n",
        "    less_ambigous_term_senses_number = sys.maxsize # Initialize with Int Max Value\n",
        "\n",
        "    for term in terms:\n",
        "\n",
        "      current_term_senses_number = len(wordnet.synsets(term, lang=second_lang))\n",
        "\n",
        "      if current_term_senses_number < less_ambigous_term_senses_number:\n",
        "        less_ambigous_terms[original_sense] = term\n",
        "        less_ambigous_term_senses_number = current_term_senses_number\n",
        "\n",
        "  return less_ambigous_terms"
      ],
      "metadata": {
        "id": "huqWDDMmaN7z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si va quindi a creare il nuovo dizionario NL contenente coppie costituite da termini in L1 e i termini associati in L2 *meno ambigui* per ogni senso dei termini in L1."
      ],
      "metadata": {
        "id": "ZcmXekUX6IFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute NL composite-term for each term in L1\n",
        "def generate_NL(first_lang_dict, first_lang, second_lang):\n",
        "  nl_dict = []\n",
        "\n",
        "  for term in first_lang_dict:\n",
        "    nl_terms = get_less_ambigous_associated_terms(term, first_lang, second_lang)\n",
        "    for nl_term in nl_terms.values():\n",
        "      nl_dict.append(NL_Term(term, nl_term))\n",
        "\n",
        "  return nl_dict"
      ],
      "metadata": {
        "id": "sdIa1gdsaY5o"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si genera NL per Italiano e Spagnolo, per poi compararla con NL generato per Italiano e Finlandese."
      ],
      "metadata": {
        "id": "1WzYspQv86wH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nl_dict = generate_NL(l1_dict, l1_lang, l2_lang)\n",
        "\n",
        "print(f\"NL computed over {l1_lang}-{l2_lang} ({len(nl_dict)} terms): \")\n",
        "\n",
        "for nl_term in nl_dict:\n",
        "  print(f\"  {nl_term} (L1 senses: {len(wordnet.synsets(nl_term.first_term, lang=l1_lang))} / L2 senses: {len(wordnet.synsets(nl_term.second_term, lang=l2_lang))})\")\n",
        "\n",
        "nl_l1_senses = sum(len(wordnet.synsets(nl_term.first_term, lang=l1_lang)) for nl_term in nl_dict)\n",
        "nl_l2_senses = sum(len(wordnet.synsets(nl_term.second_term, lang=l2_lang)) for nl_term in nl_dict)\n",
        "print(f\"\\nAmbiguity reduction: {1 - (nl_l2_senses / nl_l1_senses):0.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfZvVRG-8yxk",
        "outputId": "1310ebdd-72ab-420f-f6e1-6d070d471aae"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NL computed over ita-spa (27 terms): \n",
            "  \u001b[3mcapo-artículo\u001b[0m (L1 senses: 11 / L2 senses: 10)\n",
            "  \u001b[3mcapo-primordial\u001b[0m (L1 senses: 11 / L2 senses: 6)\n",
            "  \u001b[3mcapo-cabeza\u001b[0m (L1 senses: 11 / L2 senses: 11)\n",
            "  \u001b[3mcapo-cabeza\u001b[0m (L1 senses: 11 / L2 senses: 11)\n",
            "  \u001b[3mcapo-cabo\u001b[0m (L1 senses: 11 / L2 senses: 3)\n",
            "  \u001b[3mcapo-cabo\u001b[0m (L1 senses: 11 / L2 senses: 3)\n",
            "  \u001b[3mcapo-dirigente\u001b[0m (L1 senses: 11 / L2 senses: 3)\n",
            "  \u001b[3mcapo-jefe\u001b[0m (L1 senses: 11 / L2 senses: 6)\n",
            "  \u001b[3mcapo-responsable\u001b[0m (L1 senses: 11 / L2 senses: 5)\n",
            "  \u001b[3mterra-humanidad\u001b[0m (L1 senses: 15 / L2 senses: 3)\n",
            "  \u001b[3mterra-nación\u001b[0m (L1 senses: 15 / L2 senses: 4)\n",
            "  \u001b[3mterra-tierra\u001b[0m (L1 senses: 15 / L2 senses: 10)\n",
            "  \u001b[3mterra-región\u001b[0m (L1 senses: 15 / L2 senses: 4)\n",
            "  \u001b[3mterra-globo\u001b[0m (L1 senses: 15 / L2 senses: 4)\n",
            "  \u001b[3mterra-tierra_firme\u001b[0m (L1 senses: 15 / L2 senses: 1)\n",
            "  \u001b[3mterra-suelo\u001b[0m (L1 senses: 15 / L2 senses: 7)\n",
            "  \u001b[3mterra-finca\u001b[0m (L1 senses: 15 / L2 senses: 4)\n",
            "  \u001b[3mterra-tierra\u001b[0m (L1 senses: 15 / L2 senses: 10)\n",
            "  \u001b[3mterra-tierra\u001b[0m (L1 senses: 15 / L2 senses: 10)\n",
            "  \u001b[3mterra-suelo\u001b[0m (L1 senses: 15 / L2 senses: 7)\n",
            "  \u001b[3mscuola-colegio\u001b[0m (L1 senses: 6 / L2 senses: 3)\n",
            "  \u001b[3mscuola-escuela\u001b[0m (L1 senses: 6 / L2 senses: 4)\n",
            "  \u001b[3mscuola-colegio\u001b[0m (L1 senses: 6 / L2 senses: 3)\n",
            "  \u001b[3mporta-puerta\u001b[0m (L1 senses: 6 / L2 senses: 4)\n",
            "  \u001b[3mporta-portal\u001b[0m (L1 senses: 6 / L2 senses: 3)\n",
            "  \u001b[3mporta-área\u001b[0m (L1 senses: 6 / L2 senses: 9)\n",
            "  \u001b[3mporta-puerto\u001b[0m (L1 senses: 6 / L2 senses: 2)\n",
            "\n",
            "Ambiguity reduction: 0.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l2_lang = 'fin'\n",
        "\n",
        "nl_dict = generate_NL(l1_dict, l1_lang, l2_lang)\n",
        "\n",
        "print(f\"NL computed over {l1_lang}-{l2_lang} ({len(nl_dict)} terms): \")\n",
        "\n",
        "for nl_term in nl_dict:\n",
        "  print(f\"  {nl_term} (L1 senses: {len(wordnet.synsets(nl_term.first_term, lang=l1_lang))} / L2 senses: {len(wordnet.synsets(nl_term.second_term, lang=l2_lang))})\")\n",
        "\n",
        "nl_l1_senses = sum(len(wordnet.synsets(nl_term.first_term, lang=l1_lang)) for nl_term in nl_dict)\n",
        "nl_l2_senses = sum(len(wordnet.synsets(nl_term.second_term, lang=l2_lang)) for nl_term in nl_dict)\n",
        "print(f\"\\nAmbiguity reduction: {1 - (nl_l2_senses / nl_l1_senses):0.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YMHRDne-nqe",
        "outputId": "52371901-cb8c-4ab6-9d62-942cac0d6267"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NL computed over ita-fin (37 terms): \n",
            "  \u001b[3mcapo-kappale\u001b[0m (L1 senses: 11 / L2 senses: 10)\n",
            "  \u001b[3mcapo-pääasiallinen\u001b[0m (L1 senses: 11 / L2 senses: 2)\n",
            "  \u001b[3mcapo-pää\u001b[0m (L1 senses: 11 / L2 senses: 11)\n",
            "  \u001b[3mcapo-säie\u001b[0m (L1 senses: 11 / L2 senses: 5)\n",
            "  \u001b[3mcapo-nauha\u001b[0m (L1 senses: 11 / L2 senses: 11)\n",
            "  \u001b[3mcapo-kallo\u001b[0m (L1 senses: 11 / L2 senses: 4)\n",
            "  \u001b[3mcapo-pää\u001b[0m (L1 senses: 11 / L2 senses: 11)\n",
            "  \u001b[3mcapo-niemeke\u001b[0m (L1 senses: 11 / L2 senses: 2)\n",
            "  \u001b[3mcapo-päällikkö\u001b[0m (L1 senses: 11 / L2 senses: 6)\n",
            "  \u001b[3mcapo-jehu\u001b[0m (L1 senses: 11 / L2 senses: 1)\n",
            "  \u001b[3mcapo-päällikkö\u001b[0m (L1 senses: 11 / L2 senses: 6)\n",
            "  \u001b[3mterra-ihmiskunta\u001b[0m (L1 senses: 15 / L2 senses: 1)\n",
            "  \u001b[3mterra-maajohto\u001b[0m (L1 senses: 15 / L2 senses: 1)\n",
            "  \u001b[3mterra-valtio\u001b[0m (L1 senses: 15 / L2 senses: 3)\n",
            "  \u001b[3mterra-maa\u001b[0m (L1 senses: 15 / L2 senses: 17)\n",
            "  \u001b[3mterra-vyöhyke\u001b[0m (L1 senses: 15 / L2 senses: 6)\n",
            "  \u001b[3mterra-maapallo\u001b[0m (L1 senses: 15 / L2 senses: 4)\n",
            "  \u001b[3mterra-kuiva_maa\u001b[0m (L1 senses: 15 / L2 senses: 1)\n",
            "  \u001b[3mterra-kasvumaa\u001b[0m (L1 senses: 15 / L2 senses: 1)\n",
            "  \u001b[3mterra-Tellus\u001b[0m (L1 senses: 15 / L2 senses: 1)\n",
            "  \u001b[3mterra-maaomaisuus\u001b[0m (L1 senses: 15 / L2 senses: 1)\n",
            "  \u001b[3mterra-luonnonmineraaliväri\u001b[0m (L1 senses: 15 / L2 senses: 1)\n",
            "  \u001b[3mterra-multa\u001b[0m (L1 senses: 15 / L2 senses: 1)\n",
            "  \u001b[3mterra-maa\u001b[0m (L1 senses: 15 / L2 senses: 17)\n",
            "  \u001b[3mterra-irtomaa\u001b[0m (L1 senses: 15 / L2 senses: 1)\n",
            "  \u001b[3mscuola-koulurakennus\u001b[0m (L1 senses: 6 / L2 senses: 1)\n",
            "  \u001b[3mscuola-koulu\u001b[0m (L1 senses: 6 / L2 senses: 5)\n",
            "  \u001b[3mscuola-koulukunta\u001b[0m (L1 senses: 6 / L2 senses: 3)\n",
            "  \u001b[3mscuola-koulu\u001b[0m (L1 senses: 6 / L2 senses: 5)\n",
            "  \u001b[3mscuola-koulu\u001b[0m (L1 senses: 6 / L2 senses: 5)\n",
            "  \u001b[3mscuola-kouluaika\u001b[0m (L1 senses: 6 / L2 senses: 2)\n",
            "  \u001b[3mporta-ovi\u001b[0m (L1 senses: 6 / L2 senses: 1)\n",
            "  \u001b[3mporta-oviaukko\u001b[0m (L1 senses: 6 / L2 senses: 1)\n",
            "  \u001b[3mporta-maali\u001b[0m (L1 senses: 6 / L2 senses: 10)\n",
            "  \u001b[3mporta-liittymä\u001b[0m (L1 senses: 6 / L2 senses: 6)\n",
            "  \u001b[3mporta-pääsy\u001b[0m (L1 senses: 6 / L2 senses: 5)\n",
            "  \u001b[3mporta-sola\u001b[0m (L1 senses: 6 / L2 senses: 6)\n",
            "\n",
            "Ambiguity reduction: 0.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si può notare come lo Spagnolo copra solo $27$ dei $38$ sensi totali originali, mentre il Finlandese $37$. Questo può essere dato o da una più grande dimensione della WordNet finlandese rispetto a quella spagnola (un limite quindi della risorsa linguistica) o dalla mancata verbalizzazione di sensi che invece la posseggono in Italiano e in Finlandese. Questa minore o maggiore copertura è una componente che dovrà essere valutata durante la progettazione di uno *score* che descriva il *potere disambiguante* di una lingua L2 rispetto L1.\n",
        "\n",
        "Più interessante è invece la riduzione dell'ambiguità (il numero di sensi totali di L1 sul numero di sensi totali di L2): il Finlandese non copre solo più sensi dello Spagnolo, ma anche la sua riduzione dell'ambiguità ($57\\%$) è maggiore rispetto a quella della seconda lingua ($51\\%$). Come tutte le lingue uraliche (comprese quindi le ugrofinniche), il Finlandese è una lingua agglutinante, proprietà evidente dalle *varianti* di *koulu*, e proprio questa proprietà potrebbe essere dietro alla minor ambiguità intrinseca.\n",
        "\n",
        "È però necessario sviluppare uno *score* che evidenzi questa riduzione in maniera sistematica, per una coppia arbitraria di linguaggi."
      ],
      "metadata": {
        "id": "iR-k_hvk-HgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Potere disambiguante"
      ],
      "metadata": {
        "id": "FBMTgcPz-4fC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si introduce uno score per determinare il *potere disambiguante* di una lingua L2 rispetto un'altra L1 (dato il nuovo dizionario NL calcolato). A ogni coppia di linguaggi *L1-L2* (la NL candidata) si associa una tupla composta da:\n",
        "\n",
        "- Il numero di sensi originali in L1 che sono anche coperti in L2 (*higher is better*).\n",
        "- La riduzione dell'ambiguità, ovvero la quantità in percentuale dei sensi totali in L2 rispetto a quelli in L1  (*higher is better*).\n"
      ],
      "metadata": {
        "id": "Omf-t1dP-gxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_disambiguation_power(new_lang_dict, first_lang, second_lang):\n",
        "\n",
        "  disambiguation_power = (0, 0)\n",
        "\n",
        "  new_lang_first_senses = sum(len(wordnet.synsets(new_lang_term.first_term, lang=first_lang)) for new_lang_term in new_lang_dict)\n",
        "  new_lang_second_senses = sum(len(wordnet.synsets(new_lang_term.second_term, lang=second_lang)) for new_lang_term in new_lang_dict)\n",
        "\n",
        "  senses_coverage = len(new_lang_dict)\n",
        "  ambiguity_reduction = 1 - (new_lang_second_senses / new_lang_first_senses)\n",
        "\n",
        "  return (senses_coverage, ambiguity_reduction)"
      ],
      "metadata": {
        "id": "yfcUNX4e_Es2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l2_lang = 'spa'\n",
        "nl_dict = generate_NL(l1_dict, l1_lang, l2_lang)\n",
        "print(f\"{l1_lang}-{l2_lang}: {compute_disambiguation_power(nl_dict, l1_lang, l2_lang)}\")\n",
        "\n",
        "l2_lang = 'fin'\n",
        "nl_dict = generate_NL(l1_dict, l1_lang, l2_lang)\n",
        "print(f\"{l1_lang}-{l2_lang}: {compute_disambiguation_power(nl_dict, l1_lang, l2_lang)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHKFeQDMA0Oz",
        "outputId": "1e913478-e25d-4ac2-9269-2df7f3a8dc1d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ita-spa: (27, 0.5098039215686274)\n",
            "ita-fin: (37, 0.56575682382134)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "È quindo ora possibile calcolare il potere disambiguante rispetto a qualsiasi coppia di lingue *L1-L2*."
      ],
      "metadata": {
        "id": "CA9HRRhHBxIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l2_lang = 'jpn'\n",
        "nl_dict = generate_NL(l1_dict, l1_lang, l2_lang)\n",
        "print(f\"{l1_lang}-{l2_lang}: {compute_disambiguation_power(nl_dict, l1_lang, l2_lang)}\")\n",
        "\n",
        "l2_lang = 'heb'\n",
        "nl_dict = generate_NL(l1_dict, l1_lang, l2_lang)\n",
        "print(f\"{l1_lang}-{l2_lang}: {compute_disambiguation_power(nl_dict, l1_lang, l2_lang)}\")"
      ],
      "metadata": {
        "id": "kBZoGHxY1U_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e786430-2423-43d9-d953-1ecc59b78491"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ita-jpn: (33, 0.8638888888888889)\n",
            "ita-heb: (8, -6.813953488372093)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si può notare come il Giapponese copra meno sensi del Finlandese, ma lo faccia in maniera molto meno ambigua (maggiore riduzione dell'ambiguità). È necessario quindi permettere all'utente di esprimere una preferenza sulla copertura e sulla ambiguità volute (si veda dopo).\n",
        "\n",
        "L'Ebraico ha invece una copertura bassisima e una ambiguità addirittura maggiore di quella di partenza (valore negativo)."
      ],
      "metadata": {
        "id": "MBco5cpO2RNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linguaggio L2 più disambiguante"
      ],
      "metadata": {
        "id": "JNUx4MwuCpyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ciclando su tutti i linguaggi offerti da OMW è possibile determinare quale sia il linguaggio più disambiguante rispetto il dizionario di partenza."
      ],
      "metadata": {
        "id": "28yoIJyaCteM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_langs_with_highest_disambiguation_power(first_lang_dict, first_lang, coverage_cutoff, langs_number, coverage_preference):\n",
        "\n",
        "  disambiguation_powers = []\n",
        "\n",
        "  # Compute and store disambiguation powers while computing the highest one\n",
        "  for second_lang in wordnet.langs():\n",
        "    if second_lang not in l1_lang:\n",
        "      new_lang_dict = generate_NL(first_lang_dict, first_lang, second_lang)\n",
        "      disambiguation_power = compute_disambiguation_power(new_lang_dict, first_lang, second_lang)\n",
        "      disambiguation_powers.append((second_lang, disambiguation_power[0], disambiguation_power[1]))\n",
        "\n",
        "  # Filter disambiguation powers selecting only the ones above cutoff\n",
        "  langs_with_highest_disambiguation_power = [power for power in disambiguation_powers if power[1] > coverage_cutoff]\n",
        "\n",
        "  if coverage_preference:\n",
        "    # Sort the remaining disambiguation powers by senses coverage and then by ambiguity reduction\n",
        "    langs_with_highest_disambiguation_power.sort(key=lambda power: (power[1], power[2]), reverse=True)\n",
        "  else:\n",
        "    # Sort the remaining disambiguation powers by ambiguity reduction\n",
        "    langs_with_highest_disambiguation_power.sort(key=lambda power: power[2], reverse=True)\n",
        "\n",
        "  return langs_with_highest_disambiguation_power[:langs_number]"
      ],
      "metadata": {
        "id": "FsQP3XjqC4Tc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PARAMETERS\n",
        "cutoff = 6 # Allow sub-optimal original senses' coverage\n",
        "n_langs = 5 # Desired number of languages (1 for the best one only)\n",
        "prefer_coverage = False"
      ],
      "metadata": {
        "id": "Kxfs0JQp9oi_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "langs_with_powers = compute_langs_with_highest_disambiguation_power(l1_dict, l1_lang, l1_total_senses - cutoff, n_langs, prefer_coverage)\n",
        "\n",
        "langs = [lang[0] for lang in langs_with_powers]\n",
        "powers = [(lang[1], lang[2]) for lang in langs_with_powers]\n",
        "print(f\"{langs} with disambiguation powers {powers}\")\n",
        "print(f\"{len(langs)} out of {len(wordnet.langs())} languages available in NLTK WN\")"
      ],
      "metadata": {
        "id": "DmoVRQOaHFUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9200f506-3555-4952-e7e2-3c1c54701419"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['jpn', 'tha', 'fin', 'zsm', 'slv'] with disambiguation powers [(33, 0.8638888888888889), (35, 0.7900262467191601), (37, 0.56575682382134), (36, 0.5231958762886597), (34, 0.5027624309392265)]\n",
            "5 out of 32 languages available in NLTK WN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non si avrà quindi un unico linguaggio con più alto potere disambiguante, ma un numero parametrizzabile dipendente dalla preferenza sulla copertura e sulla riduzione d'ambiguità (per ottenere il singolo linguaggio più disambiguante è semplicemente necessario impostare `n_langs` a $1$).\n",
        "\n",
        "Bisogna però sottolineare come questo risultato ovviamente dipenda dal dizionario di partenza, come evidende di seguito."
      ],
      "metadata": {
        "id": "e6xPoG2ILHCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l1_dict = ['capo', 'terra', 'scuola', 'porta', 'spesa', 'combinazione', 'francese']\n",
        "\n",
        "l1_total_senses = sum(len(wordnet.synsets(l1_term, lang=l1_lang)) for l1_term in l1_dict)\n",
        "print(f\"{l1_total_senses} senses for the terms in L1.\")\n",
        "\n",
        "langs_with_powers = compute_langs_with_highest_disambiguation_power(l1_dict, l1_lang, l1_total_senses - cutoff, n_langs, prefer_coverage)\n",
        "\n",
        "langs = [lang[0] for lang in langs_with_powers]\n",
        "powers = [(lang[1], lang[2]) for lang in langs_with_powers]\n",
        "print(f\"{langs} with disambiguation powers {powers}\")\n",
        "print(f\"{len(langs)} out of {len(wordnet.langs())} languages available in NLTK WN\")"
      ],
      "metadata": {
        "id": "0eb5HIpQMGAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "499c2ae5-9c22-4c3c-c480-d7e9c7c7e575"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 senses for the terms in L1.\n",
            "['fin', 'slv', 'eng', 'ron'] with disambiguation powers [(49, 0.5404814004376368), (46, 0.4903846153846154), (49, 0.4485776805251641), (47, 0.3792325056433409)]\n",
            "4 out of 32 languages available in NLTK WN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l1_dict = ['salve', 'monte', 'salto', 'fonte']\n",
        "\n",
        "l1_total_senses = sum(len(wordnet.synsets(l1_term, lang=l1_lang)) for l1_term in l1_dict)\n",
        "print(f\"{l1_total_senses} senses for the terms in L1.\")\n",
        "\n",
        "langs_with_powers = compute_langs_with_highest_disambiguation_power(l1_dict, l1_lang, l1_total_senses - cutoff, n_langs, prefer_coverage)\n",
        "\n",
        "langs = [lang[0] for lang in langs_with_powers]\n",
        "powers = [(lang[1], lang[2]) for lang in langs_with_powers]\n",
        "print(f\"{langs} with disambiguation powers {powers}\")\n",
        "print(f\"{len(langs)} out of {len(wordnet.langs())} languages available in NLTK WN\")"
      ],
      "metadata": {
        "id": "9-ElXJX3N3fB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b6287e-5e37-4fa8-c79d-3a5d21f45a1e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9 senses for the terms in L1.\n",
            "['nld', 'tha', 'arb', 'jpn', 'als'] with disambiguation powers [(5, 0.7272727272727273), (8, 0.7142857142857143), (5, 0.5882352941176471), (9, 0.5365853658536586), (5, 0.5294117647058824)]\n",
            "5 out of 32 languages available in NLTK WN\n"
          ]
        }
      ]
    }
  ]
}