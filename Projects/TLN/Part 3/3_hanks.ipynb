{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW9rwpETw2QT"
      },
      "source": [
        "# Esercitazione 3 - Teoria delle valenze di Hanks\n",
        "\n",
        "Studenti:\n",
        "\n",
        "- Brunello Matteo (mat. 858867)\n",
        "- Caresio Lorenzo (mat. 836021)\n",
        "\n",
        "*Consegna*: si richiede un'implementazione della teoria sulle valenze di Patrick Hanks. In particolare, partendo da un corpus a scelta e uno specifico verbo (tendenzialmente non troppo frequente e/o generico ma nemmeno raro), l'idea è di costruire dei possibili cluster semantici, con relativa frequenza. Ad es. dato il verbo \"to see\" con valenza = 2, e usando un parser sintattico (ad es. Spacy), si possono collezionare eventuali fillers per i ruoli di *subj* e *obj* del verbo, per poi convertirli in semantic types. Un cluster frequente su \"*to see*\" potrebbe unire *subj = noun.person* con *obj = noun.artifact*. Si richiede di partire da un corpus di almeno alcune centinaia di istanze del verbo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9afcTf0wlPD",
        "outputId": "c0f1d528-7cbe-4cdc-a897-3a7580df05e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import brown, reuters, wordnet, gutenberg\n",
        "from nltk.wsd import lesk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Lemmatization and Word Senses\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Corpora\n",
        "nltk.download('brown')\n",
        "nltk.download('reuters')\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "# Parser\n",
        "import spacy\n",
        "spcy = spacy.load(\"en_core_web_sm\") # For accuracy over efficiency use en_core_web_trf\n",
        "\n",
        "# WSD\n",
        "%pip install -q pywsd\n",
        "from pywsd.lesk import adapted_lesk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb_cNYkjGSEn"
      },
      "source": [
        "### Verbo utilizzato"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noYIW5wK4rS-"
      },
      "source": [
        "Per lo svolgimento e l'analisi dell'implementazione si è deciso di utilizzare il verbo *to cut* con valenza pari a 2 (tranisitivo), abbastanza generico ma il cui utilizzo può essere sia concreto sia metaforico. Si permette di parametrizzare sia il verbo che la sua valenza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "DTyDgRYI2nxF"
      },
      "outputs": [],
      "source": [
        " # PARAMETERS\n",
        "verb = lemmatizer.lemmatize('cut')\n",
        "valence = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAfqFWEGlxuM"
      },
      "source": [
        "### Raccolta e filtraggio del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYW-BsKmhLn5"
      },
      "source": [
        "Come dataset si è deciso di unire tre corpora, il Brown, il Reuters e quello del Progetto Gutenberg (già integrati in NLTK), così da ottenere un dataset iniziale più ampio e più variegato (così da idenitificare più *significati* finali e più variegati)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKG-LB8sC9ox",
        "outputId": "862d0024-34ae-4edf-d5d5-8c71b9c59214"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "210608"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "corpus_sents = []\n",
        "\n",
        "for sentence in brown.sents():\n",
        "  corpus_sents.append(sentence)\n",
        "for sentence in reuters.sents():\n",
        "  corpus_sents.append(sentence)\n",
        "for sentence in gutenberg.sents():\n",
        "  corpus_sents.append(sentence)\n",
        "\n",
        "len(corpus_sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ8Jf3fjhqDw"
      },
      "source": [
        "Dalle duecentodiecimila frasi ottenute si filtrano quelle che contengono (in versione lemmatizata) il termine *cut*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "03K2OPu2TPLJ"
      },
      "outputs": [],
      "source": [
        "def filter_sentences(verb, sentences):\n",
        "  filtered_sentences = []\n",
        "\n",
        "  for sentence in sentences:\n",
        "    if verb in [lemmatizer.lemmatize(word) for word in sentence]:\n",
        "      filtered_sentences.append(sentence)\n",
        "\n",
        "  return filtered_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb4NDT3dbqAC",
        "outputId": "f79ae4a8-6da2-4a5d-aa25-86e72d897037"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1669"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "filtered_sents = filter_sentences(verb, corpus_sents)\n",
        "len(filtered_sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w52JpTX1c9p"
      },
      "source": [
        "Già in questa fase si sarebbe potuto eseguire il parsing delle frasi del dataset attraverso Spacy, selezionando quindi le frasi in cui il termine ricercato agisca effettivamente da verbo. Questo avrebbe però richiesto di eseguire il parsing completo delle oltre duecentomila frasi contenute nel dataset, un'operazione particolarmente esosa.\n",
        "\n",
        "Si è invece scelto di andare a lemmatizzare le frasi del corpus e a verificare che queste contenessero il lemma del verbo desiderato. Si è passato quindi da una richiesta temporale di svariati minuti a una di circa $20$ secondi, passando inoltre da duecentodiecimila frasi a $1669$ per il verbo *to cut*. Si trattano però di frasi in cui il lemma del verbo compare, non è però che detto che questi svolgano effettivamente il ruolo di verbo (potrebbero infatti, per esempio, svolgere il ruolo di sostantivi o verbi sostantivati).\n",
        "\n",
        "È quindi comunque necessario fare un parsing delle frasi filtrate, ma come anticipato lo si farà nell'ordine del migliaio e non nelle centinaia di migliaia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "YQAntpr-4Bqq"
      },
      "outputs": [],
      "source": [
        "# Function needed to stip pointless whitespaces around punctuation\n",
        "def get_processed_sent(list_of_words):\n",
        "  sentence = \" \".join(list_of_words)\n",
        "  sequence_to_replace = [' / ', ' . ', ' .', ' , ', ' - ', ' \\' ', '( ', ' )', ' :']\n",
        "  for seq in sequence_to_replace:\n",
        "    sentence = sentence.replace(seq, seq.strip())\n",
        "  return sentence\n",
        "\n",
        "def parse_and_filter_sents(verb, sentences):\n",
        "  filtered_sentences = []\n",
        "\n",
        "  for sentence in sentences:\n",
        "    parsed_sentence = spcy(get_processed_sent(sentence)) # Use Spacy for parsing and lemmatizing\n",
        "    if (verb, 'VERB') in [(w.lemma_, w.pos_) for w in parsed_sentence]:\n",
        "      filtered_sentences.append(parsed_sentence)\n",
        "\n",
        "  return filtered_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V08Sk6x26M3K",
        "outputId": "4aebe941-7bbc-4c60-8ca1-5c6cd2085b30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1224"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "parsed_sents = parse_and_filter_sents(verb, filtered_sents)\n",
        "len(parsed_sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9SYJ5x5mQZr"
      },
      "source": [
        "Le frasi in cui *cut* compare effettivamente come verbo sono quindi $1224$, il $73\\%$ delle 1669 filtrate (dalle duecentodiecimila iniziali)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_YBeyBQGWKw"
      },
      "source": [
        "### Estrazione degli argomenti del verbo\n",
        "\n",
        "In base alla valenza, si va a estrarre gli argomenti del verbo *cut*, da cui verranno poi estratti i tipi semantici.\n",
        "\n",
        "Si sfrutta il parsing a dipendenze di Spacy per discriminare sulla funzione sintattica dei vari termini (il loro ruolo come *soggetto* o *oggetto*) *figli* del verbo, andando a discriminare sul tipo della dipendenza (vari tipo di *soggetto*, *oggetto diretto* e *dativo* per i verbi transitivi, *oggetto indiretto* o *oggetto passivo* per i verbi ditransitivi)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "tvW-qMLyOWFe"
      },
      "outputs": [],
      "source": [
        "def get_verb_arguments(verb, sentences):\n",
        "\n",
        "  verb_arguments = []\n",
        "\n",
        "  for sent in sentences:\n",
        "    for word in sent:\n",
        "      if word.lemma_ == verb and word.pos_ == 'VERB':\n",
        "\n",
        "        subjs = [child for child in word.children if 'subj' in child.dep_]\n",
        "\n",
        "        if valence > 1:\n",
        "          objs = [child for child in word.children if child.dep_ == 'dobj' or child.dep_ == 'dative']\n",
        "        if valence == 3: # Add indirect objects to objs list (to cover ditransitive verb)\n",
        "          objs.extend([child for child in word.children if child.dep_ == 'iobj' or child.dep_ == 'pobj'])\n",
        "\n",
        "        # Collect the found arguments according to the valence\n",
        "        if valence == 1 and len(subjs) > 0:\n",
        "          for subj in subjs:\n",
        "              verb_arguments.append((sent, subj))\n",
        "        elif valence > 1 and len(subjs) > 0 and len(objs) > 0:\n",
        "          for subj in subjs:\n",
        "            for obj in objs:\n",
        "              verb_arguments.append((sent, subj, obj))\n",
        "\n",
        "  return verb_arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp6ento1bovB",
        "outputId": "d17c3493-5ff8-4693-9461-ce5b08c72037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "419 arguments derived from 390 sentences.\n"
          ]
        }
      ],
      "source": [
        "arguments = get_verb_arguments(verb, parsed_sents)\n",
        "print(f\"{len(arguments)} arguments derived from {len(set([argument[0] for argument in arguments]))} sentences.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL7GFoDQvALt"
      },
      "source": [
        "Il numero di frasi in cui *cut* compare come verbo transitivo è quindi $390$, il $23\\%$ delle 1669 filtrate (dalle duecentodiecimila iniziali). È ora possibile utilizzare questi argomenti in un'applicazione della Teoria di Hanks andandone a derivare i *semantic types*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA30AHq709Wn"
      },
      "source": [
        "### Teoria di Hanks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm0tAAdswyKn"
      },
      "source": [
        "Come *semantic types* si è scelto di utilizzare i *lexnames* dei synset (come suggerito d'altronde nella consegna), i nomi dei file lessicografici in cui sono raccolti i synset. I *lexnames* relativi ai sostantivi sono $25$ (consultabili [qui](https://wordnet.princeton.edu/documentation/lexnames5wn)).\n",
        "\n",
        "Dapprima si era adottata un'implementazione minimale basata sull'utilizzo del *Simple Lesk* fornito da NLTK, dopo aver constatato delle prestazioni sub-ottimali si è deciso prima di utilizzare una libreria apposita per la WSD (`pywsd`) con versioni di Lesk più avanzate, e poi di gestire manualmente i pronomi la cui risoluzione è banale (*i*, *he*, *she*, *we*). Non si gestiscono invece i casi restanti (si veda più avanti), molto più ambigui.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "gBHPkDped82N"
      },
      "outputs": [],
      "source": [
        "def get_semantic_type(word, sentence):\n",
        "  # Explicit handling of easily-resolvable pronouns\n",
        "  if word.pos_ == 'PRON' and word.lemma_.lower() in ['i', 'he', 'she']: return 'noun.person'\n",
        "  elif word.pos_ == 'PRON' and word.lemma_.lower() == 'we': return 'noun.group'\n",
        "  else:\n",
        "      synset = adapted_lesk(str(sentence), str(word), 'n') # Use Adapted Lex to do WSD\n",
        "      if synset: return synset.lexname() # If a synset is found, use its lexname as Semantic Type\n",
        "  return None\n",
        "\n",
        "def get_semantic_types(arguments):\n",
        "  semantic_types = []\n",
        "\n",
        "  for argument in arguments: # argument = (sent, subj) if valence == 1, (sent, subj, obj) if valence > 1\n",
        "\n",
        "    subj_st = get_semantic_type(argument[1], argument[0])\n",
        "    if valence > 1: obj_st = get_semantic_type(argument[2], argument[0])\n",
        "\n",
        "    # Add semantic types only if they're actually found\n",
        "    if valence == 1 and subj_st: semantic_types.append([argument[1], subj_st])\n",
        "    elif valence > 1 and subj_st and obj_st: semantic_types.append([argument[1], subj_st, argument[2], obj_st])\n",
        "\n",
        "  return semantic_types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "rkaHq1KbfCFo"
      },
      "outputs": [],
      "source": [
        "computed_semantic_types = get_semantic_types(arguments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhQMx_FQB8lI"
      },
      "source": [
        "In base alla valenza, per ogni semantic type o coppia di semantic types si raccolgono i fillers (lemmatizzati) da visualizzare successivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "SMU01Rdfe1O4"
      },
      "outputs": [],
      "source": [
        "# Compute a list of fillers for each semantic types (valence = 1) or for each semantic types combination (valence > 1)\n",
        "subjs_dict = {}\n",
        "if valence > 1: objs_dict= {}\n",
        "\n",
        "if valence == 1: # Collect fillers for each subj semantic type\n",
        "  subjs_dict = dict((subj, []) for subj in [semantic_types[1] for semantic_types in computed_semantic_types])\n",
        "  for semtantic_types in computed_semantic_types:\n",
        "    subjs_dict[semtantic_types[1]].append(semtantic_types[0].lemma_.lower())\n",
        "\n",
        "elif valence > 1: # Collect fillers for each subj-obj semantic types combination\n",
        "  for (subj_st, obj_st) in [(couple[1], couple[3]) for couple in computed_semantic_types]:\n",
        "    subjs_dict[(subj_st, obj_st)]= []\n",
        "    objs_dict[(subj_st, obj_st)] = []\n",
        "  for semtantic_types in computed_semantic_types:\n",
        "    subjs_dict[(semtantic_types[1], semtantic_types[3])].append(semtantic_types[0].lemma_.lower())\n",
        "    objs_dict[(semtantic_types[1], semtantic_types[3])].append(semtantic_types[2].lemma_.lower())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9BMOFxj-GQR"
      },
      "source": [
        "Infine si va quindi a contare le frequenze delle varie combinazioni di tipi semantici (i *significati*) e li si ordina per frequenza (visualizzando anche i filler lemmatizzati che in essi hanno rappresentato gli argomenti del verbo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENwDwKnGidb_",
        "outputId": "1c9ca1a8-6f1a-45a9-84d1-0f0f82438cfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified meanings sorted by frequency (105 meanings):\n",
            " noun.cognition [cut] noun.possession (30 times)\n",
            "   Subjects: {'it'}\n",
            "   Objects: {'cost', 'expense', 'stake', 'tariff', 'price', 'dividend', 'capital', 'surcharge', 'rate', 'deficit'}\n",
            " noun.person [cut] noun.person (18 times)\n",
            "   Subjects: {'father', 'stranger', 'jezebel', 'mayer', 'she', 'he', 'i'}\n",
            "   Objects: {'man', 'chicken', 'prophet', 'tree', 'inhabitant', 'nobles', 'judge', 'doer', 'price', 'she', 'he', 'i', 'spirit'}\n",
            " noun.person [cut] noun.artifact (14 times)\n",
            "   Subjects: {'rebel', 'he', 'i', 'she'}\n",
            "   Objects: {'bond', 'horse', 'shoe', 'engine', 'cord', 'sprig', 'chariot', 'skirt', 'image', 'ram', 'line'}\n",
            " noun.person [cut] noun.cognition (13 times)\n",
            "   Subjects: {'he', 'carpenter', 'i'}\n",
            "   Objects: {'it', 'witchcraft', 'memory', 'one'}\n",
            " noun.person [cut] noun.group (9 times)\n",
            "   Subjects: {'wife', 'economists', 'lord', 'she', 'he', 'i'}\n",
            "   Objects: {'pair', 'thicket', 'institute', 'city', 'nation', 'country', 'multitude'}\n",
            " noun.artifact [cut] noun.relation (9 times)\n",
            "   Subjects: {'bank'}\n",
            "   Objects: {'rate'}\n",
            " noun.act [cut] noun.artifact (8 times)\n",
            "   Subjects: {'suspension', 'strike', 'damage', 'raid', 'legislation'}\n",
            "   Objects: {'third', 'tape', 'pipeline', 'production', 'export'}\n",
            " noun.group [cut] noun.possession (8 times)\n",
            "   Subjects: {'bank', 'company', 'government', 'usda', 'states', 'we'}\n",
            "   Objects: {'expense', 'tax', 'way', 'price', 'rate', 'deficit'}\n",
            " noun.cognition [cut] noun.relation (8 times)\n",
            "   Subjects: {'it'}\n",
            "   Objects: {'rate'}\n",
            " noun.person [cut] noun.attribute (7 times)\n",
            "   Subjects: {'he', 'hezekiah', 'i', 'she'}\n",
            "   Objects: {'pompousness', 'figure', 'sparkle', 'gold'}\n",
            " noun.person [cut] noun.act (6 times)\n",
            "   Subjects: {'he', 'baker', 'farmer', 'i'}\n",
            "   Objects: {'pride', 'planting', 'dash', 'trip', 'stick', 'project'}\n",
            " noun.group [cut] noun.artifact (6 times)\n",
            "   Subjects: {'who', 'resistance', 'man', 'han', 'we'}\n",
            "   Objects: {'house', 'wire', 'wood', 'foil', 'production', 'lifeboat'}\n",
            " noun.group [cut] noun.relation (6 times)\n",
            "   Subjects: {'bank', 'bundesbank', 'country'}\n",
            "   Objects: {'rate'}\n",
            " noun.cognition [cut] noun.act (6 times)\n",
            "   Subjects: {'it', 'government'}\n",
            "   Objects: {'position', 'spending', 'capacity', 'investment', 'production'}\n",
            " noun.location [cut] noun.possession (6 times)\n",
            "   Subjects: {'korea', 'u.s.', 'india', 'bonn', 'taiwan'}\n",
            "   Objects: {'tariff', 'duty', 'taxis', 'deficit'}\n",
            " noun.person [cut] noun.food (5 times)\n",
            "   Subjects: {'he', 'i'}\n",
            "   Objects: {'ration', 'wing', 'neck', 'bread', 'round'}\n",
            " noun.person [cut] noun.possession (5 times)\n",
            "   Subjects: {'bank', 'builder', 'washington', 'he'}\n",
            "   Objects: {'deficit', 'line', 'part', 'cost'}\n",
            " noun.cognition [cut] noun.artifact (5 times)\n",
            "   Subjects: {'it', 'argument', 'problem'}\n",
            "   Objects: {'export', 'stake', 'ice'}\n",
            " noun.artifact [cut] noun.possession (4 times)\n",
            "   Subjects: {'bank', 'board', 'japan'}\n",
            "   Objects: {'rate', 'price', 'dividend'}\n",
            " noun.group [cut] noun.cognition (4 times)\n",
            "   Subjects: {'bundesbank', 'senate', 'we'}\n",
            "   Objects: {'it', 'amount', 'tactic'}\n",
            " noun.artifact [cut] noun.time (4 times)\n",
            "   Subjects: {'bank'}\n",
            "   Objects: {'rate'}\n",
            " noun.act [cut] noun.communication (4 times)\n",
            "   Subjects: {'organisation', 'commission', 'method'}\n",
            "   Objects: {'forecast', 'number'}\n",
            " noun.location [cut] noun.attribute (3 times)\n",
            "   Subjects: {'korea', 'italy', 'japan'}\n",
            "   Objects: {'inflation', 'surplus'}\n",
            " noun.cognition [cut] noun.communication (3 times)\n",
            "   Subjects: {'it'}\n",
            "   Objects: {'forecast', 'debt', 'number'}\n",
            " noun.location [cut] noun.relation (3 times)\n",
            "   Subjects: {'belgium', 'germany'}\n",
            "   Objects: {'rate'}\n",
            " noun.person [cut] noun.body (3 times)\n",
            "   Subjects: {'i', 'she'}\n",
            "   Objects: {'arm', 'hole'}\n",
            " noun.person [cut] noun.location (3 times)\n",
            "   Subjects: {'he', 'i'}\n",
            "   Objects: {'israel', 'male', 'man'}\n",
            " noun.person [cut] noun.communication (3 times)\n",
            "   Subjects: {'lord', 'i'}\n",
            "   Objects: {'lip', 'name', 'staff'}\n",
            " noun.person [cut] noun.plant (3 times)\n",
            "   Subjects: {'girl', 'peter', 'i'}\n",
            "   Objects: {'cedar', 'ear', 'beanstalk'}\n",
            " noun.act [cut] noun.possession (2 times)\n",
            "   Subjects: {'competition', 'law'}\n",
            "   Objects: {'fund', 'share'}\n",
            " noun.person [cut] noun.quantity (2 times)\n",
            "   Subjects: {'he', 'i'}\n",
            "   Objects: {'mile', 'enough'}\n",
            " noun.artifact [cut] noun.act (2 times)\n",
            "   Subjects: {'equipment', 'basement'}\n",
            "   Objects: {'radiation', 'drive'}\n",
            " noun.possession [cut] noun.relation (2 times)\n",
            "   Subjects: {'rate', 'revenue'}\n",
            "   Objects: {'rate', 'pct'}\n",
            " noun.act [cut] noun.substance (2 times)\n",
            "   Subjects: {'industry', 'congress'}\n",
            "   Objects: {'fat', 'timber'}\n",
            " noun.cognition [cut] noun.group (2 times)\n",
            "   Subjects: {'it'}\n",
            "   Objects: {'workforce', 'line'}\n",
            " noun.relation [cut] noun.process (2 times)\n",
            "   Subjects: {'rate'}\n",
            "   Objects: {'source'}\n",
            " noun.cognition [cut] noun.quantity (2 times)\n",
            "   Subjects: {'it'}\n",
            "   Objects: {'one', 'output'}\n",
            " noun.artifact [cut] noun.attribute (2 times)\n",
            "   Subjects: {'export', 'japan'}\n",
            "   Objects: {'surplus'}\n",
            " noun.artifact [cut] noun.communication (2 times)\n",
            "   Subjects: {'mine', 'japan'}\n",
            "   Objects: {'production', 'sale'}\n",
            " noun.location [cut] noun.communication (2 times)\n",
            "   Subjects: {'malaysia'}\n",
            "   Objects: {'output'}\n",
            " noun.artifact [cut] noun.artifact (2 times)\n",
            "   Subjects: {'knife', 'japan'}\n",
            "   Objects: {'output', 'nest'}\n",
            " noun.state [cut] noun.artifact (2 times)\n",
            "   Subjects: {'earthquake'}\n",
            "   Objects: {'pipeline'}\n",
            " noun.state [cut] noun.act (2 times)\n",
            "   Subjects: {'nec', 'company'}\n",
            "   Objects: {'production'}\n",
            " noun.substance [cut] noun.relation (2 times)\n",
            "   Subjects: {'ag'}\n",
            "   Objects: {'rate', 'pct'}\n",
            " noun.cognition [cut] noun.person (2 times)\n",
            "   Subjects: {'it'}\n",
            "   Objects: {'he', 'youth'}\n",
            " noun.group [cut] noun.plant (2 times)\n",
            "   Subjects: {'who', 'people'}\n",
            "   Objects: {'mallow', 'bough'}\n",
            " noun.person [cut] noun.time (2 times)\n",
            "   Subjects: {'he', 'i'}\n",
            "   Objects: {'life', 'yeare'}\n",
            " noun.group [cut] noun.person (2 times)\n",
            "   Subjects: {'we'}\n",
            "   Objects: {'he'}\n",
            " noun.person [cut] noun.animal (2 times)\n",
            "   Subjects: {'lord', 'i'}\n",
            "   Objects: {'man', 'prey'}\n",
            " adj.all [cut] noun.phenomenon (2 times)\n",
            "   Subjects: {'other'}\n",
            "   Objects: {'branch'}\n",
            " noun.animal [cut] noun.artifact (2 times)\n",
            "   Subjects: {'soldier', 'beard'}\n",
            "   Objects: {'square', 'rope'}\n",
            " noun.person [cut] noun.state (2 times)\n",
            "   Subjects: {'butler', 'i'}\n",
            "   Objects: {'impatience', 'occasion'}\n",
            " noun.person [cut] noun.object (1 times)\n",
            "   Subjects: {'he'}\n",
            "   Objects: {'rent'}\n",
            " noun.cognition [cut] noun.substance (1 times)\n",
            "   Subjects: {'it'}\n",
            "   Objects: {'squirrel'}\n",
            " noun.act [cut] noun.Tops (1 times)\n",
            "   Subjects: {'attention'}\n",
            "   Objects: {'unit'}\n",
            " noun.attribute [cut] noun.artifact (1 times)\n",
            "   Subjects: {'change'}\n",
            "   Objects: {'production'}\n",
            " noun.group [cut] noun.state (1 times)\n",
            "   Subjects: {'we'}\n",
            "   Objects: {'pressure'}\n",
            " noun.artifact [cut] noun.substance (1 times)\n",
            "   Subjects: {'mine'}\n",
            "   Objects: {'timber'}\n",
            " noun.communication [cut] noun.state (1 times)\n",
            "   Subjects: {'measure'}\n",
            "   Objects: {'participation'}\n",
            " noun.artifact [cut] noun.group (1 times)\n",
            "   Subjects: {'japan'}\n",
            "   Objects: {'line'}\n",
            " noun.cognition [cut] noun.attribute (1 times)\n",
            "   Subjects: {'it'}\n",
            "   Objects: {'cost'}\n",
            " noun.time [cut] noun.attribute (1 times)\n",
            "   Subjects: {'fall'}\n",
            "   Objects: {'surplus'}\n",
            " noun.communication [cut] noun.relation (1 times)\n",
            "   Subjects: {'germans'}\n",
            "   Objects: {'rate'}\n",
            " noun.location [cut] noun.act (1 times)\n",
            "   Subjects: {'arabia'}\n",
            "   Objects: {'shipment'}\n",
            " noun.attribute [cut] noun.time (1 times)\n",
            "   Subjects: {'use'}\n",
            "   Objects: {'time'}\n",
            " noun.attribute [cut] noun.person (1 times)\n",
            "   Subjects: {'use'}\n",
            "   Objects: {'partner'}\n",
            " noun.act [cut] noun.quantity (1 times)\n",
            "   Subjects: {'capacity'}\n",
            "   Objects: {'barrel'}\n",
            " noun.event [cut] noun.communication (1 times)\n",
            "   Subjects: {'producer'}\n",
            "   Objects: {'output'}\n",
            " noun.time [cut] noun.artifact (1 times)\n",
            "   Subjects: {'drought'}\n",
            "   Objects: {'crop'}\n",
            " noun.relation [cut] noun.attribute (1 times)\n",
            "   Subjects: {'rate'}\n",
            "   Objects: {'profit'}\n",
            " noun.relation [cut] noun.state (1 times)\n",
            "   Subjects: {'rate'}\n",
            "   Objects: {'turbulence'}\n",
            " noun.animal [cut] noun.attribute (1 times)\n",
            "   Subjects: {'crop'}\n",
            "   Objects: {'surplus'}\n",
            " noun.communication [cut] noun.possession (1 times)\n",
            "   Subjects: {'japanese'}\n",
            "   Objects: {'margin'}\n",
            " noun.relation [cut] noun.artifact (1 times)\n",
            "   Subjects: {'ratio'}\n",
            "   Objects: {'output'}\n",
            " noun.relation [cut] noun.object (1 times)\n",
            "   Subjects: {'ratio'}\n",
            "   Objects: {'decline'}\n",
            " noun.location [cut] noun.artifact (1 times)\n",
            "   Subjects: {'egypt'}\n",
            "   Objects: {'target'}\n",
            " noun.object [cut] noun.possession (1 times)\n",
            "   Subjects: {'segment'}\n",
            "   Objects: {'loss'}\n",
            " noun.group [cut] noun.communication (1 times)\n",
            "   Subjects: {'country'}\n",
            "   Objects: {'sale'}\n",
            " noun.possession [cut] noun.attribute (1 times)\n",
            "   Subjects: {'allowance'}\n",
            "   Objects: {'mark'}\n",
            " noun.state [cut] noun.communication (1 times)\n",
            "   Subjects: {'state'}\n",
            "   Objects: {'quota'}\n",
            " noun.time [cut] noun.act (1 times)\n",
            "   Subjects: {'drought'}\n",
            "   Objects: {'harvest'}\n",
            " noun.communication [cut] noun.cognition (1 times)\n",
            "   Subjects: {'proposal'}\n",
            "   Objects: {'program'}\n",
            " noun.phenomenon [cut] noun.quantity (1 times)\n",
            "   Subjects: {'weather'}\n",
            "   Objects: {'volume'}\n",
            " noun.artifact [cut] noun.event (1 times)\n",
            "   Subjects: {'bank'}\n",
            "   Objects: {'change'}\n",
            " noun.group [cut] noun.event (1 times)\n",
            "   Subjects: {'maker'}\n",
            "   Objects: {'loss'}\n",
            " noun.attribute [cut] noun.attribute (1 times)\n",
            "   Subjects: {'arrangement'}\n",
            "   Objects: {'cost'}\n",
            " noun.relation [cut] noun.relation (1 times)\n",
            "   Subjects: {'rate'}\n",
            "   Objects: {'rate'}\n",
            " noun.event [cut] noun.person (1 times)\n",
            "   Subjects: {'producer'}\n",
            "   Objects: {'price'}\n",
            " noun.communication [cut] noun.artifact (1 times)\n",
            "   Subjects: {'his'}\n",
            "   Objects: {'finger'}\n",
            " noun.plant [cut] noun.person (1 times)\n",
            "   Subjects: {'flesh'}\n",
            "   Objects: {'more'}\n",
            " noun.group [cut] noun.animal (1 times)\n",
            "   Subjects: {'people'}\n",
            "   Objects: {'man'}\n",
            " noun.act [cut] noun.cognition (1 times)\n",
            "   Subjects: {'wilt'}\n",
            "   Objects: {'seed'}\n",
            " noun.act [cut] noun.person (1 times)\n",
            "   Subjects: {'terror'}\n",
            "   Objects: {'i'}\n",
            " noun.person [cut] adj.all (1 times)\n",
            "   Subjects: {'i'}\n",
            "   Objects: {'righteous'}\n",
            " noun.person [cut] noun.relation (1 times)\n",
            "   Subjects: {'i'}\n",
            "   Objects: {'remnant'}\n",
            " noun.phenomenon [cut] noun.group (1 times)\n",
            "   Subjects: {'sea'}\n",
            "   Objects: {'we'}\n",
            " noun.communication [cut] noun.person (1 times)\n",
            "   Subjects: {'creed'}\n",
            "   Objects: {'he'}\n",
            " noun.quantity [cut] noun.attribute (1 times)\n",
            "   Subjects: {'shade'}\n",
            "   Objects: {'face'}\n",
            " noun.attribute [cut] noun.act (1 times)\n",
            "   Subjects: {'bulk'}\n",
            "   Objects: {'sight'}\n",
            " noun.group [cut] noun.attribute (1 times)\n",
            "   Subjects: {'we'}\n",
            "   Objects: {'dash'}\n",
            " noun.time [cut] noun.person (1 times)\n",
            "   Subjects: {'patch'}\n",
            "   Objects: {'figure'}\n",
            " noun.cognition [cut] noun.cognition (1 times)\n",
            "   Subjects: {'one'}\n",
            "   Objects: {'it'}\n",
            " noun.artifact [cut] noun.phenomenon (1 times)\n",
            "   Subjects: {'keel'}\n",
            "   Objects: {'sea'}\n",
            " noun.process [cut] noun.animal (1 times)\n",
            "   Subjects: {'sort'}\n",
            "   Objects: {'man'}\n",
            " noun.attribute [cut] noun.group (1 times)\n",
            "   Subjects: {'barrenness'}\n",
            "   Objects: {'we'}\n"
          ]
        }
      ],
      "source": [
        "if valence == 1: meanings = [st[1] for st in computed_semantic_types]\n",
        "elif valence > 1: meanings = [(st[1], st[3]) for st in computed_semantic_types]\n",
        "\n",
        "meanings_with_frequency = collections.Counter(meanings).items() # Use Counter to get frequency for each meaning\n",
        "sorted_meanings = sorted(meanings_with_frequency, key=lambda st: st[1], reverse=True) # Sort meanings by frequency\n",
        "\n",
        "# Print meanings and their lemmatized fillers\n",
        "print(f\"Identified meanings sorted by frequency ({len(sorted_meanings)} meanings):\")\n",
        "\n",
        "for meaning, freq in sorted_meanings:\n",
        "\n",
        "  if valence == 1: print(f\" {meaning} [{verb}] ({freq} times)\")\n",
        "  elif valence > 1: print(f\" {meaning[0]} [{verb}] {meaning[1]} ({freq} times)\")\n",
        "\n",
        "  if valence == 1:\n",
        "    print(f\"   Subjects: {set(subjs_dict[meaning])}\")\n",
        "  elif valence > 1:\n",
        "    print(f\"   Subjects: {set(subjs_dict[(meaning[0], meaning[1])])}\")\n",
        "    print(f\"   Objects: {set(objs_dict[(meaning[0], meaning[1])])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkuI4qmZKV8g"
      },
      "source": [
        "La presenza di svariati *significati* della forma `noun.cognition [cut]` o `noun.cognition [cut] X`  dominati dal soggetto *it* evidenzia un problema fondamentale: la necessità di una fase di risoluzione delle [coreferenze](https://w.wiki/8WXz). La quasi totalità di questi *it* infatti fa riferimento ad altri componenti delle frasi, non accessibili direttamente però tramite un *semplice* parsing a dipendenze come quello adottato.\n",
        "\n",
        " Si tratta però di un fenomeno la cui risoluzione non è banale: si potrebbe ignorare le occorrenze di *it* e dei vari pronomi ambigui (modificando quindi `get_semantic_type`), oppure, più correttamente, sarebbe necessario inserire una fase di risoluzione delle coreferenze nel parsing di SpaCy (possibile, ma ancora in fase sperimentale). In questa sede ci limitano invece semplicemente a evidenziare questa criticità."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}