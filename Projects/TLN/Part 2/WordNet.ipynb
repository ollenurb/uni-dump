{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "fcpsiO09ZbVZ",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Esercitazione 1 di Tecnologie del Linguaggio Naturale - Utilizzo di risorse lessicografiche per la concept similarity e la WSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAL0qIh-9pP6"
   },
   "source": [
    "Studenti:\n",
    "\n",
    "- Brunello Matteo (mat. 858867)\n",
    "- Caresio Lorenzo (mat. 836021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uZ8jN2SZbVe"
   },
   "source": [
    "## Conceptual similarity with WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "NEXZK_sqZbVf",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Consegna: dati in input due termini, il task di conceptual similarity consiste nel fornire un punteggio numerico di similarità che ne indichi la vicinanza semantica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znUlKe5KZbVh",
    "outputId": "04d1682c-f665-43a0-bbf1-5cb334e1a573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  7079  100  7079    0     0  65906      0 --:--:-- --:--:-- --:--:-- 66783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/lorenzo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!curl \"https://raw.githubusercontent.com/msavva/transphoner/master/data/wordsim353.csv\" -o WordSim353.csv\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "vMezhuhUZbVk",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Si va a parsare il dataset: questo è composto da 353 $3$-ple della forma $(termine1, \\: termine2, \\: media)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zGRMOWYZbVl",
    "outputId": "b15cd82c-1ef1-4b86-dd6a-9ef3942a3af0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 'sex', '6.77'),\n",
       " ('tiger', 'cat', '7.35'),\n",
       " ('tiger', 'tiger', '10.00')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "from typing import List\n",
    "\n",
    "def load_dataset(path) -> List[tuple[str, str, float]]:\n",
    "  dataset = []\n",
    "  with open(path, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        dataset.append((row['Word 1'], row['Word 2'], row['Human (mean)']))\n",
    "  return dataset\n",
    "\n",
    "dataset = load_dataset('WordSim353.csv')\n",
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "K5xdRMrzZbVm",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Il calcolo della profondità del senso attuale viene implementato come una **ricerca in ampiezza** sulla **multi-gerarchia**. È necessario implementare una (variante di una) strategia di ricerca in quanto possono esistere vari percorsi che dal dato senso portano a una radice della multi-gerarchia, alcuni con lunghezza inferiori ad altri. La profondità di un senso, in definitiva, è la lunghezza del cammino minimo che porta dal dato senso a una radice (la più vicina) della multi-gerarchia. È stata inoltre implementata la profondità massima di un senso, necessaria per il calcolo di metriche di similarità."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "MshTNmq4ZbVo"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Compute the (min) depth of a given sense (the distance between the sense and the closest hierarchy root)\n",
    "def sense_depth(sense) -> int:\n",
    "    depth = 0\n",
    "    stack = [sense]\n",
    "\n",
    "    # Breadth-First Search over hypernyms\n",
    "    while True:\n",
    "        new_stack = []\n",
    "        for node in stack:\n",
    "            hypernyms = node.hypernyms()\n",
    "            if not hypernyms: # the current node is a hierarchy root\n",
    "                return depth\n",
    "            new_stack.extend(node.hypernyms())\n",
    "\n",
    "        stack = new_stack # On the next iteration, the new hypernyms will be evaluated (expanded)\n",
    "        depth += 1\n",
    "\n",
    "# Compute the maximum depth of a given sense (the distance between the sense and the most distant hierarchy root)\n",
    "def max_sense_depth(sense) -> int:\n",
    "  depth = -1\n",
    "  stack = [sense]\n",
    "\n",
    "  # Breadth-First Search over hypernyms\n",
    "  while stack:\n",
    "      new_stack = []\n",
    "      for node in stack:\n",
    "          hypernyms = node.hypernyms()\n",
    "          if hypernyms:\n",
    "            new_stack.extend(node.hypernyms())\n",
    "\n",
    "      stack = new_stack # On the next iteration, the new hypernyms will be evaluated (expanded)\n",
    "      depth += 1\n",
    "\n",
    "  return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "-Cu-hcFwry-O"
   },
   "outputs": [],
   "source": [
    "# Check if the computed depths match the ones computed by NTLK\n",
    "syn = wn.synsets(dataset[0][0])[0]\n",
    "if sense_depth(syn) != syn.min_depth(): print(False)\n",
    "if max_sense_depth(syn) != syn.max_depth(): print(False)\n",
    "\n",
    "# Extensive check\n",
    "for record in dataset:\n",
    "  for term1 in record[0]:\n",
    "    for sense in wn.synsets(term1):\n",
    "      if sense_depth(sense) != sense.min_depth(): print(False)\n",
    "      if max_sense_depth(sense) != sense.max_depth(): print(False)\n",
    "  for term2 in record[1]:\n",
    "    for sense in wn.synsets(term2):\n",
    "      if sense_depth(sense) != sense.min_depth(): print(False)\n",
    "      if max_sense_depth(sense) != sense.max_depth(): print(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egrSz47Sl23d"
   },
   "source": [
    "### Lowest Common Subsumer\n",
    "\n",
    "Il *lowest common subsumer* (LCS) è l'iperonimo comune tra i due sensi che si trova più in profondità all'interno della multi-gerarchia (tra i possibili iperonimi in comune è quindi quello più vicino ai due sensi, o più succintamente l'*antenato comune più specifico*). In maniera consistente con la libreria NLTK, il LCS tra due sensi di POS di natura differente (e.g. un senso relativo a un verbo e uno relativo a un sostantivo) non esiste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "XmNt2rhHZbVs"
   },
   "outputs": [],
   "source": [
    "# Compute the lowest common subsumer between two senses\n",
    "def lowest_common_subsumer(sense1, sense2):\n",
    "\n",
    "    # The LCS of two senses referring to different POS doesn't exist (performance improvement, the same resulted will be computed further)\n",
    "    if sense1.pos() != sense2.pos():\n",
    "        return None\n",
    "\n",
    "    sense1_expansion = [sense1]\n",
    "    sense2_expansion = [sense2]\n",
    "\n",
    "    # Get the list of hypernyms of the first sense\n",
    "    for node in sense1_expansion:\n",
    "      current_hypernym = node.hypernyms()\n",
    "      if current_hypernym: sense1_expansion.extend(current_hypernym)\n",
    "    # Get the list of hypernyms of the second sense\n",
    "    for node in sense2_expansion:\n",
    "      current_hypernym = node.hypernyms()\n",
    "      if current_hypernym: sense2_expansion.extend(current_hypernym)\n",
    "\n",
    "    common_subsumers = set([sense for sense in sense1_expansion if sense in sense2_expansion])\n",
    "\n",
    "    # Compute the lowest common subsumer\n",
    "    lcs = None\n",
    "    lcs_depth = 0\n",
    "\n",
    "    for cs in common_subsumers:\n",
    "      ch_depth = max_sense_depth(cs)\n",
    "      if ch_depth >= lcs_depth:\n",
    "        lcs = cs\n",
    "        lcs_depth = ch_depth\n",
    "\n",
    "    return lcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "KlMrNev3r2xt"
   },
   "outputs": [],
   "source": [
    "# Check if the computed LCS match the one computed by NTLK\n",
    "s1 = wn.synsets(dataset[1][0])\n",
    "s2 = wn.synsets(dataset[1][1])\n",
    "for sense1 in s1:\n",
    "  for sense2 in s2:\n",
    "    if lowest_common_subsumer(sense1, sense2) and not(lowest_common_subsumer(sense1, sense2) in sense1.lowest_common_hypernyms(sense2)):\n",
    "      print(False)\n",
    "\n",
    "# Extensive check for LCS\n",
    "for record in dataset:\n",
    "  s1 = wn.synsets(record[0])\n",
    "  s2 = wn.synsets(record[1])\n",
    "  for sense1 in s1:\n",
    "    for sense2 in s2:\n",
    "      if lowest_common_subsumer(sense1, sense2) and not(lowest_common_subsumer(sense1, sense2) in sense1.lowest_common_hypernyms(sense2)):\n",
    "        print(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qy--BaXEkqS"
   },
   "source": [
    "Per il calcolo della distanza tra due sensi (utilizzata in alcune metriche di similarità) è necessario distinguere se il LCS tra i due sensi è presente o meno: in caso sia presente, il calcolo della distanza è immediato; in caso non sia presente, si è deciso (*assunzione di lavoro*) di sommare le profondità massime dei due sensi e aggiungere uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "RYsGBp7JZbVv"
   },
   "outputs": [],
   "source": [
    "def length_between_senses(sense1, sense2) -> int:\n",
    "  lcs = lowest_common_subsumer(sense1, sense2)\n",
    "  if lcs:\n",
    "      lcs_depth = sense_depth(lcs)\n",
    "      return (max_sense_depth(sense1) - lcs_depth) + (max_sense_depth(sense2) - lcs_depth)\n",
    "  else:\n",
    "      return max_sense_depth(sense1) + max_sense_depth(sense2) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Hj2aHxntZbVw",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Metriche di similarità\n",
    "\n",
    "Si implementano delle metriche di similarità tra sensi:\n",
    "\n",
    "- Wu & Palmer: $cs(s_1, s_2) = \\frac{2 \\cdot depth(LCS)}{depth(s_1) + depth(s_2)}$\n",
    "- Shortest Path: $sim_{path}(s_1, s_2) = 2 \\cdot depthMax - len(s_1, s_2)$\n",
    "- Leakcock & Chodorow: $sim_{ln}(s_1, s_2) = - \\log \\frac{len(s_1, s_2)}{2 \\cdot depthMax}$\n",
    "\n",
    "È importante notare che le metriche lavorano su sensi, e non su termini.\n",
    "\n",
    "La costante $depthMax$ è pari a [19 per i termini, 12 per i verbi, 0 per aggettivi e avverbi](https://github.com/nltk/wordnet/blob/ce91915ae38a341ae845be4d825ef6003cddf395/wn/constants.py#L31C1-L31C1). Invece di ricalcolarla a ogni chiamata (come invece fa NLTK nella sua attuale versione), si decide invece di mantenerla *hard-coded* nel codice (come avviene nella versione candidata di NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "BWcAiWaDZbVx"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_depth_max(sense1, sense2) -> int:\n",
    "    depth_max_s1 = 0\n",
    "    depth_max_s2 = 0\n",
    "\n",
    "    match sense1.pos():\n",
    "        case 'n': depth_max_s1 = 19\n",
    "        case 'v': depth_max_s1 = 12\n",
    "    match sense2.pos():\n",
    "        case 'n': depth_max_s2 = 19\n",
    "        case 'v': depth_max_s2 = 12\n",
    "\n",
    "    return depth_max_s1 if depth_max_s1 > depth_max_s2 else depth_max_s2\n",
    "\n",
    "def wup_similarity(sense1, sense2) -> float:\n",
    "    lcs = lowest_common_subsumer(sense1, sense2)\n",
    "    if lcs: return (2 * max_sense_depth(lcs)) / (max_sense_depth(sense1) + max_sense_depth(sense2))\n",
    "    else: return 0\n",
    "\n",
    "def path_similarity(sense1, sense2) -> int:\n",
    "    depth_max = get_depth_max(sense1, sense2)\n",
    "    return (2 * depth_max) - length_between_senses(sense1, sense2)\n",
    "\n",
    "def lch_similarity(sense1, sense2) -> float:\n",
    "    depth_max = get_depth_max(sense1, sense2)\n",
    "    depth_max = 1 if depth_max == 0 else depth_max # Needed to avoid DivisionByZero\n",
    "    return - math.log((length_between_senses(sense1, sense2) + 1) / (2 * depth_max + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Y5SXlOPzmkY",
    "outputId": "1429a4a5-4b84-497a-a137-95cb1354adbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WUP:\n",
      "  0.5\n",
      "  0.5454545454545454\n",
      "Path:\n",
      "  28\n",
      "  0.09090909090909091\n",
      "LCH:\n",
      "  1.2656663733312759\n",
      "  1.2396908869280152\n"
     ]
    }
   ],
   "source": [
    "# Check how the computed similarity metrics perform compared to the ones computed by NTLK\n",
    "s1 = wn.synsets(dataset[1][0])[0]\n",
    "s2 = wn.synsets(dataset[1][1])[0]\n",
    "\n",
    "print(\"WUP:\")\n",
    "print(\" \", wup_similarity(s1, s2))\n",
    "print(\" \", s1.wup_similarity(s2))\n",
    "\n",
    "print(\"Path:\")\n",
    "print(\" \", path_similarity(s1, s2))\n",
    "print(\" \", s1.path_similarity(s2))\n",
    "\n",
    "print(\"LCH:\")\n",
    "print(\" \", lch_similarity(s1, s2))\n",
    "print(\" \", s1.lch_similarity(s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eo8mJdl4AiCb"
   },
   "source": [
    "In relazione al risultato della *path similarity*, la libreria NLTK calcola quest'ultima come $\\frac{1}{len(s1, s2) + 1}$. Questa non è però la definizione di PS vista a lezione, a cui si è invece deciso di far riferimento. Analogamente, nella *lch similarity*, viene aggiunto $1$ solo al dividendo e non anche al divisore, differendo da quanto visto a lezione.\n",
    "\n",
    "In generale, piuttosto che ricercare una *result-parity* con l'implementazione delle metriche di NLTK, si è deciso di seguire quanto visto a lezione, fornendo un'implementazione minimale delle metriche che cercasse allo stesso tempo di coglierne l'essenza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yw0X5ffsZbVy"
   },
   "source": [
    "### Similarità massima e correlazione\n",
    "\n",
    "Data una coppia di termini, si va a calcolare la coppia di sensi dei due termini che massimizza la loro similarità. Si esegue questo calcolo per ogni coppia presente nel dataset, per poi andare a calcolare la correlazione rispetto ai valori di similarità medi forniti da soggetti umani."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "ApSsysvfZbVy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_max_similarity(term1, term2, similarity = wup_similarity):\n",
    "\n",
    "    max_similarity = 0\n",
    "\n",
    "    senses1 = wn.synsets(term1)\n",
    "    senses2 = wn.synsets(term2)\n",
    "\n",
    "    for sn1 in senses1:\n",
    "        for sn2 in senses2:\n",
    "            current_similarity = similarity(sn1, sn2)\n",
    "            max_similarity = current_similarity if current_similarity > max_similarity else max_similarity\n",
    "\n",
    "    return max_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQ59Ol3ay8_4",
    "outputId": "be21164f-d828-47c8-f935-842066da4f80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90909091 0.96296296 1.         0.85714286 0.8        0.58823529\n",
      " 0.7        0.70588235 0.         0.9       ]\n",
      "[ 6.77  7.35 10.    7.46  7.62  7.58  5.77  6.31  7.5   6.77]\n"
     ]
    }
   ],
   "source": [
    "computed_similarities = np.array([])\n",
    "dataset_similarities = np.array([])\n",
    "\n",
    "similarity_function = wup_similarity # Set here the preferred similarity metrics {wup_similarity, lch_similarity, path_similarity}\n",
    "\n",
    "for record in dataset:\n",
    "    dataset_similarities = np.append(dataset_similarities, record[2])\n",
    "\n",
    "    ms = compute_max_similarity(record[0], record[1], similarity_function)\n",
    "    computed_similarities = np.append(computed_similarities, ms)\n",
    "\n",
    "print(computed_similarities[:10])\n",
    "dataset_similarities = dataset_similarities.astype(float) # From Unicode to float\n",
    "print(dataset_similarities[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtsluHL4ZbV0"
   },
   "source": [
    "Sui valori di similarità ottenuti si va a calcolare la correlazione rispetto ai valori contenuti nel dataset.\n",
    "\n",
    "Correlazione prevista: 30-35%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMoBudArZbV1",
    "outputId": "596c3d47-e0b5-4a6f-e827-723e9da175ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlations computed on wup similarity \n",
      "  Pearson: 0.27563185573720034\n",
      "  Spearman: 0.32205979107230365\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(\"Correlations computed on\", similarity_function.__name__.replace(\"_\", \" \"), \"\")\n",
    "res_pear = stats.pearsonr(dataset_similarities, computed_similarities)\n",
    "print(\"  Pearson:\", res_pear.statistic)\n",
    "res_spear = stats.spearmanr(dataset_similarities, computed_similarities)\n",
    "print(\"  Spearman:\", res_spear.statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GD0x9AEIKrCp"
   },
   "source": [
    "Per valutare la bontà della nostra implementazione si va quindi a calcolare la correlazione sulle similarità massime ottenute tramite la libreria NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "-eaUhOZ_Jx0-"
   },
   "outputs": [],
   "source": [
    "def compute_max_similarity_with_ntlk(term1, term2, similarity):\n",
    "\n",
    "    max_similarity = 0\n",
    "\n",
    "    senses1 = wn.synsets(term1)\n",
    "    senses2 = wn.synsets(term2)\n",
    "\n",
    "    for sn1 in senses1:\n",
    "        for sn2 in senses2:\n",
    "\n",
    "            match similarity.__name__:\n",
    "              case \"path_similarity\":\n",
    "                current_similarity = sn1.path_similarity(sn2)\n",
    "              case \"lch_similarity\":\n",
    "                current_similarity = sn1.lch_similarity(sn2) if sn1.pos() == sn2.pos() else 0\n",
    "              case _:\n",
    "                current_similarity = sn1.wup_similarity(sn2)\n",
    "\n",
    "            max_similarity = current_similarity if current_similarity > max_similarity else max_similarity\n",
    "\n",
    "    return max_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7Tj2gnUy2EI",
    "outputId": "8833abee-4515-4fee-baf3-0305f63bff71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlations computed on wup similarity \n",
      "  Pearson: 0.297440832551365\n",
      "  Spearman: 0.33850273836806116\n",
      "\n",
      "  Difference (Pearson): 0.02180897681416466\n",
      "  Difference (Spearman): 0.016442947295757515\n"
     ]
    }
   ],
   "source": [
    "nltk_similarities = np.array([])\n",
    "\n",
    "for record in dataset:\n",
    "    ms = compute_max_similarity_with_ntlk(record[0], record[1], similarity_function)\n",
    "    nltk_similarities = np.append(nltk_similarities, ms)\n",
    "\n",
    "print(\"Correlations computed on\", similarity_function.__name__.replace(\"_\", \" \"), \"\")\n",
    "\n",
    "res_lib_pear = stats.pearsonr(dataset_similarities, nltk_similarities)\n",
    "print(\"  Pearson:\", res_lib_pear.statistic)\n",
    "res_lib_spear = stats.spearmanr(dataset_similarities, nltk_similarities)\n",
    "print(\"  Spearman:\", res_lib_spear.statistic)\n",
    "\n",
    "print(\"\\n  Difference (Pearson):\", res_lib_pear.statistic - res_pear.statistic)\n",
    "print(\"  Difference (Spearman):\", res_lib_spear.statistic - res_spear.statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgcnpd_PaJsE"
   },
   "source": [
    "## Word Sense Disambiguation\n",
    "\n",
    "Consegna: implementare l'algoritmo Lesk.\n",
    "\n",
    "  1. Estrarre 50 frasi dal corpus SemCor e disambiguare (almeno) un sostantivo per frase. Calcolare l'accuratezza del sistema implementato sulla base dei sensi annotati in SemCor.\n",
    "  2. Randomizzare la selezione delle 50 frasi e la selezione del termine da disambiguare, e restituire l'accuratezza media su (per esempio) 10 esecuzioni del programma.\n",
    "  - Opzionale: implementare l'algoritmo Corpus Lesk utilizzando Sem-Cor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oww8MfYzlMNR"
   },
   "source": [
    "### Algoritmo di Lesk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhAutRyjAueP"
   },
   "source": [
    "La seguente implementazione segue in maniera fedele quella vista a lezione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "OinvfOZlA0FJ"
   },
   "outputs": [],
   "source": [
    "def simplified_lesk(word, sentence):\n",
    "\n",
    "  word_senses = wn.synsets(word)\n",
    "  best_sense = word_senses[0]\n",
    "\n",
    "  if len(word_senses) == 1: return best_sense\n",
    "\n",
    "  max_overlap = 0\n",
    "\n",
    "  # The context is the corresponding set of words contained in the given sentence (minus non-alphanumerical chars)\n",
    "  context = set(remove_non_alphanumerical_chars(sentence).split(' '))\n",
    "\n",
    "  for word_sense in word_senses:\n",
    "\n",
    "    # Compute current sense signature (definition + examples)\n",
    "    signature = set(remove_non_alphanumerical_chars(word_sense.definition()).split(' '))\n",
    "    for example in word_sense.examples():\n",
    "      # Add current example words to the signature (minus non-alphanumerical chars)\n",
    "      signature.update(remove_non_alphanumerical_chars(example).split(' '))\n",
    "\n",
    "    overlap = len(signature.intersection(context)) # Compute the overlap between current signature and the given context\n",
    "\n",
    "    if overlap > max_overlap:\n",
    "      max_overlap = overlap\n",
    "      best_sense = word_sense\n",
    "\n",
    "  return best_sense\n",
    "\n",
    "def remove_non_alphanumerical_chars(s):\n",
    "  return ''.join(filter(lambda c: c.isalnum() or c.isspace(), s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqD9TzkFPl86"
   },
   "source": [
    "Si esegue l'algoritmo di Lesk sulla frase d'esempio *the bank can guarantee deposits will eventually cover future tuition costs because it invests in adjustable-rate mortgage securities*, dove si vuole disambiguare il termine *bank*.\n",
    "\n",
    "Per un parlante Inglese umano è evidente come *bank* in questa frase si riferisca a un'istituzione finanziaria (corrispondente al synset `'depository_financial_institution.n.01'`) e non ad altri possibili sensi, come, per esempio, quello di *banchina* (`'bank.n.01'`) o di insieme di oggetti (`'bank.n.04'`, *e.g.* un banco di pesci)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dVjrQN_GPlpY",
    "outputId": "f4ecbc1e-87a5-4cda-fef1-0a12a028f347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('depository_financial_institution.n.01')\n"
     ]
    }
   ],
   "source": [
    "word = 'bank'\n",
    "sentence = 'the bank can guarantee deposits will eventually cover future tuition costs because it invests in adjustable-rate mortgage securities'\n",
    "\n",
    "print(simplified_lesk(word, sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDnyY0EWtEvh"
   },
   "source": [
    "Si può constatare da questo primo esempio come già questa versione simplificata dell'algoritmo di Lesk, basata sul calcolo e il confronto della sovrapposizione delle *signature* dei vari sensi con il dato contesto, permetta una corretta disambiguazione del senso di una parola (almeno per un esempio immediato come quello in analisi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azaGXtGQtmq4"
   },
   "source": [
    "#### Rimozione delle *stop words*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQBS3hRMt6k4"
   },
   "source": [
    "Analizzando gli overlap tra le varie *signature* e il dato contesto è possibile constatare come la maggiorparte delle sovrapposizioni siano composte da *stop words* (nel caso visto sopra, da *in* e *the*), parole non utili all'obiettivo di disambiguare il senso di una data parola.\n",
    "\n",
    "Si decide quindi di implementare, utilizzando gli strumenti forniti da NLTK per il trattamento delle *stop words*, una versione semplificata dell'algoritmo di Lesk che ignori proprio quest'ultime, così da evitare che overlap composti in parte o nella totalità da *stop words* finiscano per oscurare (numericamente) overlap con parole utili al task di disambiguazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgNmtLl6t6Iq",
    "outputId": "a08ba336-d0d8-43e4-9c1f-e07109780d12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lorenzo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stopwordless_simplified_lesk(word, sentence):\n",
    "\n",
    "  word_senses = wn.synsets(word)\n",
    "  best_sense = word_senses[0]\n",
    "\n",
    "  if len(word_senses) == 1: return best_sense\n",
    "\n",
    "  max_overlap = 0\n",
    "\n",
    "  # The context is the corresponding set of words contained in the given sentence (minus non-alphanumerical chars and stop words)\n",
    "  context = set(remove_non_alphanumerical_chars(sentence).split(' ')).difference(stop_words)\n",
    "\n",
    "  for word_sense in word_senses:\n",
    "\n",
    "    # Compute current sense signature (definition + examples)\n",
    "    signature = set(remove_non_alphanumerical_chars(word_sense.definition()).split(' '))\n",
    "    for example in word_sense.examples():\n",
    "      # Add current example words to the signature (minus non-alphanumerical chars)\n",
    "      signature.update(remove_non_alphanumerical_chars(example).split(' '))\n",
    "    signature = signature.difference(stop_words) # Remove stop words from signature\n",
    "\n",
    "    overlap = len(signature.intersection(context)) # Compute the overlap between current signature and the given context\n",
    "\n",
    "    if overlap > max_overlap:\n",
    "      max_overlap = overlap\n",
    "      best_sense = word_sense\n",
    "\n",
    "  return best_sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5yntljcx7fF"
   },
   "source": [
    "Utilizzando l'esempio precedente sulla disambiguazione della parola *bank*, è possibile constatare come il risultato dell'algoritmo non vari, in quanto le parole in overlap per il senso (corretto) `'depository_financial_institution.n.01'` sono $\\{deposits,\\: mortgage,\\: bank\\}$, tre termini che non vengono filtrati non essendo *stop words*. D'ora in avanti l'implementazione dell'algoritmo presa in considerazione sarà questa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRkIgqc8ybkM",
    "outputId": "811c41e8-8abb-4bba-da90-363ea276637e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('depository_financial_institution.n.01') True\n"
     ]
    }
   ],
   "source": [
    "print(stopwordless_simplified_lesk(word, sentence), stopwordless_simplified_lesk(word, sentence) == simplified_lesk(word, sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoiqAC2P2K8b"
   },
   "source": [
    "### Disambiguare frasi in SemCor\n",
    "\n",
    "Come dal punto $1$ della consegna, si estraggono cinquanta frasi dal corpus SemCor e si va a disambiguare (almeno) un sostantivo per frase.\n",
    "\n",
    "Per evitare disambiguazioni *banali*, si decide prima di selezionare i sostantivi *polisemici* dell'attuale frase (quelli con almeno due synset di tipo `NOUN` associati) per poi andarli a disambiguare utilizzando la versione semplificata dell'algoritmo di Lesk vista sopra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "SdI2E7sW5RpC"
   },
   "outputs": [],
   "source": [
    "def compute_ambiguous_nouns(word_list):\n",
    "  words = set(word_list).difference(stop_words)\n",
    "  ambiguous_nouns = []\n",
    "\n",
    "  for word in words:\n",
    "    word_senses = wn.synsets(word, pos=wn.NOUN)\n",
    "    if word_senses and len(word_senses) > 1:\n",
    "      ambiguous_nouns.append(word)\n",
    "\n",
    "  return ambiguous_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sHcPcURk6Bm8",
    "outputId": "97a89139-1758-4231-e13b-3efa58a2e0c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['securities', 'future', 'guarantee', 'costs', 'bank', 'cover', 'tuition', 'deposits']\n",
      "   securities with 9 noun synsets\n",
      "   future with 3 noun synsets\n",
      "   guarantee with 3 noun synsets\n",
      "   costs with 4 noun synsets\n",
      "   bank with 10 noun synsets\n",
      "   cover with 10 noun synsets\n",
      "   tuition with 2 noun synsets\n",
      "   deposits with 9 noun synsets\n"
     ]
    }
   ],
   "source": [
    "ambiguous_nouns = compute_ambiguous_nouns(sentence.split())\n",
    "\n",
    "print(ambiguous_nouns)\n",
    "\n",
    "for ambiguous_noun in ambiguous_nouns:\n",
    "  print(f\"   {ambiguous_noun} with {len(wn.synsets(ambiguous_noun, pos=wn.NOUN))} noun synsets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtZ7z7fe7fnI"
   },
   "source": [
    "Verificandone il funzionamento con la frase d'esempio (di sopra) è però evidente da subito una criticità: si vanno anche a considerare componenti della frase che in questo caso non hanno il ruolo di sostantivi, come *guarantee* e *cover*, che svolgono invece il ruolo di verbi.\n",
    "\n",
    "È presente quindi, oltre a un livello di ambiguità semantica, anche un livello di ambiguità sintattica: sarà necessario utilizzare un PoS Tagger per filtrare i soli sostantivi o utilizzare un dataset già annotato con i corretti PoS (come nel caso di SemCor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOg-cwwv9xV1",
    "outputId": "c02db90d-6858-4af3-8404-06c49bbe6871"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package semcor to /home/lorenzo/nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('semcor')\n",
    "from nltk.corpus import semcor\n",
    "\n",
    "semcor_sentences = semcor.sents()\n",
    "semcor_tagged_sentences = semcor.tagged_sents(tag='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d_xzX3LfIZV"
   },
   "source": [
    "#### SemCor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubwfqI0VFCIl"
   },
   "source": [
    "SemCor è un corpus annotato dove a ogni parola delle 37176 frasi presenti nel dataset è associato il corretto synset WordNet e il corretto tag PoS (Penn Treebank tagset) nel contesto della frase (l'associazione è stata fatta da umani, trattasi perciò di un *golden corpus*), è quindi possibile utilizzare SemCor per discriminare i sostantivi da scegliere per la disambiguazione (e, soprattutto, come benchmark per valutare le prestazioni della nostra implementazione).\n",
    "\n",
    "Ogni termine in una frase (o insieme di termini, in quanto sono gestite anche le espressioni multi-termine) è rappresentato come un albero NLTK, con il suo tag PoS (l'etichetta dell'albero) e l'effettivo termine (o termini) come foglie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "oe9GXahG_W5c"
   },
   "outputs": [],
   "source": [
    "def get_nouns_in_semcore_sentence(sent_id):\n",
    "\n",
    "  noun_list = []\n",
    "\n",
    "  for element in semcor_tagged_sentences[sent_id]:\n",
    "\n",
    "      # This complex multi-part condition is needed (instead of a simpler\n",
    "      # `if element.label() == 'NN'` from a SemCore PoS Tree) to cover those\n",
    "      # rare cases when a noun hasn't a corresponding WordNet synset\n",
    "      # (within our experience this happened just once with the noun *none*)\n",
    "      if isinstance(element, nltk.Tree) and \\\n",
    "         isinstance(element.label(), nltk.corpus.reader.wordnet.Lemma) and \\\n",
    "         element[0].label() == 'NN':  # Fetch nouns only\n",
    "            noun_list.append(\" \".join(element.leaves()))\n",
    "\n",
    "  return noun_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cjme0WJoOkmc"
   },
   "source": [
    "Si utilizza la prima frase fornita da SemCor per valutare la bontà di quanto implementato finora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4nElHMLLHZRG",
    "outputId": "54ef1dbb-af47-4506-e150-b0560504caa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The Fulton County Grand Jury said Friday an investigation of Atlanta 's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "Nouns:\n",
      "   ['Friday', 'investigation', 'Atlanta', 'primary election', 'evidence', 'irregularities']\n",
      "Ambiguous nouns:\n",
      "   evidence with 3 noun synsets\n",
      "   Atlanta with 2 noun synsets\n",
      "   investigation with 2 noun synsets\n",
      "   irregularities with 4 noun synsets\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence:\", \" \".join(semcor_sentences[0]))\n",
    "\n",
    "nouns = get_nouns_in_semcore_sentence(0)\n",
    "print(\"Nouns:\")\n",
    "print(\"  \", nouns)\n",
    "\n",
    "ambiguous_nouns = compute_ambiguous_nouns(nouns)\n",
    "print(\"Ambiguous nouns:\")\n",
    "for ambiguous_noun in ambiguous_nouns:\n",
    "  print(f\"   {ambiguous_noun} with {len(wn.synsets(ambiguous_noun, pos=wn.NOUN))} noun synsets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QDaJuXuf0nH"
   },
   "source": [
    "#### Esecuzione su singolo batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2Wh_4Qpp1rv"
   },
   "source": [
    "Si estraggono (le prime) cinquanta frasi dal corpus SemCor con sostantivi ambigui e di questi (per ogni frase) si va a disambiguare o il primo, o il *più ambiguo* (*i.e* quello con più synset di tipo `NOUN` associati) o il *meno ambiguo*. Una volta disambiguati si va a calcolare l'accuratezza confrontando il risultato con i valori contenuti in SemCor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "hlTQD81ghX7n"
   },
   "outputs": [],
   "source": [
    "def compute_most_ambiguous_noun(noun_list):\n",
    "  most_ambiguous_noun = None\n",
    "  max_sense_n = 0\n",
    "\n",
    "  for noun in noun_list:\n",
    "    if len(wn.synsets(noun, pos=wn.NOUN)) > max_sense_n:\n",
    "      max_sense_n = len(wn.synsets(noun, pos=wn.NOUN))\n",
    "      most_ambiguous_noun = noun\n",
    "\n",
    "  return most_ambiguous_noun\n",
    "\n",
    "def compute_least_ambiguous_noun(noun_list):\n",
    "  least_ambiguous_noun = noun_list[0]\n",
    "  min_sense_n = len(wn.synsets(noun_list[0]))\n",
    "\n",
    "  for noun in noun_list:\n",
    "    if len(wn.synsets(noun, pos=wn.NOUN)) < min_sense_n:\n",
    "      min_sense_n = len(wn.synsets(noun, pos=wn.NOUN))\n",
    "      least_ambiguous_noun = noun\n",
    "\n",
    "  return least_ambiguous_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "5DienoY-9RnC"
   },
   "outputs": [],
   "source": [
    "# Get the corresponding synset of a given noun in a given sentence in the SemCor corpus\n",
    "def get_semcor_synset(noun, sent_id):\n",
    "  for element in semcor_tagged_sentences[sent_id]:\n",
    "    if isinstance(element, nltk.Tree) and \\\n",
    "       isinstance(element.label(), nltk.corpus.reader.wordnet.Lemma) and \\\n",
    "       element[0].label() == 'NN' and \\\n",
    "       \" \".join(element.leaves()) == noun:\n",
    "          return element.label().synset()\n",
    "\n",
    "# Get the first n SemCor sentences (id) which contain ambiguous nouns\n",
    "def get_n_semcor_sentences_with_ambiguous_nouns(n):\n",
    "  id_list = []\n",
    "  i = 0\n",
    "\n",
    "  while (n > 0):\n",
    "    nouns = get_nouns_in_semcore_sentence(i)\n",
    "    ambiguous_nouns = compute_ambiguous_nouns(nouns)\n",
    "    if ambiguous_nouns:\n",
    "      id_list.append(i)\n",
    "      n = n - 1\n",
    "    i = i + 1\n",
    "\n",
    "  return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTX3P9vj_acJ",
    "outputId": "69f032d3-8556-4207-f9a9-4b3dfe8be65c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching SemCor sentences with ambiguous nouns...\n",
      "Currently disambiguating...\n",
      "Accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "sentences_number = 50\n",
    "\n",
    "print(\"Fetching SemCor sentences with ambiguous nouns...\")\n",
    "sentences_ids = get_n_semcor_sentences_with_ambiguous_nouns(sentences_number)\n",
    "\n",
    "print(\"Currently disambiguating...\")\n",
    "\n",
    "correct_disambiguations = 0\n",
    "\n",
    "for sentence_id in sentences_ids:\n",
    "  nouns = get_nouns_in_semcore_sentence(sentence_id)\n",
    "  ambiguous_nouns = compute_ambiguous_nouns(nouns)\n",
    "\n",
    "  # Set to compute_most_ambiguous_noun or to ambiguous_nouns[0] to change selection criterion\n",
    "  noun_to_disambiguate = compute_least_ambiguous_noun(ambiguous_nouns)\n",
    "  lesk_result = stopwordless_simplified_lesk(noun_to_disambiguate, \" \".join(semcor_sentences[sentence_id]))\n",
    "  semcor_annotation = get_semcor_synset(noun_to_disambiguate, sentence_id)\n",
    "\n",
    "  if lesk_result == semcor_annotation:\n",
    "    correct_disambiguations = correct_disambiguations + 1\n",
    "\n",
    "print(\"Accuracy:\", correct_disambiguations / len(sentences_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56sj_Dz2p1ry"
   },
   "source": [
    "Con le prime cinquanta frasi con sostantivi ambigui l'accuratezza è la seguente:\n",
    "\n",
    "- Meno ambiguo (*best case scenario*): $68\\%$\n",
    "- Più ambiguo (*worst case scenario*): $28\\%$\n",
    "- Primo sostantivo ambiguo (scelta aribitraria): $56\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyVB6VdIp1ry"
   },
   "source": [
    "#### Esecuzione su batch randomizzati\n",
    "\n",
    "\n",
    "Come dal punto $2$ della consegna, si randomizza la selezione delle 50 frasi e la selezione del termine da disambiguare, e si va a restituire l'accuratezza media su (per esempio) 10 esecuzioni del programma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "Rf0SUdUXp1rz"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_random_ambiguous_noun(noun_list):\n",
    "  return random.choice(noun_list)\n",
    "\n",
    "# Get n random SemCor sentences (id) which contain ambiguous nouns (that haven't been already picked)\n",
    "def get_n_random_semcor_sentences_with_ambiguous_nouns(n, previous_ids):\n",
    "  id_list = []\n",
    "  i = 0\n",
    "\n",
    "  while (n > 0):\n",
    "\n",
    "    # Generate a random id and check if it has already been picked\n",
    "    random_id = random.randint(0, len(semcor_sentences)-1)\n",
    "\n",
    "    while random_id in previous_ids:\n",
    "      random_id = random.randint(0, len(semcor_sentences)-1)\n",
    "\n",
    "    # Add a new random sentence id only if it hasn't already picked and if the sentence has ambiguous nouns\n",
    "    nouns = get_nouns_in_semcore_sentence(random_id)\n",
    "    ambiguous_nouns = compute_ambiguous_nouns(nouns)\n",
    "    if ambiguous_nouns:\n",
    "      id_list.append(random_id)\n",
    "      n = n - 1\n",
    "\n",
    "    # Append the current random id even if it's not useful in order to avoid future re-calculation\n",
    "    previous_ids.append(random_id)\n",
    "\n",
    "  return (id_list, previous_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "PftKtJ91p1r0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently disambiguating 10 batch of 50 sentences...\n",
      "Average accuracy: 0.476\n"
     ]
    }
   ],
   "source": [
    "sentences_number = 50\n",
    "iterations_number = 10\n",
    "\n",
    "print(f\"Currently disambiguating {iterations_number} batch of {sentences_number} sentences...\")\n",
    "\n",
    "cumulative_accuracy = 0\n",
    "\n",
    "previous_ids = []\n",
    "\n",
    "for i in range(0, iterations_number):\n",
    "  (sentences_ids, previous_ids) = get_n_random_semcor_sentences_with_ambiguous_nouns(sentences_number, previous_ids)\n",
    "\n",
    "  correct_disambiguations = 0\n",
    "\n",
    "  for sentence_id in sentences_ids:\n",
    "    nouns = get_nouns_in_semcore_sentence(sentence_id)\n",
    "    ambiguous_nouns = compute_ambiguous_nouns(nouns)\n",
    "\n",
    "    noun_to_disambiguate = get_random_ambiguous_noun(ambiguous_nouns) # Randomized ambiguous noun choice\n",
    "    lesk_result = stopwordless_simplified_lesk(noun_to_disambiguate, \" \".join(semcor_sentences[sentence_id]))\n",
    "    semcor_annotation = get_semcor_synset(noun_to_disambiguate, sentence_id)\n",
    "\n",
    "    if lesk_result == semcor_annotation:\n",
    "      correct_disambiguations = correct_disambiguations + 1\n",
    "\n",
    "  cumulative_accuracy = cumulative_accuracy + (correct_disambiguations / len(sentences_ids))\n",
    "\n",
    "print(\"Average accuracy:\", cumulative_accuracy / iterations_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4CDLinwp1r1"
   },
   "source": [
    "Eseguendo più volte il codice è possibile constatare come l'accuratezza media oscilli tra il $40\\%$ e il $50\\%$, prestazioni al disopra del *worst case scenario* visto sopra ma comunque sufficientemente inferiori a quello migliore, e non migliori di un *random guessing*. Per ottenere prestazioni migliori è necessario implementare *Corpus Lesk*, così da poter guidare le scelte dei vari sensi utilizzando i più usati all'interno del corpus di SemCor."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
