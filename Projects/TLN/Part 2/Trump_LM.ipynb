{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdXan58ysdIY"
   },
   "source": [
    "# Esercitazione 3 di Tecnologie del Linguaggio Naturale - Twitting (like) a Trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studenti:\n",
    "\n",
    "- Brunello Matteo (mat. 858867)\n",
    "- Caresio Lorenzo (mat. 836021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZilRu5wjyHO"
   },
   "source": [
    "## 1. Download del dataset\n",
    "Iniziamo scaricando ed estraendo il dataset dal provider (moodle).\n",
    "\n",
    "Dopo essersi autenticati su moodle, inserire il valore del cookie `MoodleSession` (consultabile nella tab *Storage* nella finestra di ispeziona elemento su qualsiasi browser) all'interno della variabile `moodle_session_cookie` della cella seguente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFal6eg3sjhJ",
    "outputId": "29c2a9ca-7a22-4203-a1ce-5acf7ccd4d11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 27935  100 27935    0     0   149k      0 --:--:-- --:--:-- --:--:--  150k\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.macl'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.macromates.visibleIndex'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.macromates.selectionRange'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.macl'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.macromates.visibleIndex'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.macromates.selectionRange'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.macl'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.lastuseddate#PS'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.macromates.visibleIndex'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.macromates.selectionRange'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CSV header is:\n",
      "source,text,created_at,retweet_count,favorite_count,is_retweet,id_str\n"
     ]
    }
   ],
   "source": [
    "# Open the inspect element into your moodle session, then paste the \"MoodleSession\" field value in the storage/cookies tab\n",
    "moodle_session_cookie = '87qml4svnpjvgvlh5frht45rtc'\n",
    "\n",
    "!curl --cookie 'MoodleSession={moodle_session_cookie}' \"https://informatica.i-learn.unito.it/pluginfile.php/364318/mod_folder/content/0/utils/trump_twitter_archive.tgz?forcedownload=1\" -o trump.tgz\n",
    "!tar -xf trump.tgz\n",
    "!rm trump.tgz\n",
    "print(100*'-')\n",
    "print(\"CSV header is:\")\n",
    "!head -1 trump_twitter_archive/tweets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKHk2fDOkyTE"
   },
   "source": [
    "Dall'output della cella precedente è possibile notare che l'header del file `.csv` contenuto nell'archivio sia il seguente:\n",
    "\n",
    "```\n",
    "source,text,created_at,retweet_count,favorite_count,is_retweet,id_str\n",
    "```\n",
    "\n",
    "Siccome vogliamo creare un modello del linguaggio, si è interessati solamente al campo `text`.\n",
    "Dopo una prima esplorazione del dataset, risulta evidente per diverse ragioni che il testo necessiti di una prima fase di preprocessing. La prima è che sono inclusi i testi dei tweets a cui Trump ha risposto (re-tweets), i quali compaiono con il seguente formato:\n",
    "\n",
    "```\n",
    "\"@BridgetGonzale3: @realDonaldTrump I mean seriously come on Donald Trump is the coolest guy ever.\"  Thanks but easy against the losers!\n",
    "```\n",
    "\n",
    "Si noti che il testo del tweet a cui Trump ha risposto è quotato tra virgolette, mentre il vero contenuto (quello di Trump) è quello rimanente.\n",
    "Per eliminare queste occorrenze, utilizziamo una semplice espressione regolare `r'\".*?\"'`.\n",
    "\n",
    "La seconda è che i tweets presentano anche links, per cui anche per questi si è optato per eliminarli tramite un'espressione regolare `r'http[s]?://\\S+'`. Possiamo poi utilizzare la funzione `re.sub` per sostituire tutte le occorrenze che fanno match con le espressioni regolari (concatenate in *OR*) tra loro con una stringa vuota.\n",
    "\n",
    "La funzione `process_tweet(text)` implementa questa fase di preprocessing per il singolo tweet. Durante la fase di caricamento del dataset, applichiamo questa fase di preprocessing al contenuto di ogni Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YH60b5v1zwpn",
    "outputId": "907404e5-c33e-4967-d23e-b63f5e2571b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm sarah westcot-williams incompetence should not be rewarded you should vote for anyone who runs against her—loser @primeministersx\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from typing import List\n",
    "import random\n",
    "\n",
    "def load_dataset(path) -> List[str]:\n",
    "  with open(path, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=',', quotechar='|')\n",
    "    return list(map(lambda r: process_tweet(r['text']), reader))\n",
    "\n",
    "def process_tweet(text: str) -> str:\n",
    "    # pattern = r'&amp;|@\\b\\w+\\b[:]?|http[s]?://\\S+|\\\".*?\\\"'\n",
    "    pattern = r'\\\".*?\\\"|&amp;|\"@\\b\\w+\\b:|http[s]?://\\S+'\n",
    "    # pattern = r'&amp;|http[s]?://\\S+|\\\".*?\\\"'\n",
    "    text = text.lower()\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    # Remove duplicate spaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    # Clear each word by stripping special characters\n",
    "    cleaned_text = ' '.join([word.strip(\".,?!\\\"'():;.…-“”\") for word in cleaned_text.split()])\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "dataset = load_dataset('trump_twitter_archive/tweets.csv')\n",
    "# Print a random tweet from the pre-processed dataset\n",
    "print(dataset[random.randint(0, len(dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLYP18kGuWpK"
   },
   "source": [
    "## 2. Definizione del modello\n",
    "Definiamo ora il modello del linguaggio. Si è optato per ragioni didattiche di implementare il modello da zero, senza ausilio di librerie esterne.\n",
    "\n",
    "Il modello in questione prende come iperparametro (costruttore) la grandezza del contesto (`ngram_size`). In questo modo possiamo avere una definizione unica per una molteplicità di modelli.\n",
    "Il metodo `fit` serve ad apprende le probabilità dal corpus. Il suo funzionamento consiste inizialmente nell'aggiungere per ogni tweet un numero opportuno di token di inizio (`</s>`) e fine (`<s>`) tramite la funzione `fill_start_symbols`.\n",
    "\n",
    "Successivamente, vengono memorizzati sia i conteggi dei *contesti* che degli *n-grammi* in dei dizionari opportuni. Tali conteggi, vengono poi utilizzati per calcolare le probabilità dei vari n-grammi che verranno sempre memorizzate all'interno di un dizionario apposito, che rappresenta di fatto i parametri del modello (`weights`).\n",
    "Le chiavi di questo dizionario sono gli n-grammi, mentre i valori sono probabilità associati ad essi.\n",
    "\n",
    "Ad esempio nel caso di un modello a bigrammi, l'entry $(w_i, w_j)$ sarà associata a $P(w_j \\mid w_i)$, mentre in un modello a trigrammi l'entry $(w_i, w_j, w_k)$ sarà associata a $P(w_k \\mid w_i, w_j)$.\n",
    "\n",
    "Il metodo `generate`, invece, genera una frase che può essere lunga al più `max_words` (parametro del metodo). L'approccio di generazione consiste nel generare inizialmente un contesto sufficiente composto da soli starting symbols (in accordo ovviamente con la dimensione $n$ del modello).\n",
    "\n",
    "Successivamente, si utilizza una funzione chiamata `generate_next_word`, che dato un contesto $C$, campiona la parola $w$ dalla distribuzione $P(W \\mid C)$. Per fare ciò, si utilizza un approccio di campionamento \"*weighted roulette selection*\". Essenzialmente, dato il contesto $C$, si selezionano tutti gli n-grammi della forma $(C, w_i)$. Di questi se ne considerano le parole $w_i$ e le probabilità associate $p_i$.\n",
    "A questo punto, si crea un vettore $\\vec{v}$ nel modo seguente:\n",
    "\n",
    "$$\n",
    "v_i = \\sum^i_{j=0} p_j\n",
    "$$\n",
    "\n",
    "Infine, si campiona un numero random tra $0$ e $1$ e se questo numero ricade all'interno del valore dell'entry $k$-esima, allora verrà generata la parola $w_k$.\n",
    "\n",
    "Una volta generata la parola, si ripete il ciclo, modificando opportunamente il contesto sulla frase attuale che si sta generando. Il ciclo termina quando l'ultima parola generata è il token EOS (`</s>`) oppure si è raggiunto il numero massimo di parole della frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "YGGFrgBmvWFx"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class NGramModel:\n",
    "\n",
    "  def __init__(self, ngram_size):\n",
    "    self.ngram_size = ngram_size\n",
    "    self.weights = {}\n",
    "\n",
    "  def fill_start_symbols(self, sentence: str):\n",
    "    if self.ngram_size > 1:\n",
    "      pre = ' '.join(['<s>' for i in range(self.ngram_size - 1)])\n",
    "      post = ' '.join(['</s>' for i in range(self.ngram_size - 1)])\n",
    "      sentence = f'{pre} {sentence} {post}'\n",
    "    return sentence\n",
    "\n",
    "  # Apprendere un modello consiste nei seguenti passaggi:\n",
    "  # 1. Aggiungere opportunamente alle frasi un padding di caratteri speciali\n",
    "  #    in base al tipo di modello (bigramma 1, trigramma 2, ecc..)\n",
    "  # 2. Creare dei conteggi per il bigramma, trigramma, n-gramma..\n",
    "  # 3. Creare dei conteggi per gli unigrammi\n",
    "  # 4. Utilizzare i conteggi per ottenere le probabilita'\n",
    "  def fit(self, dataset: List[str]):\n",
    "    context, ngram_counts = {}, {}\n",
    "    # Get counts\n",
    "    for sentence in dataset:\n",
    "      # Pad with as many start symbols as needed to create a sufficient context\n",
    "      sentence = self.fill_start_symbols(sentence)\n",
    "      words = sentence.split()\n",
    "      # Get context counts\n",
    "      for ngram in NGramModel.to_ngrams(words, self.ngram_size-1):\n",
    "        context[ngram] = context.get(ngram, 0) + 1\n",
    "      # Get ngram counts\n",
    "      for ngram in NGramModel.to_ngrams(words, self.ngram_size):\n",
    "        ngram_counts[ngram] = ngram_counts.get(ngram, 0) + 1\n",
    "    # Compute probabilities\n",
    "    for ngram, count in ngram_counts.items():\n",
    "      self.weights[ngram] = count / context[ngram[:-1]]\n",
    "\n",
    "\n",
    "  # Generate the ngrams using a sliding window approach, then eliminates\n",
    "  # the \"artifacts\" with the slicing operator\n",
    "  def to_ngrams(words: List[str], n: int):\n",
    "    ngrams = [tuple(words[i:i+n]) for i in range(len(words))]\n",
    "    if n != 1: return ngrams[:-(n-1)]\n",
    "    else: return ngrams\n",
    "\n",
    "  # Generate a phrase with length <= max_words\n",
    "  def generate(self, max_words: int):\n",
    "    # Initialize both the current sentence and the current_context with as many\n",
    "    # intial symbols as needed\n",
    "    current_context = ['<s>' for i in range(self.ngram_size - 1)]\n",
    "    sentence = ['<s>' for i in range(self.ngram_size - 1)]\n",
    "    # Generate until we hit max_words length or the generated character is EOS\n",
    "    for i in range(0, max_words):\n",
    "      generated_word = self.generate_next_word(tuple(current_context))\n",
    "      # Add generated words\n",
    "      sentence.append(generated_word)\n",
    "      if generated_word == '</s>': break\n",
    "      # Update the current context using a \"sligind window\" on the generated words\n",
    "      current_context = sentence[i+1:]\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "  def generate_next_word(self, context):\n",
    "    # Given a (n-1)gram (context), get the relevant word\n",
    "    words, probs = [], []\n",
    "    for ngram, prob in self.weights.items():\n",
    "      if ngram[:-1] == context:\n",
    "        words.append(ngram[-1])\n",
    "        probs.append(prob)\n",
    "\n",
    "    # At this point, we can implement a weighted roulette weight selection sampling method\n",
    "    # Create a cumulative distribution\n",
    "    cumulative_probs = [sum(probs[:i+1]) for i in range(len(probs))]\n",
    "    # Draw a random number between 0-1\n",
    "    rand = random.random()\n",
    "    # Return the (n-1)gram that is consistent with the drawn number\n",
    "    for i, cumulative_prob in enumerate(cumulative_probs):\n",
    "        if rand < cumulative_prob:\n",
    "          return words[i]\n",
    "    # This branch of execution is impossible, so we throw an exception\n",
    "    raise Exception('Weighted roulette reached end of loop. Unsound procedure.')\n",
    "\n",
    "  # Use the model to assign a probability to a sentence using the (log) MLE principle\n",
    "  # Given a sentence W = w1 w2 .. wn and a language model LM, LM(W) = P(W)\n",
    "  def __call__(self, sentence: str) -> float:\n",
    "    # Add start/end symbols to the sentence\n",
    "    sentence = self.fill_start_symbols(sentence)\n",
    "    ngrams = NGramModel.to_ngrams(sentence.split(), self.ngram_size)\n",
    "    # Cycle through each ngram, getting its conditional probability and\n",
    "    # accumulating it into the result\n",
    "    result = 0.0\n",
    "    for ngram in ngrams:\n",
    "      print(ngram)\n",
    "      # get the log of the conditional probability\n",
    "      result += math.log(self.weights.get(ngram, 1))\n",
    "    # get back the result\n",
    "    return math.exp(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0ZG4dYVSsvt"
   },
   "source": [
    "# 3. Generazione di Tweet\n",
    "\n",
    "A questo punto, è possibile utilizzare il modello per generare alcuni tweets. Di seguito generiamo 5 tweets sia con un modello a bigrammi che con uno a trigrammi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-XjkSdXcSuSR"
   },
   "outputs": [],
   "source": [
    "def generate_sentences(dataset: List[str], n_sentences: int, n: int):\n",
    "  max_sentence_size = 20\n",
    "  print(f'''{max_sentence_size*'-'} Generating {n_sentences} sentences with a {n}-gram model {max_sentence_size*'-'}''')\n",
    "  # Define the n-gram model\n",
    "  model = NGramModel(n)\n",
    "  # Learn the model on the given dataset\n",
    "  model.fit(dataset)\n",
    "  # Generate n_sentences using the generative feature of the model\n",
    "  for i in range(n_sentences):\n",
    "    print(model.generate(max_sentence_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HAsLO6ZVDnrN",
    "outputId": "e7a6b0d2-6dd6-4361-e687-e85613daa48a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Generating 5 sentences with a 2-gram model --------------------\n",
      "<s> i only horse in the haters and win texas on wednesday january 17th rather expose them as presidential elections be\n",
      "<s> she will and losers donaldtrump is loser zero presence you quit trying mike type performance is a nice cold loser\n",
      "<s> for themselves they are no one good aspect of bernie </s>\n",
      "<s> @bearmntn but i like a route it out of his lover the totally overrated loser mr kellyanne did to more\n",
      "<s> it again in political sites losers with.little imagination and losers must admit that matters for it makes statements about and\n"
     ]
    }
   ],
   "source": [
    "generate_sentences(dataset, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9UwYzoGZEb5P",
    "outputId": "6f800a04-f5ea-4185-a3f2-a1d5a1075c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Generating 5 sentences with a 3-gram model --------------------\n",
      "<s> <s> @frankluntz works really hard but is a real loser named tim o'brien--and it's never recovered </s>\n",
      "<s> <s> happy father's day to all even the haters and losers never give me credit for that too </s>\n",
      "<s> <s> @subhana_anwar it's easy just think of haters as losers with.little imagination and even less understanding of success-and very lazy </s>\n",
      "<s> <s> what separates the winners from the losers on like @repswalwell who got zero as presidential candidate before quitting pramila jayapal\n",
      "<s> <s> non-existent sources and a victor at the same time you can only smile when the losers is how a person\n"
     ]
    }
   ],
   "source": [
    "generate_sentences(dataset, 5, 3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
