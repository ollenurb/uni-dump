{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ON-0h_TPjX-y"
   },
   "source": [
    "# Esercitazione 4 di Tecnologie del Linguaggio Naturale - Text categorization with Vector Space Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studenti:\n",
    "\n",
    "- Brunello Matteo (mat. 858867)\n",
    "- Caresio Lorenzo (mat. 836021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbtLnkMWjtOb"
   },
   "source": [
    "## 1. Download e caricamento del dataset\n",
    "Iniziamo scaricando ed estraendo il dataset dal provider (moodle).\n",
    "\n",
    "Dopo essersi autenticati su moodle, inserire il valore del cookie `MoodleSession` (consultabile nella tab *Storage* nella finestra di ispeziona elemento su qualsiasi browser) all'interno della variabile `moodle_session_cookie` della cella seguente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7yE2BPEjMj8",
    "outputId": "0acc2fa3-696c-434b-913a-e9a7b435acdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1226k  100 1226k    0     0  3357k      0 --:--:-- --:--:-- --:--:-- 3369k\n",
      "alt.atheism\t\t  rec.autos\t      sci.space\n",
      "comp.graphics\t\t  rec.motorcycles     soc.religion.christian\n",
      "comp.os.ms-windows.misc   rec.sport.baseball  talk.politics.guns\n",
      "comp.sys.ibm.pc.hardware  rec.sport.hockey    talk.politics.mideast\n",
      "comp.sys.mac.hardware\t  sci.crypt\t      talk.politics.misc\n",
      "comp.windows.x\t\t  sci.electronics     talk.religion.misc\n",
      "misc.forsale\t\t  sci.med\n"
     ]
    }
   ],
   "source": [
    "# Open the inspect element into your moodle session, then paste the \"MoodleSession\" field value in the storage/cookies tab\n",
    "moodle_session_cookie = '87qml4svnpjvgvlh5frht45rtc'\n",
    "\n",
    "!curl --cookie 'MoodleSession={moodle_session_cookie}' \"https://informatica.i-learn.unito.it/pluginfile.php/364320/mod_folder/content/0/utils/data.zip?forcedownload=1\" -o data.zip\n",
    "!unzip -q data.zip \"data/20_NGs_400/*\" -x \"data/20_NGs_400/.DS_Store\"\n",
    "!ls data/20_NGs_400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzj089ceGb_O"
   },
   "source": [
    "Notiamo in primo luogo che il dataset è suddiviso per classi in diverse directories. Ogni directory contiene i documenti che fanno riferimento alla classe che rappresenta la directory stessa. Per caricare il dataset, si attraversa in una prima fase tutta la gerarchia delle directory, salvando durante il processo i vari path dei files e le loro classi corrispondenti. Durante questo processo vengono generati quindi 3 array:\n",
    "\n",
    "* `paths`: contiene i path (completi) di tutti i file presenti nel dataset.\n",
    "* `Y`: contiene le classi dei vari documenti.\n",
    "* `Y_labels`: contiene i nomi delle classi (sono i nomi delle directories).\n",
    "\n",
    "Una volta completata questa fase preliminare, si utilizza successivamente l'utility `TfidfVectorizer` della libreria `sklearn` per trasformare il vettore `paths` nei corrispondenti vettori con rappresentazione *Tfidf*.\n",
    "I parametri del vettorizzatore sono:\n",
    "\n",
    "* `input = 'filename'`: specifica che ogni riga della matrice passata in input rappresenta un path del documento che il vettorizzatore aprirà in lettura.\n",
    "* `strip_accents = 'unicode'`: indica che la punteggiatura deve essere rimossa, così come i caratteri speciali.\n",
    "* `stop_words = 'english'`: specifica che le stopwords devono essere eliminate. Esse sono determinate da un vocabolario nella lingua di riferimento dei documenti (in questo caso, Inglese).\n",
    "\n",
    "Impostando questi parametri, l'utility si preoccuperà di aprire e leggere i vari files dei documenti, rimuovendo punteggiatura e stopwords, calcolando infine le frequenze necessarie per ottenere la rappresentazione vettoriale desiderata.\n",
    "\n",
    "Una volta ottenuta questa nuova rappresentazione, la procedura di caricamento può terminare correttamente, ritornando il dataset in formato *Tfidf*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A8oXikpglQg5"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# The idea is to first traverse the entire dataset's hierarchy while keeping\n",
    "# track of classes and the corresponding paths of each document. In this way,\n",
    "# we can then use the list of paths to create (Tf * Idf) weight vectors for each\n",
    "# document (in the same order on which we visited the dataset), just by using\n",
    "# the sklearn's utility TfIdfVectorizer\n",
    "def load_tfidf_dataset(base_dir: str):\n",
    "  X, Y, paths = [], [], []\n",
    "  # y labels are just going to be directory names\n",
    "  Y_labels = np.array(os.listdir(base_dir))\n",
    "  for c_i, class_name in enumerate(Y_labels):\n",
    "    # get the documents of that class\n",
    "    for document in os.listdir(f'{base_dir}/{class_name}'):\n",
    "      Y.append(c_i)                                       # append the class index\n",
    "      paths.append(f'{base_dir}/{class_name}/{document}') # append the path of the document (needed by the TfidfVectorizer)\n",
    "  # transform Y into a column vector\n",
    "  Y = np.array(Y)\n",
    "  # build and fit the vectorizer\n",
    "  vectorizer = TfidfVectorizer(\n",
    "      input = 'filename',\n",
    "      strip_accents = 'unicode',\n",
    "      stop_words = 'english'\n",
    "  )\n",
    "  # create the weight vectors and transform to a normal array (since it returns a sparse representation matrix)\n",
    "  X = vectorizer.fit_transform(paths).toarray()\n",
    "  return X, Y, Y_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjRAGnA9EoFm"
   },
   "source": [
    "## 2. Definizione del modello\n",
    "Definiamo ora il modello di Rocchio per fare classificazione. Come ogni modello di machine learning, esso avrà degli iperparametri definibili dall'utente e dei parametri che verranno invece appresi durante la fase di learning.\n",
    "\n",
    "Nel nostro caso, gli iperparametri possono essere impostati attraverso il costruttore della classe e sono:\n",
    "\n",
    "* $\\gamma$ e $\\beta$: si veda la formula citata più avanti.\n",
    "* `threshold` ($t$): se specificato, il training considererà i near positives come insieme di negativi, altrimenti l'intero set dei negativi verrà considerato (*ulteriori dettagli verranno discussi successivamente*).\n",
    "* `similarity`: funzione di similarità che deve utilizzare il modello. Di default è la *Cosine Similarity*.\n",
    "\n",
    "L'implementazione segue a grandi linee le scelte di design della libreria `sklearn`, per cui il modello mette a disposizione 2 metodi:\n",
    "\n",
    "* `fit`: apprende i parametri del modello sulla base dei dati passati in input.\n",
    "* `__call__`: ritorna la classe corrispondente al profilo di Rocchio più vicino all'esempio passato in input. L'override dell'operatore di *call* permette di chiamare il classificatore come se fosse una funzione, che ne aumenta la leggibilità del codice.\n",
    "\n",
    "La fase di apprendimento si occupa di calcolare i parametri del modello, che corrispondono ai profili di Rocchio per ogni classe. Questi profili sono appresi per mezzo dell'implementazione diretta della seguente formula (in caso l'iperparametro `threshold` non sia specificato):\n",
    "\n",
    "$$\n",
    "f_{ki} = \\beta \\cdot \\sum_{d_j \\in POS_i} \\frac{w_{kj}}{|POS_i|} - \\gamma \\cdot \\sum_{d_j \\in NEG_i} \\frac{w_{kj}}{|NEG_i|}\n",
    "$$\n",
    "\n",
    "Altrimenti, per mezzo dell'implementazione della seguente:\n",
    "\n",
    "$$\n",
    "f_{ki} = \\beta \\cdot \\sum_{d_j \\in POS_i} \\frac{w_{kj}}{|POS_i|} - \\gamma \\cdot \\sum_{d_j \\in NPOS_i} \\frac{w_{kj}}{|NPOS_i|}\n",
    "$$\n",
    "\n",
    "Nel caso della seconda, i *near positives* sono definiti come\n",
    "$$\n",
    "NPOS_i = \\{ \\vec{w} \\in NEG_i  \\; : \\; sim(\\vec{w}, \\vec{p}) \\geq t\\}\n",
    "$$\n",
    "dove:\n",
    "\n",
    "* $NEG_i$ è l'insieme dei vettori della classe negativa.\n",
    "* $\\vec{p}$ è il centroide della classe positiva.\n",
    "* $t$ è un iperparametro di threshold.\n",
    "* $sim(x, y)$ è la funzione che ritorna la similarità tra $x$ e $y$.\n",
    "\n",
    "Per effettuare i calcoli necessari in modo efficiente, si è optato di utilizzare la libreria per il cacolo numerico `numpy`. Vettorizzando efficientemente le varie operazioni, la fase di learning può sfruttare le varie ottimizzazioni messe in atto dalla libreria, tra cui l'utilizzo delle routine SIMD per l'accelerazione del calcolo.\n",
    "\n",
    "La fase di prediction, invece, consiste semplicemente nel calcolare la distanza del vettore in input (che rappresenta un documento) con tutti i profili di Rocchio appresi nella fase di learning. Successivamente, si ritorna la classe associata al profilo di Rocchio più prossimo al vettore in input (*highest rank*). Dal punto di vista implementativo, si è utilizzato il [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html) in modo da sfruttare le ottimizzazioni di `numpy`. Inoltre, l'impiego di routine di `numpy` permette di fare prediction su un batch di vettori con una sola operazione in modo molto efficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dOnkm9yXEXcF"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class RocchioClassifier:\n",
    "\n",
    "  def __init__(self, beta = 16, gamma = 4, threshold = None, similarity = cosine_similarity):\n",
    "    self.beta = beta\n",
    "    self.gamma = gamma\n",
    "    self.profiles = None\n",
    "    self.similarity = similarity\n",
    "    self.threshold = threshold\n",
    "\n",
    "  # Build the Rocchio profiles for each class in the dataset\n",
    "  def fit(self, X, Y):\n",
    "    # get the number of classes\n",
    "    profiles = []\n",
    "    for c_i in np.unique(Y):\n",
    "      # take the set of positives and negatives by comparing their class with the current class\n",
    "      pos, neg = X[Y == c_i], X[Y != c_i]\n",
    "      # compute the positive centroid\n",
    "      p_centroid = np.mean(pos, axis = 0)\n",
    "      # if the threshold is defined, then we need to use the near-positives formulation\n",
    "      # for the negative samples\n",
    "      if self.threshold:\n",
    "        # first compute the similarities between each negative sample and\n",
    "        # the positive centroid\n",
    "        similarities = self.similarity(neg, p_centroid.reshape(1, -1)).flatten()\n",
    "        # we re-define the negatives as all the documents that are at least similar\n",
    "        # according to a minimum threshold value\n",
    "        neg = neg[similarities >= self.threshold]\n",
    "      # compute the negative centroid\n",
    "      n_centroid = np.mean(neg, axis = 0)\n",
    "      # add the current rocchio profile to the array of classes\n",
    "      profile = (self.beta * p_centroid) - (self.gamma * n_centroid)\n",
    "      profiles.append(profile)\n",
    "    # set the learned profiles as the previously computed profiles\n",
    "    self.profiles = np.array(profiles)\n",
    "\n",
    "  # predict the most similar class given a vector (or a series of vectors)\n",
    "  def __call__(self, X):\n",
    "    # compute all similarities between each input vector and each class\n",
    "    similarities = self.similarity(X, self.profiles)\n",
    "    # return the most similar class for each input\n",
    "    return np.argmax(similarities, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKi1JPHiEgfr"
   },
   "source": [
    "## 3. Valutazione delle performance\n",
    "Una volta ottenuto il modello è necessario valutarne le performance. Una tecnica molto nota nel machine learning è il *K-fold Validation*, in cui si utilizzano diversi split *train-test* (chiamati *fold*) del dataset valutandone prima le performance sul singolo fold, per poi fare una media totale dei risultati associati ai vari fold.\n",
    "\n",
    "Per calcolare i vari fold, è stata utilizzata l'utility `KFold` di `sklearn`. Essa ritorna una lista di tuple contenenti due liste di indici associati agli elementi che rappresentano rispettivamente il *training set* e il *test set*.\n",
    "\n",
    "L'implementazione della procedura a questo punto consiste nell'iterare i vari fold, recuperando inizialmente gli elementi nel dataset corrispondenti agli indici. Successivamente si apprende il modello sui dati di train del fold, per poi fare prediction e infine calcolare l'accuracy del fold considerato.\n",
    "I vari risultati sono accumulati in una variabile, che viene poi divisa per il numero di fold per ottenere la media totale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EDUS2tq7Aywl"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def kfold_eval(folds, model = RocchioClassifier()):\n",
    "  kfold = KFold(n_splits = folds, shuffle=True, random_state = 42)\n",
    "  print(f'\\u001b[36m------ Evaluating over {folds} folds  ------\\u001b[0m')\n",
    "  print('\\tK-Fold Nr. \\t Accuracy')\n",
    "  print('\\u001b[36m---------------------------------------\\u001b[0m')\n",
    "  mean_acc = 0\n",
    "  for i, (train_idx, test_idx) in enumerate(kfold.split(X)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "    # train the model\n",
    "    model.fit(X_train, Y_train)\n",
    "    # evaluate the accuracy of the model\n",
    "    Y_pred = model(X_test)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    print(f'\\t {i+1} \\t\\t {accuracy}')\n",
    "    mean_acc += accuracy\n",
    "  # compute the mean accuracy over all folds\n",
    "  print('\\u001b[36m---------------------------------------\\u001b[0m')\n",
    "  print(f'Mean Accuracy over {folds} folds: {mean_acc / folds}%')\n",
    "  print('\\u001b[36m---------------------------------------\\u001b[0m\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcG8U3z4l3Ik"
   },
   "source": [
    "Una volta implementata la funzione, è sufficiente caricare il dataset con l'utility apposita, definire il modello e richiamare la funzione atta a fare *K-fold validation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NEbz1V4_uFVH",
    "outputId": "4beca8ce-25de-4bb1-b688-9fe86de04908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m------ Evaluating over 10 folds  ------\u001b[0m\n",
      "\tK-Fold Nr. \t Accuracy\n",
      "\u001b[36m---------------------------------------\u001b[0m\n",
      "\t 1 \t\t 0.725\n",
      "\t 2 \t\t 0.65\n",
      "\t 3 \t\t 0.65\n",
      "\t 4 \t\t 0.75\n",
      "\t 5 \t\t 0.8\n",
      "\t 6 \t\t 0.6\n",
      "\t 7 \t\t 0.575\n",
      "\t 8 \t\t 0.7\n",
      "\t 9 \t\t 0.575\n",
      "\t 10 \t\t 0.75\n",
      "\u001b[36m---------------------------------------\u001b[0m\n",
      "Mean Accuracy over 10 folds: 0.6775%\n",
      "\u001b[36m---------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[36m------ Evaluating over 10 folds  ------\u001b[0m\n",
      "\tK-Fold Nr. \t Accuracy\n",
      "\u001b[36m---------------------------------------\u001b[0m\n",
      "\t 1 \t\t 0.725\n",
      "\t 2 \t\t 0.65\n",
      "\t 3 \t\t 0.625\n",
      "\t 4 \t\t 0.75\n",
      "\t 5 \t\t 0.8\n",
      "\t 6 \t\t 0.6\n",
      "\t 7 \t\t 0.575\n",
      "\t 8 \t\t 0.7\n",
      "\t 9 \t\t 0.575\n",
      "\t 10 \t\t 0.75\n",
      "\u001b[36m---------------------------------------\u001b[0m\n",
      "Mean Accuracy over 10 folds: 0.675%\n",
      "\u001b[36m---------------------------------------\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = 'data/20_NGs_400'\n",
    "\n",
    "# Load the dataset\n",
    "X, Y, Y_l = load_tfidf_dataset(dataset_dir)\n",
    "folds = 10\n",
    "# Evaluate the first model using the k-fold evaluation method\n",
    "model = RocchioClassifier()\n",
    "kfold_eval(folds, model)\n",
    "# Evaluate the second model using the k-fold evaluation method, using a threshold of 0.2\n",
    "model = RocchioClassifier(threshold = 0.02)\n",
    "kfold_eval(folds, model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
